{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"readme/","title":"Readme","text":""},{"location":"readme/#multi-cluster-configuration-management-for-edge-multi-cloud-and-hybrid-cloud","title":"Multi-cluster Configuration Management for Edge, Multi-Cloud, and Hybrid Cloud","text":"<p>Imagine KubeStellar as a post office for your Kubernetes resources. When you drop packages at the post office, they don't open them; they deliver them to the right recipients. Similarly, KubeStellar works like this for your Kubernetes resources. Instead of running resources right away, KubeStellar safely stores and sends resources to selected clusters across the globe\u2014whether they're in public clouds, private clouds, or on the edge of your network. It's a super useful tool for spreading your Kubernetes resources wherever you need them without disrupting your existing tools and workflows.</p> <p>How does KubeStellar resist the temptation to run your Kubernetes resources right away? KubeStellar accepts your applied resources in a special staging area (virtual cluster) where pods can't be created. Then, at your direction, KubeStellar transfers your applied resources to remote clusters where they can create pods and other required resource dependencies. KubeStellar does this using many different lightweight virtual cluster providers (Kind, KubeFlex, KCP, etc.) to create this special staging area. </p> <p>KubeStellar is an innovative way to stage inactive Kubernetes resources and then apply them to any cluster to run. KubeStellar introduces a native way to expand, optimize, and protect your Kubernetes resources from individual cluster misconfiguration, utilization, and failure. </p> <p>Don't change anything, just add KubeStellar!</p>"},{"location":"readme/#kubestellar-treats-multiple-kubernetes-clusters-as-one-so-you-can","title":"KubeStellar treats multiple Kubernetes clusters as one so you can:","text":"<ul> <li>Centrally apply Kubernetes resources for selective deployment across multiple clusters </li> <li>Use standard Kubernetes native deployment tools (kubectl, Helm, Kustomize, ArgoCD, Flux); no resource bundling required</li> <li>Discover dynamically created objects created on remote clusters</li> <li>Make disconnected cluster operation possible</li> <li>Scale with 1:many and many:1 scenarios</li> <li>Remain compatible with cloud-native solutions</li> </ul>"},{"location":"readme/#kubestellar-virtual-clusters-spaces-are-our-secret","title":"KubeStellar virtual clusters (Spaces) are our secret","text":"<ul> <li>KubeStellar uses lightweight virtual clusters (Spaces) that run inside the KubeStellar hosting cluster</li> <li>Standard Kubernetes clusters have 2-300 api-resources, KubeStellar Spaces have only 40</li> <li>Fewer api-resources mean resources remain inactive (denatured) \u2013 they do not expand into other resources like replicasets, pods, etc.</li> <li>Denaturing is the key to accepting native, unbundled Kubernetes resources as input without running them</li> <li>Unbundled resources are the default and preferred output of most cloud-native tools making KubeStellar use and integration easy</li> </ul>"},{"location":"readme/#quickstart","title":"QuickStart","text":"<p>Checkout our QuickStart Guide</p>"},{"location":"readme/#roadmaps-for-the-project","title":"Roadmaps for the Project","text":"<p>We have defined and largely completed the PoC2023q1. The current activity is refining the definition of, and producing, the PoC2023q4. Goals not addressed in that PoC are to be explored later.</p>"},{"location":"readme/#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"readme/#getting-in-touch","title":"Getting in touch","text":"<p>There are several ways to communicate with us:</p> <p>Instantly get access to our documents and meeting invites http://kubestellar.io/joinus</p> <ul> <li>The <code>#kubestellar-dev</code> channel in the Kubernetes Slack workspace</li> <li>Our mailing lists:<ul> <li>kubestellar-dev for development discussions</li> <li>kubestellar-users for discussions among users and potential users</li> </ul> </li> <li>Subscribe to the community calendar for community meetings and events<ul> <li>The kubestellar-dev mailing list is subscribed to this calendar</li> </ul> </li> <li>See recordings of past KubeStellar community meetings on YouTube</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kubestellar-dev mailing list can view this drive</li> </ul> </li> <li>Read our documentation</li> <li>Follow us on:</li> <li>LinkedIn - #kubestellar</li> <li>Medium - kubestellar.medium.com</li> </ul>"},{"location":"readme/#contributors","title":"\u2764\ufe0f Contributors","text":"<p>Thanks go to these wonderful people:</p> Jun Duan\ud83d\udc40 Braulio Dumba\ud83d\udc40 Mike Spreitzer\ud83d\udc40 Paolo Dettori\ud83d\udc40 Andy Anderson\ud83d\udc40 Franco Stellari\ud83d\udc40 Ezra Silvera\ud83d\udc40 Bob Filepp\ud83d\udc40 Alexei Karve\ud83d\udc40 Maria Camila Ruiz Cardenas\ud83d\udc40 Aleksander Slominski\ud83d\udc40 Aashni Manroa\ud83d\udc40 Kevin Roche\ud83d\udc40 Nick Masluk\ud83d\udc40 Francois Abel\ud83d\udc40"},{"location":"Coding%20Milestones/PoC2023q1/coding-milestone-invite-q1/","title":"Invitation","text":"<p>Dear Contributors,</p> <p>We are excited to invite you to join the first KubeStellar opensource community coding sprint. We will be focus on several key projects that are critical to the development of state-based edge solutions. Our collective work will be showcased to the opensource community on Thursday, April 27th.</p> <p>This coding sprint will provide a great opportunity for you to showcase your skills, learn new techniques, and collaborate with other experienced engineers in the KubeStellar community. We believe that your contributions will be invaluable in helping us achieve our goals and making a lasting impact in the field of state-based edge technology.</p> <p>The coding sprint will be dedicated to completing the following workload management elements:</p> <ul> <li>Implementing a Where Resolver and a Placement Translator, including customization options,</li> <li>Incorporating existing customization API into the KubeStellar repo,</li> <li>Investigating implementation of a status summarizer, starting with basic implicit status, and later adding programmed summarization,</li> <li>Updating summarization API and integrating it into the KubeStellar repo,</li> <li>Defining the API for identifying associated objects and its interaction with summarization, and implementing these,</li> <li>Streamlining the creation of workload management workspaces,</li> <li>Examining the use of Postgresql through Kine instead of etcd for scalability,</li> <li>Revising the milestone outline with regards to defining bootstrapping and support for cluster-scoped resources.</li> </ul> <p>In addition to workload management, we will also be working on inventory management for the demo, as well as designing various demo scenarios, including a baseline demo with kubectl, demos with ArgoCD, FluxCD, and the European Space Agency (ESA). To support the engineers and demonstrations we will also need to automate the process of creating infrastructure, deploying demo pieces and instrumentation, bootstrapping, running scenarios, and collecting data.</p> <p>If you are interested in joining us for this exciting coding sprint, please check out our 'good first issue' list, or slack me @Andy Anderson so I can connect you with others in your area of interest.  There is a place for every skillset to contribute. Not quite sure?  You can join our bi-weekly community meetings to watch our progress.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/","title":"Commands","text":"<p>This PoC includes two sorts of commands for users to use. Most are executables designed to be accessed in the usual ways: by appearing in a directory on the user's <code>$PATH</code> or by the user writing a pathname for the executable on the command line. The one exception is the bootstrap script, which is designed to be fetched from github and fed directly into <code>bash</code>.</p> <p>This document describes both commands that a platform administrator uses and commands that a platform user uses.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#deployment-style","title":"Deployment style","text":"<p>There normal way of deploying the space framework, a space provider, and the central KubeStellar controllers is as workload in a Kubernetes (possibly OpenShift) cluster.  Contributors may instead run the Space Manager, possibly space provider (dependening on the provider), and KubeStellar central controllers as bare processes on a machine of supported OS and ISA. Even in this case a Kubernetes cluster is required, at least to hold the Space Management API objects; this role is called the Space Management API Space (SMAS). The KubeFlex space provider must be deployed in a cluster, which is called the KubeFlex hosting cluster. That cluster can also play the SMAS role.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#in-cluster","title":"In-cluster","text":"<p>Some commands take an optional flag <code>--in-cluster</code> that is about the networking relationship between (a) that running command and (b) the apiserver(s) for the space(s) that the command accesses. This flag can be used when both (a) and (b) are in the same Kubernetes cluster. This flag can also be used when both (a) and (b) are not in a Kubernetes cluster but rather are bare processes on the same machine. Omitting this flag means that the running command will use the more general way of addressing the apiserver(s), which is functional in more situations but possibly less efficient.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#os-namespace-consistency","title":"OS namespace consistency","text":"<p>For some of the commands, all of them must always be invoked in the context of the same filesystem, working directory, and process space. This is a point-in-time limitation due to the way kube-bind konnectors are launched. The relevant commands are as follows.</p> <ul> <li><code>kubectl kubestellar ensure inventory-space</code></li> <li><code>kubectl kubestellar ensure wmw</code></li> <li><code>kubestellar</code> (which is also constrained because <code>start</code> and <code>stop</code>   are implemented by ordinary commands, no remote agent is involved)</li> <li><code>kubestellar-kube-bind</code></li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#bare-process-deployment","title":"Bare process deployment","text":""},{"location":"Coding%20Milestones/PoC2023q1/commands/#kcp-process-control","title":"kcp process control","text":"<p>KubeStellar has some commands to support situations in which some clients of the kcp server need to connect to it by opening a connection to a DNS domain name rather than an IP address that the server can bind its socket to.  The key idea is using the <code>--tls-sni-cert-key</code> command line flag of <code>kcp start</code> to configure the server to respond with a bespoke server certificate in TLS handshakes in which the client addresses the server by a given domain name.</p> <p>These commands are used separately from starting the kcp server and are designed so that they can be used multiple times if there are multiple sets of clients that need to use a distinct domain name. Starting the kcp server is not described here, beyond the particular consideration needed for the <code>--tls-sni-cert-key</code> flag, because it is otherwise ordinary.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-ensure-kcp-server-creds","title":"kubestellar-ensure-kcp-server-creds","text":""},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-ensure-kcp-server-creds-pre-reqs","title":"kubestellar-ensure-kcp-server-creds pre-reqs","text":"<p>The <code>kubestellar-ensure-kcp-server-creds</code> command requires that Easy-RSA is installed.  As outlined in https://easy-rsa.readthedocs.io/en/latest/#obtaining-and-using-easy-rsa, this involves selecting a release archive from the list on GitHub, unpacking it, and adding the EasyRSA directory to your <code>$PATH</code>; <code>easyrsa</code> is a bash script, so you do not need to worry about building or fetching a binary specific to your OS or computer architecture.</p> <p>Easy-RSA uses OpenSSL, so you will need that installed too.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-ensure-kcp-server-creds-usage","title":"kubestellar-ensure-kcp-server-creds usage","text":"<p>This command is given exactly one thing on the command line, a DNS domain name.  This command creates --- or re-uses if it finds already existing --- a private key and public X.509 certificate for the kcp server to use.  The certificate has exactly one SubjectAlternativeName, which is of the DNS form and specifies the given domain name.  For example: <code>kubestellar-ensure-kcp-server-creds foo.bar</code> creates a certificate with one SAN, commonly rendered as <code>DNS:foo.bar</code>.</p> <p>This command uses a Public Key Infrastructure (PKI) and Certificate Authority (CA) implemented by easy-rsa, rooted at the subdirectory <code>pki</code> of the current working directory.  This command will create the PKI if it does not already exist, and will initialize the CA if that has not already been done.  The CA's public certificate appears at the usual place for easy-rsa: <code>pki/ca.crt</code>.</p> <p>This command prints some stuff --- mostly progress remarks from easy-rsa --- to stderr and prints one line of results to stdout.  The <code>bash</code> shell will parse that one line as three words.  Each is an absolute pathname of one certificate or key.  The three are as follows.</p> <ol> <li>The CA certificate for the client to use in verifying the server.</li> <li>The X.509 certificate for the server to put in its ServerHello in a    TLS handshake in which the ClientHello has a Server Name Indicator    (SNI) that matches the given domain name.</li> <li>The private key corresponding to that server certificate.</li> </ol> <p>Following is an example of invoking this command and examining its results.</p> <pre><code>bash-5.2$ eval pieces=($(kubestellar-ensure-kcp-server-creds yep.yep))\nRe-using PKI at /Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki\nRe-using CA at /Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/private/ca.key\nAccepting existing credentials\nbash-5.2$ echo ${pieces[0]}\n/Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/ca.crt\nbash-5.2$ echo ${pieces[1]}\n/Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/issued/kcp-DNS-yep.yep.crt\nbash-5.2$ echo ${pieces[2]}\n/Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/private/kcp-DNS-yep.yep.key\n</code></pre> <p>Following is an example of using those results in launching the kcp server.  The <code>--tls-sni-cert-key</code> flag can appear multiple times on the command line, configuring the server to respond in a different way to each of multiple different SNIs.</p> <pre><code>bash-5.2$ kcp start --tls-sni-cert-key ${pieces[1]},${pieces[2]} &amp;&gt; /tmp/kcp.log &amp;\n[1] 66974\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#wait-and-switch-domain","title":"wait-and-switch-domain","text":"<p>This command is for using after the kcp server has been launched. Since the <code>kcp start</code> command really means <code>kcp run</code>, all usage of that server has to be done by concurrent processes.  The <code>wait-and-switch-domain</code> command bundles two things: waiting for the kcp server to start handling kubectl requests, and making an alternate kubeconfig file for a set of clients to use.  This command is pointed at an existing kubeconfig file and reads it but does not write it; the alternate config file is written (its directory must already exist and be writable).  This command takes exactly six command line positional arguments, as follows.</p> <ol> <li>Pathname (absolute or relative) of the input kubeconfig.</li> <li>Pathname (absolute or relative) of the output kubeconfig.</li> <li>Name of the kubeconfig \"context\" that identifies what to replace.</li> <li>Domain name to put in the replacement server URLs.</li> <li>Port number to put in the replacement server URLs.</li> <li>Pathname (absolute or relative) of a file holding the CA    certificate to put in the alternate kubeconfig file.</li> </ol> <p>Creation of the alternate kubeconfig file starts by looking in the input kubeconfig file for the \"context\" with the given name, to find the name of a \"cluster\".  The server URL of that cluster is examined, and its <code>protocol://host:port</code> prefix is extracted.  The alternate kubeconfig will differ from the input kubeconfig in the contents of the cluster objects whose server URLs start with that same prefix. There will be the following two differences.</p> <ol> <li>In the server URL's <code>protocol://host:port</code> prefix, the host will be    replaced by the given domain name and the port will be replaced by    the given port.</li> <li>The cluster will be given a <code>certificate-authority-data</code> that holds    the contents (base64 encoded, as usual) of the given CA certificate    file.</li> </ol> <p>Following is an example of using this command and examining the results.  The context and port number chosen work for the kubeconfig file that <code>kcp start</code> (kcp release v0.11.0) creates by default.</p> <pre><code>bash-5.2$ wait-and-switch-domain .kcp/admin.kubeconfig test.yaml root yep.yep 6443 ${pieces[0]}\nbash-5.2$ diff -w .kcp/admin.kubeconfig test.yaml\n4,5c4,5\n&lt;     certificate-authority-data: LS0...LQo=\n&lt;     server: https://192.168.something.something:6443\n---\n&gt;       certificate-authority-data: LS0...LQo=\n&gt;       server: https://yep.yep:6443\n8,9c8,9\n&lt;     certificate-authority-data: LS0...LQo=\n&lt;     server: https://192.168.something.something:6443/clusters/root\n---\n&gt;       certificate-authority-data: LS0...LQo=\n&gt;       server: https://yep.yep:6443/clusters/root\n</code></pre> <p>Following is an example of using the alternate kubeconfig file, in a context where the domain name \"yep.yep\" resolves to an IP address of the network namespace in which the kcp server is running.</p> <pre><code>bash-5.2$ KUBECONFIG=.kcp-yep.yep/admin.kubeconfig kubectl ws .\nCurrent workspace is \"root\".\n</code></pre> <p>Because this command reads the given kubeconfig file, it is important to invoke this command while nothing is concurrently writing it and while the caller reliably knows the name of a kubeconfig context that identifies what to replace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#switch-domain","title":"switch-domain","text":"<p>This command is the second part of <code>wait-and-switch-domain</code>: the part of creating the alternate kubeconfig file.  It has the same inputs and outputs and concurrency considerations.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-process-control","title":"KubeStellar process control","text":"<p>The <code>kubestellar</code> command has three subcommands, one to finish setup and two for process control.</p> <p>Except for the <code>stop</code> subcommand, this command has the following pre-requisites.</p> <ul> <li> <p>The space manager is running, there is a space provider named   \"default\", and that is the one to use.</p> </li> <li> <p>When not given any command-line flags, <code>kubectl</code> will access the   Space Management API Space (SMAS).</p> </li> </ul> <p>This command is one of those requiring OS namespace consistency.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubestellar [flags] subcommand [flags]\n</code></pre> <p>This command accepts the following command line flags, which can appear before and/or after the subcommand.  The <code>--log-folder</code> flag is only meaningful for the <code>start</code> subcommand. The <code>--in-cluster</code> flag is not meaningful for the <code>stop</code> subcommand. The <code>--ensure-imw</code> and  <code>--ensure-wmw</code> flags are only meaningful for the <code>start</code> or <code>init</code> subcommands.</p> <ul> <li><code>-V</code> or <code>--verbose</code>: calls for more verbose output.  This is a   binary choice, not a matter of degree.</li> <li><code>-X</code>: turns on echoing of script lines</li> <li><code>--log-folder $pathname</code>: says where to put the logs from the   controllers.  Will be <code>mkdir -p</code> if absent.  Defaults to   <code>${PWD}/kubestellar-logs</code>.</li> <li><code>--in-cluster</code>: as usual.</li> <li><code>--ensure-imw</code>: provide a comma separated list of pathnames for inventory workspaces, e.g. \"root:imw1,root:imw2\". Defaults to \"root:imw1\". To prevent the creation of any inventory workspace, then pass \"\".</li> <li><code>--ensure-wmw</code>: provide a comma separated list of pathnames for workload management workspaces, e.g. \"root:wmw1,root:imw2\". Defaults to \"root:wmw1\". To prevent the creation of any workload management workspace, then pass \"\".</li> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-init","title":"Kubestellar init","text":"<p>This subcommand is used after installation to finish setup and does the following five things.</p> <ol> <li> <p>Ensure that the KubeStellar Core Space (KCS, formerly called edge    service provider workspace (ESPW)) exists and has the required    contents.</p> </li> <li> <p>Ensure the existence of zero, one, or more inventory spaces    depending on the value of <code>--ensure-imw</code> flag. Default is one    inventory space named \"imw1\".</p> </li> <li> <p>Ensure the existence of zero, one, or more workload description    spaces (WDS, formerly workload management workspaces (WMW))    depending on the value of <code>--ensure-wmw</code> flag. Default is one WDS    named \"wmw1\". The WDSes have CRDs for the Kubernetes APIS for    management of containerized workloads.</p> </li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-start","title":"KubeStellar start","text":"<p>This subcommand is used after installation or process stops.</p> <p>This subcommand stops any running kubestellar controllers and then starts them all.  It also does the same things as <code>kubestellar init</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-stop","title":"KubeStellar stop","text":"<p>This subcommand undoes the primary function of <code>kubestellar start</code>, stopping any running KubeStellar controllers.  It does not tear down the ESPW.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#deployment-into-a-kubernetes-cluster","title":"Deployment into a Kubernetes cluster","text":"<p>These commands administer a deployment of the central components --- the space provider server, PKI, and the central KubeStellar components  --- in a Kubernetes cluster that will be referred to as \"the hosting  cluster\".  These commands are framed as \"kubectl plugins\" and thus need  to be explicitly or implicitly given a kubeconfig file for the hosting cluster.</p> <p>You need a Kubernetes cluster with an Ingress controller deployed and configured in a way that does not terminate TLS connections (this abstinence is often called \"SSL passthrough\"). An OpenShift cluster would be one qualifying thing. Another would be an ordinary Kubernetes cluster with the nginx Ingress controller deployed and configured appropriately. Please note that special considerations apply when deploying an ingress controller in <code>kind</code>. See a fully worked example with kind and nginx. You will need to know the port number at which the Ingress controller is listening for HTTPS connections.</p> <p>IF your Kubernetes cluster has any worker nodes --- real or virtual --- with the x86_64 instruction set, they need to support the extended instruction set known as \"x64-64-v2\". If using hardware bought in the last 10 years, you can assume that is true. If using emulation, you need to make sure that your emulator is emulating that extended instruction set --- some emulators do not do this by default. See QEMU configuration recommendations, for example.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that gets to the Ingress controller's listening socket.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#deploy-to-cluster","title":"Deploy to cluster","text":"<p>Deployment is done with a \"kubectl plugin\" that is invoked as <code>kubectl kubestellar deploy</code> and creates a Helm \"release\" in the hosting cluster. As such, it relies on explicit (on the command line) or implicit (in environment variables and/or <code>~/.kube/config</code>) configuration needed to execute Helm commands. The following flags can appear on the command line, in any order.</p> <ul> <li><code>--openshift $bool</code>, saying whether the hosting cluster is an   OpenShift cluster.  If so then a Route will be created to the kcp   server; otherwise, an Ingress object will direct incoming TLS   connections to the kcp server.  The default is <code>false</code>.</li> <li><code>--external-endpoint $domain_name:$port</code>, saying how the kcp server   will be reached from outside the cluster.  The given domain name   must be something that the external clients will resolve to an IP   address where the cluster's Ingress controller or OpenShift router   will be listening, and the given port must be the corresponding TCP   port number.  For a plain Kubernetes cluster, this must be   specified.  For an OpenShift cluster this may be omitted, in which   case the command will (a) assume that the external port number is   443 and (b) extract the external hostname from the Route object   after it is created and updated by OpenShift.  FYI, that external   hostname will start with a string derived from the Route in the   chart (currently \"kubestellar-route-kubestellar\") and continue with   \".\" and then the ingress domain name for the cluster.</li> <li>a command line flag for the <code>helm upgrade</code> command. This includes   the usual control over namespace: you can set it on the command   line, otherwise the namespace that is current in your kubeconfig   applies.</li> <li><code>-X</code> turns on debug echoing of all the commands in the script that   implements this command.</li> <li><code>-h</code> prints a brief usage message and terminates with success.</li> </ul> <p>For example, to deploy to a plain Kubernetes cluster whose Ingress controller can be reached at <code>my-long-application-name.my-region.some.cloud.com:1234</code>, you would issue the following command.</p> <pre><code>kubectl kubestellar deploy --external-endpoint my-long-application-name.my-region.some.cloud.com:1234\n</code></pre> <p>The Helm chart takes care of setting up the KubeStellar Core, accomplishing the same thing as the kubestellar start command above.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#fetch-a-log-from-a-kubestellar-runtime-container","title":"Fetch a log from a KubeStellar runtime container","text":""},{"location":"Coding%20Milestones/PoC2023q1/commands/#remove-deployment-to-a-kubernetes-cluster","title":"Remove deployment to a Kubernetes cluster","text":"<p>The deployment of kcp and KubeStellar as a Kubernetes workload is done by making a \"release\" (this is a technical term in Helm whose meaning might surprise you) of a Helm chart.  This release is named \"kubestellar\".  To undo the deployment, just use the Helm command to delete that release.</p> <pre><code>helm delete kubestellar\n</code></pre> <p>NOTE: this is a detail that might change in the future.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-platform-user-commands","title":"KubeStellar platform user commands","text":"<p>The remainder of the commands in this document are for users rather than administrators of the service that KubeStellar provides.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-release","title":"KubeStellar-release","text":"<p>This command just echoes the semantic version of the release used.  This command is only available in archives built for a release.  Following is an example usage.</p> <p><pre><code>kubestellar-release\n</code></pre> <pre><code>v0.2.3-preview\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-version","title":"Kubestellar-version","text":"<p>This executable prints information about itself captured at build time.  If built by <code>make</code> then this is information conveyed by the Makefile; otherwise it is the Kubernetes defaults.</p> <p>It will either print one requested property or a JSON object containing many.</p> <p><pre><code>kubestellar-version help\n</code></pre> <pre><code>Invalid component requested: \"help\"\nUsage: kubestellar-version [buildDate|gitCommit|gitTreeState|platform]\n</code></pre></p> <p><pre><code>kubestellar-version buildDate\n</code></pre> <pre><code>2023-05-19T02:54:01Z\n</code></pre></p> <p><pre><code>kubestellar-version gitCommit\n</code></pre> <pre><code>1747254b\n</code></pre></p> <p><pre><code>kubestellar-version\n</code></pre> <pre><code>{\"major\":\"1\",\"minor\":\"24\",\"gitVersion\":\"v1.24.3+kcp-v0.2.1-20-g1747254b880cb7\",\"gitCommit\":\"1747254b\",\"gitTreeState\":\"dirty\",\"buildDate\":\"2023-05-19T02:54:01Z\",\"goVersion\":\"go1.19.9\",\"compiler\":\"gc\",\"platform\":\"darwin/amd64\"}\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-an-inventory-space","title":"Creating an Inventory Space","text":"<p>This command will create an inventory space (IS) of a given name if it does not already exist and ensure that it is properly prepared for use.</p> <p>This command is one of those requiring OS namespace consistency.</p> <p>Preparing for use includes using kube-bind to import the definitions of <code>SyncTarget</code> and <code>Location</code>. If that has not already been done then it will be done by invoking kubestellar-kube-bind, with the attendant side-effects to the filesystem (possibly creating <code>${PWD}/konnector-${space_name}/</code> and/or a temporary kubeconfig file in <code>$PWD</code>) and constraint on working directory (always the same for a given WDS).</p> <p>This command is given a kube client configuration for access to the Space Management API Space (SMAS), via the usual <code>kubectl</code> flags and <code>$KUBECONFIG</code>.</p> <p>The usage synopsis for this command is as follows.</p> <pre><code>kubectl kubestellar ensure inventory-space flag... inventory_space_name\n</code></pre> <p>This command accepts the following flags. They can be intermixed with the positional argument.</p> <ul> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate   successfully.</li> <li><code>-X</code>: turn on debug echoing of the commands inside the script that   implements this command.</li> <li>a <code>kubectl</code> flag (but not <code>-o</code> or <code>--output</code>).</li> <li><code>--in-cluster</code>: as usual.</li> <li><code>--output-kubeconfig $file_pathname</code>: requests that an in-cluster   kubeconfig for the space be output to the given file pathname.</li> </ul> <p>This script works in idempotent style, doing whatever work remains to be done.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-synctargetlocation-pairs","title":"Creating SyncTarget/Location pairs","text":"<p>In this PoC, the interface between infrastructure and workload management is inventory API objects.  Specifically, for each workload execution cluster there is a unique pair of SyncTarget and Location objects in a so-called inventory space.  These kinds of objects were originally defined in kcp TMC, and now there is a copy of those definitions (CRDs) in KubeStellar.  It is the definitions in KubeStellar that should be referenced.  Those are in the Kubernetes API group <code>edge.kubestellar.io</code>, and they are exported from the KubeStellar Core Space (KCS)) (the space named <code>espw</code>).</p> <p>The following command helps with making that SyncTarget and Location pair. This command operates on an existing inventory space. This command is given a kube client config for that space via the usual <code>kubectl</code> command line flags and <code>$KUBECONFIG</code>.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar ensure location flag... objname labelname=labelvalue...\n</code></pre> <p>Here <code>objname</code> is the name for the SyncTarget object and also the name for the Location object.  This command ensures that these objects exist and have at least the given labels.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>This command accepts the following flags.</p> <ul> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate successfully.</li> <li><code>-X</code>: turn on debug echoing of the commands inside the script that   implements this command.</li> <li>a <code>kubectl</code> flag (but not <code>-o</code> or <code>--output</code>).</li> </ul> <p>This command does not depend on the action of any of the KubeStellar controllers but does require that the KubeStellar Core Space (KCS) has been set up.</p> <p>An example usage follows.</p> <p><pre><code>KUBECONFIG=imw1.kubeconfig kubectl kubestellar ensure location demo1 foo=bar the-word=the-bird\n</code></pre> <pre><code>synctarget.workload.kcp.io/demo1 created\nlocation.scheduling.kcp.io/demo1 created\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\n</code></pre></p> <p>The above example shows using this script to create a SyncTarget and a Location named <code>demo1</code> with labels <code>foo=bar</code> and <code>the-word=the-bird</code>. This was equivalent to the following commands.</p> <pre><code>KUBECONFIG=imw1.kubeconfig kubectl create -f -&lt;&lt;EOF\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\n  name: demo1\n  labels:\n    id: demo1\n    foo: bar\n    the-word: the-bird\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\n  name: demo1\n  labels:\n    foo: bar\n    the-word: the-bird\nspec:\n  resource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\n  instanceSelector:\n    matchLabels: {\"id\":\"demo1\"}\nEOF\n</code></pre> <p>Note that the extraction of space kubeconfig file can be accomplished using the  <code>kubectl kubestellar space get-kubeconfig</code> command.</p> <p>This command operates in idempotent style, making whatever changes (if any) are needed to move from the current state to the desired state.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-synctargetlocation-pairs","title":"Removing SyncTarget/Location pairs","text":"<p>THIS SCRIPT IS BROKEN</p> <p>The following script undoes whatever remains from a corresponding usage of <code>kubectl kubestellar ensure location</code>.  It has all the same command line syntax and semantics except that the <code>labelname=labelvalue</code> pairs do not appear.</p> <p>This command does not depend on the action of any of the KubeStellar controllers.</p> <p>The following session demonstrates usage, including idempotency.</p> <p><pre><code>KUBECONFIG=$imw_space_config kubectl kubestellar remove location demo1\n</code></pre> <pre><code>synctarget.workload.kcp.io \"demo1\" deleted\nlocation.scheduling.kcp.io \"demo1\" deleted\n</code></pre></p> <pre><code>KUBECONFIG=$imw_space_config kubectl kubestellar remove location demo1\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#getting-a-kubeconfig-of-a-given-space","title":"Getting a kubeconfig of a given space","text":"<p>This command fetches a kubeconfig for super-user access to a given space into a given filename. The kubeconfig will be good for one year since the creation of the space unless it was configured unusually.</p> <p>Each space has two such kubeconfig files. One kubeconfig is known as the \"in-cluster\" kubeconfig and is for accessing the space from within the same Kubernetes cluster as the space's apiserver(s) (assuming they are in a cluster). The other is known as the \"external\" kubeconfig and is for accessing the space from outside the space's host cluster (assuming there is one). Clients inside that cluster generally can use the external kubeconfig successfully but it is less efficient. The precise truth of the matter depends on the hostname configured for external access at deployment time.</p> <p>This command is given a client configuration for accessing the Space Management API Space (SMAS), via the usual ways for configuring <code>kubectl</code>.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar space get-kubeconfig flag... $space_name $output_pathname\n</code></pre> <p>This command accepts the following flags.</p> <ul> <li><code>-h</code> or <code>--help</code>: print usage and terminate with success.</li> <li><code>-X</code>: turn on debug echoing of the script as it executes.</li> <li>a <code>kubectl</code> flag (but not <code>-o</code> or <code>--output</code>).</li> <li><code>--in-cluster</code>: indicates that the kubeconfig is for use   in-cluster.</li> </ul> <p>Following is an example usage.</p> <pre><code>kubectl kubestellar space get-kubeconfig imw1 imw1.kubeconfig\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#syncer-preparation-and-installation","title":"Syncer preparation and installation","text":"<p>The syncer runs in each edge cluster and also talks to the corresponding mailbox space.  In order for it to be able to do that, there is some work to do in the mailbox space to create a ServiceAccount for the syncer to authenticate as and create RBAC objects to give the syncer the privileges that it needs.  The following script does those things and also outputs YAML to be used to install the syncer in the edge cluster.</p> <p>This script is given a kube client config for accessing the SMAS via the usual <code>kubectl</code> command line flags and <code>$KUBECONFIG</code>.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar prep-for-syncer flag... synctarget_name\n</code></pre> <p>Here <code>synctarget_name</code> is the name of the <code>SyncTarget</code> object, in the relevant IMW, corresponding to the relevant edge cluster.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>This command accepts the following flags.</p> <ul> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate successfully.</li> <li><code>-X</code>: turn on debug echoing of commands inside the script that   implements this command.</li> <li>a <code>kubectl</code> flaag (but not <code>-o</code> or <code>--output</code>).</li> <li><code>--in-cluster</code>: as usual.</li> <li><code>--imw space_name</code>: specifies which space holds the relevant   SyncTarget object.</li> <li><code>--espw space_name</code>: specifies where to find the edge service   provider space.</li> <li><code>--syncer-image image_ref</code>: specifies the container image that runs   the syncer.  The default is <code>quay.io/kubestellar/syncer:latest</code>.</li> <li><code>-o output_pathname</code>: specifies where to write the YAML definitions   of the API objects to create in the edge cluster in order to deploy   the syncer there.  The default is <code>synctarget_name +   \"-syncer.yaml\"</code>. NOTE this is differnt from the usual meaning of   <code>-o</code> for <code>kubectl</code>.</li> <li><code>-s</code>: exceptionally low info output.</li> </ul> <p>This command will only succeed if the mailbox controller has created and conditioned the mailbox space for the given SyncTarget.  This command will wait for 10 to 70 seconds for that to happen.</p> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw-1 demo1\n</code></pre> <pre><code>Current space is \"4yqm57kx0m6mn76c-mb-406c54d1-64ce-4fdc-99b3-cef9c4fc5010\" Creating service account \"kubestellar-syncer-demo1-28at01r3\"\nCreating cluster role \"kubestellar-syncer-demo1-28at01r3\" to give service account \"kubestellar-syncer-demo1-28at01r3\"\n1. write and sync access to the synctarget \"kubestellar-syncer-demo1-28at01r3\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-demo1-28at01r3\" to bind service account \"kubestellar-syncer-demo1-28at01r3\" to cluster role \"kubestellar-syncer-demo1-28at01r3\".\n\nWrote WEC manifest to demo1-syncer.yaml for namespace \"kubestellar-syncer-demo1-28at01r3\". Use\n\nKUBECONFIG=&lt;space-execution-cluster-config&gt; kubectl apply -f \"demo1-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;space-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-demo1-28at01r3\" kubestellar-syncer-demo1-28at01r3\n\nto verify the syncer pod is running.\n</code></pre></p> <p>Once that script has run, the YAML for the objects to create in the edge cluster is in your chosen output file.  The default for the output file is the name of the SyncTarget object with \"-syncer.yaml\" appended.</p> <p>Create those objects with a command like the following; adjust as needed to configure <code>kubectl</code> to modify the edge cluster and read your chosen output file.</p> <pre><code>KUBECONFIG=$demo1_kubeconfig kubectl apply -f demo1-syncer.yaml\n</code></pre> <p>You can preserve this file for future use if you want to remove the syncer later.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#syncer-removal","title":"Syncer removal","text":"<p>The previous section shows the two steps involved in establishing a syncer: one modifies the core and one adds stuff into the WEC. If for any reason your core gets deleted (e.g., deliberate replacement) and you want to undo the addition of the syncer into the WEC, you can use the YAML file generated above with <code>kubectl delete</code> instead of <code>kubectl apply</code> (of course using the right kubeconfig and context therein).</p> <p>A syncer that is configured to work with an extinct core is harmless except that its pod will be doing some futile work (failing to make requests on the core and logging error messages).</p> <p>If you need to manually remove the syncer from your WEC, following is the list of things that went into the WEC.</p> <ul> <li>A namespace named   <code>kubestellar-syncer-${SyncTarget_Name}-${UID}</code>. The UID is a random   ID that is uniquely generated each time you run prep-for-syncer.</li> <li>A ServiceAccount (with associated token Secret) in that namespace.</li> <li>Another Secret in that namespace.</li> <li>A Deployment in that namespace</li> <li>A ClusterRole with the same name as the namespace.</li> <li>A ClusterRoleBinding with the same name as the namespace.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#workload-execution-cluster-on-boarding","title":"Workload execution cluster on-boarding","text":"<p>The following command is a combination of <code>kubectl kubestellar ensure-location</code> and <code>kubectl kubestellar prep-for-syncer</code>, and takes the union of their command line flags and arguments with the following exceptions/modifications.</p> <ul> <li>The <code>-o</code> flag is not accepted.</li> <li>The <code>kubectl</code> flags and <code>$KUBECONFIG</code> say how to access the Space   Management API Space (SMAS), not the inventory space.</li> </ul> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar prep-for-cluster --imw imw-1 demo2 key1=val1\n</code></pre> <pre><code>synctarget.workload.kcp.io/demo2 created\nlocation.scheduling.kcp.io/demo2 created\nsynctarget.workload.kcp.io/demo2 labeled\nlocation.scheduling.kcp.io/demo2 labeled\nCurrent space is \"1cpf1cd4ydy13vo1-mb-3c354acd-ed86-45bb-a60d-cee8e59973f7\" Creating service account \"kubestellar-syncer-demo2-15nq4e94\"\nCreating cluster role \"kubestellar-syncer-demo2-15nq4e94\" to give service account \"kubestellar-syncer-demo2-15nq4e94\"\n1. write and sync access to the synctarget \"kubestellar-syncer-demo2-15nq4e94\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-demo2-15nq4e94\" to bind service account \"kubestellar-syncer-demo2-15nq4e94\" to cluster role \"kubestellar-syncer-demo2-15nq4e94\".\n\nWrote WEC manifest to demo2-syncer.yaml for namespace \"kubestellar-syncer-demo2-15nq4e94\". Use\n\nKUBECONFIG=&lt;space-execution-cluster-config&gt; kubectl apply -f \"demo2-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;space-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-demo2-15nq4e94\" kubestellar-syncer-demo2-15nq4e94\n\nto verify the syncer pod is running.\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-a-workload-description-space","title":"Creating a Workload Description Space","text":"<p>This command will create a WDS of a given name if it does not already exist and ensure that it is properly prepared for use.</p> <p>This command is one of those requiring OS namespace consistency.</p> <p>Preparing for use includes using kube-bind to import the relevant parts of the KubeStellar API. If that has not already been done then it will be done by invoking kubestellar-kube-bind, with the attendant side-effects to the filesystem (possibly creating <code>${PWD}/konnector-${space_name}/</code> and/or a temporary kubeconfig file in <code>$PWD</code>) and constraint on working directory (always the same for a given WDS).</p> <p>Preparing for use also includes ensuring the CRDs for the Kubernetes APIs for management of containerized workloads and present iff desired.</p> <p>This command is given a kube client configuration for access to the Space Management API Space (SMAS), via the usual <code>kubectl</code> flags and <code>$KUBECONFIG</code>.</p> <p>The usage synopsis for this command is as follows.</p> <pre><code>kubectl kubestellar ensure wmw flag... wm_space_name\n</code></pre> <p>This command accepts the following flags. They can be intermixed with the positional argument.</p> <ul> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate   successfully.</li> <li><code>-X</code>: turn on debug echoing of the commands inside the script that   implements this command.</li> <li><code>--in-cluster</code>: as usual.</li> <li>a <code>kubectl</code> flag (but not <code>-o</code> or <code>--output</code>).</li> <li><code>--output-kubeconfig $file_pathname</code>: requests that an in-cluster   kubeconfig for the WDS be output to the given file pathname.</li> <li><code>--with-kube boolean</code>: specifies whether or not the WMW should   include CRDs for the supported subset of the Kubernetes API   for management of containerized workloads. See the categorization   in the design for details.</li> </ul> <p>This script works in idempotent style, doing whatever work remains to be done.</p> <p>The following session shows some example usages, including demonstration of idempotency and changing whether the kube CRDs are included.</p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw\n</code></pre> <pre><code>Error from server (NotFound): spaces.space.kubestellar.io \"example-wmw\" not found\nspace.space.kubestellar.io/example-wmw created\nWaiting for space example-wmw to get into the Ready phase.\nSpace example-wmw created.\nSECRET_NAME=external-example-wmw\nSECRET_NAMESPACE=spaceprovider-default\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nstarting kube-bind konnector for space example-wmw in background, logs writing to /home/ubuntu/kubestellar/kubestellar-kube-bind-logs/kube-bind-konnector-example-wmw.log\nStoring konnector kubeconfigs in /home/fred/kubestellar-konnectors/231212-010203-edgeplacements-1234\nbinding edgeplacements for example-wmw\ncluster namespace is kube-bind-tmflg\nCurrent workspace is \"root:espw\".\nnamespace kubestellar already exists\nconfigmap/kbmap-fcf8hzj7mlx156cm created\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nkube-bind konnector for space example-wmw already running\nStoring konnector kubeconfigs in /home/fred/kubestellar-konnectors/231212-010213-customizers-1245\nbinding customizers for example-wmw\ncluster namespace is kube-bind-tmflg\nCurrent workspace is \"root:espw\".\nnamespace kubestellar already exists\nconfigmap/kbmap-fcf8hzj7mlx156cm configured\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nkube-bind konnector for space example-wmw already running\nStoring konnector kubeconfigs in /home/fred/kubestellar-konnectors/231212-010233-singleplacementslices-1256\nbinding singleplacementslices for example-wmw\ncluster namespace is kube-bind-tmflg\nCurrent workspace is \"root:espw\".\nnamespace kubestellar already exists\nconfigmap/kbmap-fcf8hzj7mlx156cm configured\ncustomresourcedefinition.apiextensions.k8s.io/apiservices.apiregistration.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/persistentvolumes.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/flowschemas.flowcontrol.apiserver.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/prioritylevelconfigurations.flowcontrol.apiserver.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/ingressclasses.networking.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/runtimeclasses.node.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/priorityclasses.scheduling.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/csidrivers.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/csinodes.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/storageclasses.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/volumeattachments.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/daemonsets.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/deployments.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/replicasets.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/statefulsets.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/horizontalpodautoscalers.autoscaling serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/cronjobs.batch serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/jobs.batch serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/endpoints.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/persistentvolumeclaims.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/pods.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/podtemplates.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/replicationcontrollers.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/services.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/endpointslices.discovery.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.networking.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/poddisruptionbudgets.policy serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/csistoragecapacities.storage.k8s.io serverside-applied\n</code></pre></p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw\n</code></pre> <pre><code>SECRET_NAME=external-example-wmw\nSECRET_NAMESPACE=spaceprovider-default\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nCRD for edgeplacements already in place\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nCRD for customizers already in place\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nCRD for singleplacementslices already in place\ncustomresourcedefinition.apiextensions.k8s.io/apiservices.apiregistration.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/persistentvolumes.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/flowschemas.flowcontrol.apiserver.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/prioritylevelconfigurations.flowcontrol.apiserver.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/ingressclasses.networking.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/runtimeclasses.node.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/priorityclasses.scheduling.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/csidrivers.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/csinodes.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/storageclasses.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/volumeattachments.storage.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/daemonsets.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/deployments.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/replicasets.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/statefulsets.apps serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/horizontalpodautoscalers.autoscaling serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/cronjobs.batch serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/jobs.batch serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/endpoints.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/persistentvolumeclaims.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/pods.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/podtemplates.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/replicationcontrollers.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/services.core serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/endpointslices.discovery.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/ingresses.networking.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.networking.k8s.io serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/poddisruptionbudgets.policy serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/csistoragecapacities.storage.k8s.io serverside-applied\n</code></pre></p> <p><pre><code>IN_CLUSTER=false kubectl kubestellar ensure wmw example-wmw --with-kube false\n</code></pre> <pre><code>SECRET_NAME=external-example-wmw\nSECRET_NAMESPACE=spaceprovider-default\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nCRD for edgeplacements already in place\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nCRD for customizers already in place\nCurrent workspace is \"root\".\nkcp workspace example-wmw's ID is fcf8hzj7mlx156cm\nCurrent workspace is \"root:example-wmw\".\nCRD for singleplacementslices already in place\ncustomresourcedefinition.apiextensions.k8s.io \"apiservices.apiregistration.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"persistentvolumes.core\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"flowschemas.flowcontrol.apiserver.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"prioritylevelconfigurations.flowcontrol.apiserver.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"ingressclasses.networking.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"runtimeclasses.node.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"priorityclasses.scheduling.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"csidrivers.storage.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"csinodes.storage.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"storageclasses.storage.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"volumeattachments.storage.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"daemonsets.apps\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"deployments.apps\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"replicasets.apps\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"statefulsets.apps\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"horizontalpodautoscalers.autoscaling\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"cronjobs.batch\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"jobs.batch\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"endpoints.core\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"persistentvolumeclaims.core\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"pods.core\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"podtemplates.core\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"replicationcontrollers.core\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"services.core\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"endpointslices.discovery.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"ingresses.networking.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"networkpolicies.networking.k8s.io\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"poddisruptionbudgets.policy\" deleted\ncustomresourcedefinition.apiextensions.k8s.io \"csistoragecapacities.storage.k8s.io\" deleted\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-a-workload-description-space","title":"Removing a Workload Description Space","text":"<p>THIS SCRIPT IS BROKEN</p> <p>Deleting a WDS (formerly WMW) can be done by simply deleting its <code>space</code> object from the parent.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default example-wmw\n</code></pre> <pre><code>workspace.tenancy.kcp.io \"example-wmw\" deleted\n</code></pre></p> <p>Alternatively, you can use the following command line whose design completes the square here.  Invoke it when the current workspace is the parent of the workload management workspace to delete.</p> <p><pre><code>kubectl kubestellar remove wmw -h\n</code></pre> <pre><code>Usage: kubectl kubestellar remove wmw [-X] kubectl_flag... wm_workspace_name\n</code></pre></p> <p><pre><code>kubectl kubestellar remove wmw demo1\n</code></pre> <pre><code>Current workspace is \"root\".\nworkspace.tenancy.kcp.io \"demo1\" deleted\n</code></pre></p> <p><pre><code>kubectl kubestellar remove wmw demo1\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-list-syncing-objects","title":"kubestellar-list-syncing-objects","text":"<p>NOTE: This command works directly with the kcp server, it has not   yet been converted to go through the space abstraction layer.</p> <p>The <code>kubestellar-list-syncing-objects</code> command will list or watch one kind of objects in the mailbox spaces. These are full (not summarized) workload objects in the intermediate place between WDS' and WEC', participating in downsync or upsync. The user of this command is not very exposed to the mailbox spaces themselves; the command is directed at the whole of this intermediate place and lists all of the requested kind of objects there. The kind of objects to list/watch is given by command line flags. In general the user has to also give the \"resource\" that is roughly equivalent to the \"kind\", but that can be defaulted in the easy case.</p> <p>The output is suitable for piping to <code>jq</code> or <code>yq</code>. In the JSON case, the output is one object per line (not pretty-printed). The default is to output YAML.</p> <p>This command will either do a one-shot listing or an ongoing list+watch. In the latter case each object is extended with a field named <code>Action</code> having a value of either <code>add</code>, <code>update</code>, or <code>delete</code>.</p> <p>This command is given two Kubernetes client configurations.  One, called \"all\", is for reading the chosen objects from all workspaces. The other, called \"parent\", is for reading the mailbox Workspace objects from their parent Workspace.</p> <p>Following are the command line flags beyond the baseline golang flags.</p> <pre><code>      --api-group string                 API group of objects to watch (default is Kubernetes core group)\n--api-kind string                  kind of objects to watch\n      --api-resource string              API resource (lowercase plural) of objects to watch (defaults to lowercase(kind)+'s')\n--api-version string               API version (just version, no group) of objects to watch (default \"v1\")\n--json                             indicates whether to output as lines of JSON rather than YAML\n      --watch                            indicates whether to inform rather than just list\n...\n      --all-cluster string               The name of the kubeconfig cluster to use for access to the chosen objects in all clusters\n      --all-context string               The name of the kubeconfig context to use for access to the chosen objects in all clusters (default \"system:admin\")\n--all-kubeconfig string            Path to the kubeconfig file to use for access to the chosen objects in all clusters\n      --all-user string                  The name of the kubeconfig user to use for access to the chosen objects in all clusters\n...\n      --parent-cluster string              The name of the kubeconfig cluster to use for access to the parent of mailbox workspaces\n      --parent-context string            The name of the kubeconfig context to use for access to the parent of mailbox workspaces (default \"root\")\n--parent-kubeconfig string           Path to the kubeconfig file to use for access to the parent of mailbox workspaces\n      --parent-user string                 The name of the kubeconfig user to use for access to the parent of mailbox workspaces\n</code></pre> <p>Following is an example of its usage; the ellipses show where this document omits many lines for brevity.</p> <pre><code>$ kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n...\nstatus:\n  availableReplicas: 1\n  fullyLabeledReplicas: 1\n  observedGeneration: 1\n  readyReplicas: 1\n  replicas: 1\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n...\nstatus:\n  availableReplicas: 1\n  fullyLabeledReplicas: 1\n  observedGeneration: 1\n  readyReplicas: 1\n  replicas: 1\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#bootstrap","title":"Bootstrap","text":"<p>This is a combination of some installation and setup steps, for use in the QuickStart.</p> <p>The script can be read directly from https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh and does the following things.</p> <ol> <li>Downloads and installs kcp executables if they are not already    evident on <code>$PATH</code>.</li> <li>Downloads and installs kubestellar executables if they are not    already evident on <code>$PATH</code>.</li> <li>Ensures that kcp and KubeStellar are deployed (i.e., their    processes are running and their initial configurations have been    established) either as bare processes or as workload in a    pre-existing Kubernetes cluster.</li> </ol> <p>This script accepts the following command line flags; all are optional.  The <code>--os</code>, <code>--arch</code>, and <code>--bind-address</code> flags are only useful when deploying as bare processes.  The deployment will be into a Kubernetes cluster if either <code>--external-endpoint</code> or <code>--openshift true</code> is given.</p> <ul> <li><code>--kubestellar-version $version</code>: specifies the release of   KubeStellar to use.  When using a specific version, include the   leading \"v\".  The default is the latest regular release, and the   value \"latest\" means the same thing.</li> <li><code>--kcp-version $version</code>: specifies the kcp release to use.  The   default is the one that works with the chosen release of   KubeStellar.</li> <li><code>--openshift $bool</code>: specifies whether to the hosting cluster is an   OpenShift cluster. The default value is <code>false</code>.</li> <li><code>--endpoint-address $domain_name:$port</code>: specifies where an Ingress   controller or OpenShift router is listening for incoming TLS   connections from external (to the hosting cluster) clients of kcp   and KubeStellar.</li> <li><code>--os $OS</code>: specifies the operating system to use in selecting the   executables to download and install.  Choices are <code>linux</code> and   <code>darwin</code>.  Auto-detected if omitted.</li> <li><code>--arch $IAS</code>: specifies the instruction set architecture to use in   selecting the executables to download and install.  Choices are   <code>amd64</code> and <code>arm64</code>.  Auto-detected if omitted.</li> <li><code>--bind-address $IPADDR</code>: directs that the kcp server (a) write that   address for itself in the kubeconfig file that it constructs and (b)   listens only at that address.  The default is to pick one of the   host's non-loopback addresses to write into the kubeconfig file and   not bind a listening address.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies the parent folder   for downloads.  Will be <code>mkdir -p</code>.  The default is the current   working directory.  The download of kcp, if any, will go in   <code>$install_parent_dir/kcp</code>.  The download of KubeStellar will go in   <code>$install_parent_dir/kubestellar</code>.</li> <li><code>--host-ns $namespace_in_hosting_cluster</code>: specifies the namespace   in the hosting cluster where the core will be deployed. Defaults to   \"kubestellar\".</li> <li><code>-V</code> or <code>--verbose</code>: increases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here \"install\" means only to (a) unpack the distribution archives into the relevant places under <code>$install_parent_dir</code> and (b) enhance the <code>PATH</code>, and <code>KUBECONFIG</code> in the case of kcp, environment variables in the shell running the script.  Of course, if you run the script in a sub-shell then those environment effects terminate with that sub-shell; this script also prints out messages showing how to update the environment in another shell.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#internal-commands","title":"Internal commands","text":"<p>The following commands are used by other commands but not expected to be invoked by users.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-kube-bind","title":"kubestellar-kube-bind","text":"<p>This command invokes <code>kubectl bind</code> to bind one resource into a space. Also, this command will launch a kube-bind <code>konnector</code> process if there is not already one running locally that was launched by an earlier invocation of this command.</p> <p>This command is one of those requiring OS namespace consistency.</p> <p>This command is given a kube client config for accessing the SMAS, via the usual <code>kubectl</code> flags and <code>$KUBECONFIG</code>.</p> <p>Usage synposis is as follows.</p> <pre><code>kubestellar-kube-bind flag... space_name resource\n</code></pre> <p>This command accepts the following flags.</p> <ul> <li><code>-h</code> or <code>--help</code>: prints usage and terminates successfully.</li> <li><code>-X</code>: turn on echoing of script internals for debugging.</li> <li>a <code>kubectl</code> flag (but not <code>-o</code> or <code>--output</code>).</li> <li><code>--in-cluster</code>: as usual.</li> </ul> <p>If this command launches a <code>konnector</code> then this command will <code>mkdir -p ${PWD}/konnector-$space_name</code> to hold the konnector's kubeconfig input (named <code>kubeconfig</code>) and log file (named <code>log</code>).</p> <p>Even if this invocation does not launch a konnector, this command will dump some contents of <code>{$PWD}/konnector-${space_name}/log</code> when reporting certain errors.</p> <p>This command may also create a temporary kubeconfig file in <code>$PWD</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#ensure-the-existence-of-a-space","title":"Ensure the existence of a space","text":"<p>This command will create a space if it does not already exist, and can optionally output a kubeconfig for super-user access to that space.</p> <p>This command is given a client configuration for accessing the Space Management API Space (SMAS), via the usual ways for configuring <code>kubectl</code>.</p> <p>Synopsis:</p> <pre><code>kubectl kubestellar space ensure flag... space_name\n</code></pre> <p>This command accepts the following flags. The space name, output kubeconfig, and SMAS kubeconfig and context must be supplied.</p> <ul> <li><code>-h</code> or <code>--help</code></li> <li><code>-X</code></li> <li>A <code>kubectl</code> flag (but not <code>-o</code> or <code>--output</code>).</li> <li><code>--output-kubeconfig $pathname</code></li> <li><code>--in-cluster</code>: relevant iff requesting a kubeconfig for the space</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/","title":"Example1 kubeflex","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/example1.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre> <p>This doc shows a detailed example usage of the KubeStellar components.</p> <p>This example involves two edge clusters and two workloads.  One workload goes on both edge clusters and one workload goes on only one edge cluster.  Nothing changes after the initial activity.</p> <p>This example is presented in stages.  The controllers involved are always maintaining relationships.  This document focuses on changes as they appear in this example.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#stage-1","title":"Stage 1","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox spaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#your-kubeconfig-files","title":"Your kubeconfig files","text":"<p>This example (and others) involve user-managed kubeconfig files. These examples organize those files into one subdirectory. Start by making sure that directory exists without any contents left over from previous runs.</p> <pre><code>MY_KUBECONFIGS=${PWD}/my-kubeconfigs\nrm -rf \"$MY_KUBECONFIGS\"\nmkdir -p \"$MY_KUBECONFIGS\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#create-kind-cluster-for-space-management","title":"Create Kind cluster for space management","text":"<pre><code>kind create cluster --name sm-mgt --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 9443\n    protocol: TCP\nEOF\nkubectl create -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\nsleep 20\nkubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=360s\n\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-sm-mgt sm-mgt\nSM_CONFIG=~/.kube/config\n</code></pre> <p>The subsequent uses of <code>$SM_CONFIG</code> in this example assume that the current context is still the one just established, \"sm-mgt\".</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#the-space-manager-controller","title":"The space-manager controller","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <p><pre><code>cd space-framework\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> Next deploy the space framework CRDs in the space management cluster. <pre><code>KUBECONFIG=$SM_CONFIG kubectl apply -f config/crds/\ncd ..\n</code></pre> Finally, start the space-manager controller.</p> <pre><code>space-manager --kubeconfig $SM_CONFIG --context sm-mgt -v 4 &amp;&gt; /tmp/space-manager.log &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#deploy-kubeflex-and-kubestellar-as-bare-processes","title":"Deploy KubeFlex and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#initialize-kubeflex","title":"Initialize KubeFlex","text":"<pre><code>wget https://github.com/kubestellar/kubeflex/releases/download/v0.3.3/kubeflex_0.3.3_linux_amd64.tar.gz\nmkdir kubeflex\ntar xf kubeflex_0.3.3_linux_amd64.tar.gz -C kubeflex\nkubeflex/bin/kflex --kubeconfig $SM_CONFIG init\nrm kubeflex_0.3.3_linux_amd64.tar.gz\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#create-a-space-provider-description-for-kubeflex","title":"Create a space provider description for KubeFlex","text":"<p>Space provider for KubeFlex will allow you to use KubeFlex as backend provider for spaces. Use the following commands to create a provider secret for KubeFlex access and a space provider definition.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl --context sm-mgt create secret generic kfsec --from-file=kubeconfig=$SM_CONFIG --from-file=incluster=$SM_CONFIG\nKUBECONFIG=$SM_CONFIG kubectl --context sm-mgt apply -f - &lt;&lt;EOF\napiVersion: space.kubestellar.io/v1alpha1\nkind: SpaceProviderDesc\nmetadata:\n  name: default\nspec:\n  ProviderType: \"kubeflex\"\n  SpacePrefixForDiscovery: \"ks-\"\n  secretRef:\n    namespace: default\n    name: kfsec\nEOF\n</code></pre> <p>Next, use the following command to wait for the space-manger to process the provider.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl --context sm-mgt wait --for=jsonpath='{.status.Phase}'=Ready spaceproviderdesc/default --timeout=90s\n</code></pre> <p>The following variable will be used in later commands to indicate that they are being invoked close enough to the provider's apiserver to use the more efficient networking (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"--in-cluster\"\nkube_needed=false\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#get-binaries-of-kube-bind-and-dex","title":"Get binaries of kube-bind and dex","text":"<p>The command below makes kube-bind binaries and dex binary available in <code>$PATH</code>.</p> <pre><code>rm -rf kube-bind\ngit clone https://github.com/waltforme/kube-bind.git &amp;&amp; \\\npushd kube-bind &amp;&amp; \\\ngit checkout kubestellar &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd &amp;&amp; \\\ngit clone https://github.com/dexidp/dex.git &amp;&amp; \\\npushd dex &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build &amp;&amp; \\\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the KubeStellar Core Space (KCS) (formerly called the Edge Service Provider Workspace (ESPW)), which exports the KubeStellar API.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar -X init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code> and the current context is for the KubeStellar hosting cluster.</p> <p>The following variable will be used in later commands to indicate that they are not being invoked from within the hosting cluster (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>IMW1_KUBECONFIG=\"${MY_KUBECONFIGS}/imw1.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig imw1 --kubeconfig $SM_CONFIG $in_cluster $IMW1_KUBECONFIG\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"describe the florin location object\"\nKUBECONFIG=$IMW1_KUBECONFIG kubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two <code>ensure location</code> calls are equivalent to creating the following four objects.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>KUBECONFIG=$SM_CONFIG mailbox-controller --external-access -v=4 &amp;&gt; /tmp/mailbox-controller.log &amp;\nsleep 20\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox spaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox spaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>while [ $(KUBECONFIG=$SM_CONFIG kubectl get spaces -A | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into /tmp/mailbox-controller.log if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI1218 16:36:14.743434  239027 controller.go:277] \"Created missing space\" worker=0 mbsName=\"imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\"\n...\nI1218 16:36:14.759426  239027 controller.go:277] \"Created missing space\" worker=1 mbsName=\"imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\"\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox spaces.</p> <p>A mailbox space name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox spaces as follows.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <pre><code>NAMESPACE               NAME                                           AGE\nspaceprovider-default   espw                                           84s\nspaceprovider-default   imw1                                           76s\nspaceprovider-default   imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   20s\nspaceprovider-default   imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   20s\nspaceprovider-default   wmw1                                           53s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -n spaceprovider-default -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                           SYNCTARGET   CLUSTER\nespw                                           &lt;none&gt;       &lt;none&gt;\nimw1                                           &lt;none&gt;       &lt;none&gt;\nimw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   florin       &lt;none&gt;\nimw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   guilder      &lt;none&gt;\nwmw1                                           &lt;none&gt;       &lt;none&gt;\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox space by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox space name is $GUILDER_SPACE\n</code></pre> <pre><code>The guilder mailbox space name is imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\n</code></pre></p> <p><pre><code>FLORIN_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox space name is $FLORIN_SPACE\n</code></pre> <pre><code>The florin mailbox space name is imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#connect-guilder-edge-cluster-with-its-mailbox-space","title":"Connect guilder edge cluster with its mailbox space","text":"<p>The following command will (a) create, in the mailbox space for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster guilder\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\".\n\nWrote workload execution cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\n\nto verify the syncer pod is running.\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system                                           coredns                                               2/2     2            2           4m1s\nkubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   1/1     1            1           0s\nlocal-path-storage                                    local-path-provisioner                                1/1     1            1           3m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#connect-florin-edge-cluster-with-its-mailbox-space","title":"Connect florin edge cluster with its mailbox space","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster florin\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\".\n\nWrote workload execution cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\n\nto verify the syncer pod is running.\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nserviceaccount/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\ndeployment.apps/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.  These commands create the WMW, makes KubeStellar edge APIs available for the WMW via kube-bind, and optionally makes some Kubernetes APIs available for the WMW by populating a set of CRDs into the WMW.</p> <pre><code># TODO: Needs to pass --with-kube false or true\nWMW_C_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-c.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-c --with-kube $kube_needed --output-kubeconfig \"$WMW_C_SPACE_CONFIG\"\nsleep 15\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code># TODO: needs to set --with-kube false/true\nWMW_S_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-s.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-s --with-kube $kube_needed --output-kubeconfig \"$WMW_S_SPACE_CONFIG\"\n</code></pre> <p>In this workload we will also demonstrate how to downsync objects whose kind is defined by a <code>CustomResourceDefinition</code> object. We will use the one from the Kubernetes documentation for CRDs, modified so that the resource it defines is in the category <code>all</code>. First, create the definition object with the following command.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;\n  name: crontabs.stable.example.com\nspec:\n  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;\n  group: stable.example.com\n  # list of versions supported by this CustomResourceDefinition\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                cronSpec:\n                  type: string\n                image:\n                  type: string\n                replicas:\n                  type: integer\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;\n    plural: crontabs\n    # singular name to be used as an alias on the CLI and for display\n    singular: crontab\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: CronTab\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - ct\n    categories:\n    - all\nEOF\n</code></pre> <p>Next, use the following command to wait for the apiserver to process that definition.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG wait --for condition=Established crd crontabs.stable.example.com\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n  annotations: {just-for: fun}\n---\napiVersion: \"stable.example.com/v1\"\nkind: CronTab\nmetadata:\n  name: my-new-cron-object\n  namespace: specialstuff\nspec:\n  cronSpec: \"* * * * */5\"\n  image: my-awesome-cron-image\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p>The \"what predicate\" explicitly includes the <code>Namespace</code> object named \"specialstuff\", which causes all of its desired state (including labels and annotations) to be downsynced. This contrasts with the common EdgePlacement, which does not explicitly mention the <code>commonstuff</code> namespace, relying on the implicit creation of namespaces as needed in the WECs.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    objectNames: [ v1090.example.my ]\n  - apiGroup: stable.example.com\n    resources: [ crontabs ]\n    namespaces: [ specialstuff ]\n    objectNames: [ my-new-cron-object ]\n  - apiGroup: \"\"\n    resources: [ namespaces ]\n    objectNames: [ specialstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar-where-resolver --external-access -v=4 &amp;&gt; /tmp/where-resolver.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>while ! kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nwhile ! kubectl --kubeconfig $WMW_S_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I1219 16:40:49.814564  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-florin\" location=\"kube-bind-sx6pl-florin\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n...\nI1219 16:40:49.797689  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-guilder\" location=\"kube-bind-sx6pl-guilder\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox spaces and create <code>SyncerConfig</code> objects there.</p> <p>If you have deployed the KubeStellar core as workload in a Kubernetes cluster then the placement translator is running in a Pod there. If instead you are running the core controllers as bare processes then use the following commands to launch the placement translator; it requires the ESPW to be current at start time.</p> <pre><code>KUBECONFIG=$SM_CONFIG placement-translator --external-access -v=4  &amp;&gt; /tmp/placement-translator.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait for the placement translator to get its job done for this example.</p> <pre><code>FLORIN_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${FLORIN_SPACE}.kubeconfig\"\nGUILDER_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${GUILDER_SPACE}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $FLORIN_SPACE --kubeconfig $SM_CONFIG $in_cluster $FLORIN_MB_KUBECONFIG\nkubectl-kubestellar-space-get_kubeconfig $GUILDER_SPACE --kubeconfig $SM_CONFIG $in_cluster $GUILDER_MB_KUBECONFIG\n# wait until SyncerConfig, ReplicaSets and Deployments are ready\nmbxws=($FLORIN_SPACE $GUILDER_SPACE)\nfor ii in \"${mbxws[@]}\"; do\n(\nexport KUBECONFIG=\"${MY_KUBECONFIGS}/${ii}.kubeconfig\"\n# wait for SyncerConfig resource\nwhile ! kubectl get SyncerConfig the-one &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* SyncerConfig resource exists in mailbox $ii\"\n# wait for ReplicaSet resource\nwhile ! kubectl get rs &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* ReplicaSet resource exists in mailbox $ii\"\n# wait until ReplicaSet in mailbox\nwhile ! kubectl get rs -n commonstuff commond; do\nsleep 10\ndone\necho \"* commonstuff ReplicaSet in mailbox $ii\"\n)\ndone\n(\nexport KUBECONFIG=$GUILDER_MB_KUBECONFIG\n# check for deployment in guilder\nwhile ! kubectl get deploy -A &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* Deployment resource exists\"\nwhile ! kubectl get deploy -n specialstuff speciald; do\nsleep 10\ndone\necho \"* specialstuff Deployment in its mailbox\"\n# wait for crontab CRD to be established\nwhile ! kubectl get crd crontabs.stable.example.com; do sleep 10; done\nkubectl wait --for condition=Established crd crontabs.stable.example.com\n  echo \"* CronTab CRD is established in its mailbox\"\n# wait for my-new-cron-object to be in its mailbox\nwhile ! kubectl get ct -n specialstuff my-new-cron-object; do sleep 10; done\necho \"* CronTab my-new-cron-object is in its mailbox\"\n)\n</code></pre> <p>You can check that the common workload's ReplicaSet objects got to their mailbox spaces with the following command. It will list the two copies of that object, each with an annotation whose key is <code>kcp.io/cluster</code> and whose value is the kcp <code>logicalcluster.Name</code> of the mailbox space; those names appear in the \"CLUSTER\" column of the custom-columns listing near the end of the section above about the mailbox controller.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1y7wll1dz806h3sb\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1najcltzt2nqax47\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n</code></pre> <p>That display should show objects in two different mailbox spaces; the following command checks that.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet | grep \"^ *kcp.io/cluster: [0-9a-z]*$\" | sort | uniq | wc -l) -ge 2\n</code></pre> <p>The CustomResourceDefinition objects involved should also appear in the mailbox spaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | fgrep -w \"name: crontabs.stable.example.com\"\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: replicasets.apps\") -ge 2\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: deployments.apps\") -ge 1\n</code></pre> <p>The <code>APIService</code> of the special workload should also appear, along with some error messages about <code>APIService</code> not being known in the other mailbox workspaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiregistration.k8s.io --api-kind APIService 2&gt;&amp;1 | grep -v \"APIService.*the server could not find the requested resource\" | fgrep -w \"name: v1090.example.my\"\n</code></pre> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize the name of the mailbox workspace for florin (which you stored in Stage 1) here.</p> <pre><code>KUBECONFIG=$FLORIN_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    resource: configmaps\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the mailbox workspace name that you stored in Stage 1.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  clusterScope:\n  - apiVersion: v1\n    group: \"\"\nobjects:\n    - specialstuff\n    resource: namespaces\n  - apiVersion: v1\n    group: apiextensions.k8s.io\n    objects:\n    - crontabs.stable.example.com\n    resource: customresourcedefinitions\n  - apiVersion: v1\n    group: apiregistration.k8s.io\n    objects:\n    - v1090.example.my\n    resource: apiservices\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  - apiVersion: v1\n    group: stable.example.com\n    objectsByNamespace:\n    - names:\n      - my-new-cron-object\n      namespace: specialstuff\n    resource: crontabs\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - speciald\n      namespace: specialstuff\n    resource: deployments\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    - names:\n      - httpd-htdocs\n      namespace: specialstuff\n    resource: configmaps\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p>You can check for specific workload objects here with the following command.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get deployments,replicasets -A\n</code></pre> <pre><code>NAMESPACE      NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff   deployment.apps/speciald   0/0     1            0           12m\n\nNAMESPACE     NAME                      DESIRED   CURRENT   READY   AGE\ncommonstuff   replicaset.apps/commond   0         1         1       7m4s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>( KUBECONFIG=~/.kube/config\n  let tries=1\nwhile ! kubectl --context kind-florin get ns commonstuff &amp;&gt; /dev/null; do\nif (( tries &gt;= 30)); then\necho 'The commonstuff namespace failed to appear in florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n    sleep 10\ndone\nkubectl --context kind-florin get ns\n)\nsleep 15\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy,rs -A | egrep 'NAME|stuff'\nsleep 15\n</code></pre> <pre><code>NAMESPACE                            NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\nNAMESPACE                            NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                          replicaset.apps/commond                                         1         1         1       13m\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces, the Deployment, and both ReplicaSets.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy,rs -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff                          deployment.apps/speciald                              1/1     1            1           23m\nNAMESPACE                             NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                           replicaset.apps/commond                                         1         1         1       23m\nspecialstuff                          replicaset.apps/speciald-76cdbb69b5                             1         1         1       14s\n</code></pre></p> <p>Examine the <code>APIService</code> objects in the guilder cluster, find the one named <code>v1090.example.my</code>. It is broken because it refers to a Service object that we have not bothered to create.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get apiservices | grep 1090\n</code></pre> <pre><code>v1090.example.my                       my-example/my-service   False (ServiceNotFound)   2m39s\n</code></pre></p> <p>See the crontab in the guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get crontabs -n specialstuff\nsleep 15\n</code></pre> <pre><code>NAME                 AGE\nmy-new-cron-object   37m\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get rs -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8094 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>let tries=1\nwhile ! curl http://localhost:8097 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The special workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8096 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#stage-5","title":"Stage 5","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#singleton-reported-state-return","title":"Singleton reported state return","text":"<p>The two <code>EdgePlacement</code> objects above assert that the expected number of executing copies of their matching workload objects is 1 and request return of reported state to the WDS when the number of executing copies is exactly 1.</p> <p>For the common workload, that assertion is not correct: the number of executing copies should be 2.  The assertion causes the actual number of executing copies to be reported.  Check that the reported number is 2.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG get rs -n commonstuff commond -o yaml | grep 'kubestellar.io/executing-count: \"2\"' || { kubectl --kubeconfig $WMW_C_SPACE_CONFIG get rs -n commonstuff commond -o yaml; false; }\n</code></pre> <p>For the special workload, the number of executing copies should be 1. Check that the reported number agrees.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml | grep 'kubestellar.io/executing-count: \"1\"' || { kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml; false; }\n</code></pre> <p>Look at the status section of the \"speciald\" <code>Deployment</code> and see that it has been filled in with the information from the guilder cluster.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml\n</code></pre> <p>Current status might not be there yet. The following command waits for status that reports that there is a special workload pod \"ready\".</p> <pre><code>let count=1\nwhile true; do\nrsyaml=$(kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml)\nif grep 'readyReplicas: 1' &lt;&lt;&lt;\"$rsyaml\"\nthen break\nfi\necho \"\"\necho \"Got:\"\ncat &lt;&lt;&lt;\"$rsyaml\"\nif (( count &gt; 5 )); then\necho 'Giving up!' &gt;&amp;2\nfalse\nfi\nsleep 15\nlet count=count+1\ndone\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#status-summarization-aspirational","title":"Status Summarization (aspirational)","text":"<p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the special workload, creates a status summary object in the specialstuff namespace in the special workload workspace holding a summary of the corresponding Deployment objects. In this case there is just one such object, in the mailbox workspace for the guilder cluster.</p> <p></p> <p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the common workload, creates a status summary object in the commonstuff namespace in the common workload workspace holding a summary of the corresponding Deployment objects.  Those are the <code>commond</code> Deployment objects in the two mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default imw1\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default $FLORIN_SPACE\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default $GUILDER_SPACE\nkubectl kubestellar remove wmw wmw-c\nkubectl kubestellar remove wmw wmw-s\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-kubeflex/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/","title":"Extended Example","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/example1.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre> <p>This doc shows a detailed example usage of the KubeStellar components.</p> <p>This example involves two edge clusters and two workloads.  One workload goes on both edge clusters and one workload goes on only one edge cluster.  Nothing changes after the initial activity.</p> <p>When deploying the KubeStellar core as Kubernetes workload there is a third <code>kind</code> cluster involved. This usually tips <code>kind</code> over an edge and you need to prepare for that by following the instructions for that on the kind website.</p> <p>This example is presented in stages.  The controllers involved are always maintaining relationships.  This document focuses on changes as they appear in this example.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-1","title":"Stage 1","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox spaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#your-kubeconfig-files","title":"Your kubeconfig files","text":"<p>This example (and others) involve user-managed kubeconfig files. These examples organize those files into one subdirectory. Start by making sure that directory exists without any contents left over from previous runs.</p> <pre><code>MY_KUBECONFIGS=${PWD}/my-kubeconfigs\nrm -rf \"$MY_KUBECONFIGS\"\nmkdir -p \"$MY_KUBECONFIGS\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-kind-cluster-for-space-management","title":"Create Kind cluster for space management","text":"<pre><code>kind create cluster --name sm-mgt --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 9443\n    protocol: TCP\nEOF\nkubectl create -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\nsleep 20\nkubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=360s\n\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-sm-mgt sm-mgt\nSM_CONFIG=~/.kube/config\n</code></pre> <p>The subsequent uses of <code>$SM_CONFIG</code> in this example assume that the current context is still the one just established, \"sm-mgt\".</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#the-space-manager-controller","title":"The space-manager controller","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <p><pre><code>cd space-framework\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> Next deploy the space framework CRDs in the space management cluster. <pre><code>KUBECONFIG=$SM_CONFIG kubectl apply -f config/crds/\ncd ..\n</code></pre> Finally, start the space-manager controller.</p> <pre><code>space-manager --kubeconfig $SM_CONFIG --context sm-mgt -v 4 &amp;&gt; /tmp/space-manager.log &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#deploy-kcp-and-kubestellar","title":"Deploy kcp and KubeStellar","text":"<p>You need kcp and KubeStellar and can deploy them in either of two ways: as bare processes on whatever host you are using to run this example, or as workload in a Kubernetes cluster (an OpenShift cluster qualifies).  Do one or the other, not both.</p> <p>KubeStellar only works with release <code>v0.11.0</code> of kcp.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.  See the start of the kcp quickstart for instructions on that, but get release v0.11.0 rather than the latest (the downloadable assets appear after the long list of changes/features/etc).</p> <p>Clone the v0.11.0 branch of the kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> and build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Running the kcp server creates a hidden subdirectory named <code>.kcp</code> to hold all sorts of state related to the server. If you have run it before and want to start over from scratch then you should <code>rm -rf .kcp</code> first.</p> <p>Use the following commands to: (a) run the kcp server in a forked command, (b) update your <code>KUBECONFIG</code> environment variable to configure <code>kubectl</code> to use the kubeconfig produced by the kcp server, and (c) wait for the kcp server to get through some initialization. The choice of <code>-v=3</code> for the kcp server makes it log a line for every HTTP request (among other things).</p> <pre><code>kcp start -v=3 &amp;&gt; /tmp/kcp.log &amp;\nPROVIDER_KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\n# wait until KCP is ready checking availability of ws resource\nwhile ! KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubectl ws tree &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that the kcp server creates. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-a-space-provider-description-for-kcp","title":"Create a space provider description for KCP","text":"<p>Space provider for KCP will allow you to use KCP as backend provider for spaces. Use the following commands to create a provider secret for KCP access and a space provider definition.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl create secret generic kcpsec --from-file=kubeconfig=$PROVIDER_KUBECONFIG --from-file=incluster=$PROVIDER_KUBECONFIG\nKUBECONFIG=$SM_CONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: space.kubestellar.io/v1alpha1\nkind: SpaceProviderDesc\nmetadata:\n  name: default\nspec:\n  ProviderType: \"kcp\"\n  SpacePrefixForDiscovery: \"ks-\"\n  secretRef:\n    namespace: default\n    name: kcpsec\nEOF\n</code></pre> <p>Next, use the following command to wait for the space-manger to process the provider.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl wait --for=jsonpath='{.status.Phase}'=Ready spaceproviderdesc/default --timeout=90s\n</code></pre> <p>The following variable will be used in later commands to indicate that they are being invoked close enough to the provider's apiserver to use the more efficient networking (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"--in-cluster\"\nkube_needed=true\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#get-binaries-of-kube-bind-and-dex","title":"Get binaries of kube-bind and dex","text":"<p>The command below makes kube-bind binaries and dex binary available in <code>$PATH</code>.</p> <pre><code>rm -rf kube-bind\ngit clone https://github.com/waltforme/kube-bind.git &amp;&amp; \\\npushd kube-bind &amp;&amp; \\\ngit checkout kubestellar &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd &amp;&amp; \\\ngit clone https://github.com/dexidp/dex.git &amp;&amp; \\\npushd dex &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build &amp;&amp; \\\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the KubeStellar Core Space (KCS) (formerly called the Edge Service Provider Workspace (ESPW)), which exports the KubeStellar API.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar -X init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code> and the current context is for the KubeStellar hosting cluster.</p> <p>The following variable will be used in later commands to indicate that they are not being invoked from within the hosting cluster (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>IMW1_KUBECONFIG=\"${MY_KUBECONFIGS}/imw1.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig imw1 --kubeconfig $SM_CONFIG $in_cluster $IMW1_KUBECONFIG\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"describe the florin location object\"\nKUBECONFIG=$IMW1_KUBECONFIG kubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two <code>ensure location</code> calls are equivalent to creating the following four objects.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>KUBECONFIG=$SM_CONFIG mailbox-controller --external-access -v=4 &amp;&gt; /tmp/mailbox-controller.log &amp;\nsleep 20\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox spaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox spaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>while [ $(KUBECONFIG=$SM_CONFIG kubectl get spaces -A | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into /tmp/mailbox-controller.log if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI1218 16:36:14.743434  239027 controller.go:277] \"Created missing space\" worker=0 mbsName=\"imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\"\n...\nI1218 16:36:14.759426  239027 controller.go:277] \"Created missing space\" worker=1 mbsName=\"imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\"\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox spaces.</p> <p>A mailbox space name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox spaces as follows.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <pre><code>NAMESPACE               NAME                                           AGE\nspaceprovider-default   espw                                           84s\nspaceprovider-default   imw1                                           76s\nspaceprovider-default   imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   20s\nspaceprovider-default   imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   20s\nspaceprovider-default   wmw1                                           53s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -n spaceprovider-default -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                           SYNCTARGET   CLUSTER\nespw                                           &lt;none&gt;       &lt;none&gt;\nimw1                                           &lt;none&gt;       &lt;none&gt;\nimw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   florin       &lt;none&gt;\nimw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   guilder      &lt;none&gt;\nwmw1                                           &lt;none&gt;       &lt;none&gt;\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox space by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox space name is $GUILDER_SPACE\n</code></pre> <pre><code>The guilder mailbox space name is imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\n</code></pre></p> <p><pre><code>FLORIN_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox space name is $FLORIN_SPACE\n</code></pre> <pre><code>The florin mailbox space name is imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-guilder-edge-cluster-with-its-mailbox-space","title":"Connect guilder edge cluster with its mailbox space","text":"<p>The following command will (a) create, in the mailbox space for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster guilder\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\".\n\nWrote workload execution cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\n\nto verify the syncer pod is running.\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system                                           coredns                                               2/2     2            2           4m1s\nkubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   1/1     1            1           0s\nlocal-path-storage                                    local-path-provisioner                                1/1     1            1           3m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-florin-edge-cluster-with-its-mailbox-space","title":"Connect florin edge cluster with its mailbox space","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster florin\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\".\n\nWrote workload execution cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\n\nto verify the syncer pod is running.\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nserviceaccount/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\ndeployment.apps/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.  These commands create the WMW, makes KubeStellar edge APIs available for the WMW via kube-bind, and optionally makes some Kubernetes APIs available for the WMW by populating a set of CRDs into the WMW.</p> <pre><code># TODO: Needs to pass --with-kube false or true\nWMW_C_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-c.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-c --with-kube $kube_needed --output-kubeconfig \"$WMW_C_SPACE_CONFIG\"\nsleep 15\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code># TODO: needs to set --with-kube false/true\nWMW_S_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-s.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-s --with-kube $kube_needed --output-kubeconfig \"$WMW_S_SPACE_CONFIG\"\n</code></pre> <p>In this workload we will also demonstrate how to downsync objects whose kind is defined by a <code>CustomResourceDefinition</code> object. We will use the one from the Kubernetes documentation for CRDs, modified so that the resource it defines is in the category <code>all</code>. First, create the definition object with the following command.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;\n  name: crontabs.stable.example.com\nspec:\n  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;\n  group: stable.example.com\n  # list of versions supported by this CustomResourceDefinition\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                cronSpec:\n                  type: string\n                image:\n                  type: string\n                replicas:\n                  type: integer\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;\n    plural: crontabs\n    # singular name to be used as an alias on the CLI and for display\n    singular: crontab\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: CronTab\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - ct\n    categories:\n    - all\nEOF\n</code></pre> <p>Next, use the following command to wait for the apiserver to process that definition.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG wait --for condition=Established crd crontabs.stable.example.com\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n  annotations: {just-for: fun}\n---\napiVersion: \"stable.example.com/v1\"\nkind: CronTab\nmetadata:\n  name: my-new-cron-object\n  namespace: specialstuff\nspec:\n  cronSpec: \"* * * * */5\"\n  image: my-awesome-cron-image\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p>The \"what predicate\" explicitly includes the <code>Namespace</code> object named \"specialstuff\", which causes all of its desired state (including labels and annotations) to be downsynced. This contrasts with the common EdgePlacement, which does not explicitly mention the <code>commonstuff</code> namespace, relying on the implicit creation of namespaces as needed in the WECs.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    objectNames: [ v1090.example.my ]\n  - apiGroup: stable.example.com\n    resources: [ crontabs ]\n    namespaces: [ specialstuff ]\n    objectNames: [ my-new-cron-object ]\n  - apiGroup: \"\"\n    resources: [ namespaces ]\n    objectNames: [ specialstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar-where-resolver --external-access -v=4 &amp;&gt; /tmp/where-resolver.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>while ! kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nwhile ! kubectl --kubeconfig $WMW_S_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I1219 16:40:49.814564  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-florin\" location=\"kube-bind-sx6pl-florin\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n...\nI1219 16:40:49.797689  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-guilder\" location=\"kube-bind-sx6pl-guilder\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox spaces and create <code>SyncerConfig</code> objects there.</p> <p>If you have deployed the KubeStellar core as workload in a Kubernetes cluster then the placement translator is running in a Pod there. If instead you are running the core controllers as bare processes then use the following commands to launch the placement translator; it requires the ESPW to be current at start time.</p> <pre><code>KUBECONFIG=$SM_CONFIG placement-translator --external-access -v=4  &amp;&gt; /tmp/placement-translator.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait for the placement translator to get its job done for this example.</p> <pre><code>FLORIN_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${FLORIN_SPACE}.kubeconfig\"\nGUILDER_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${GUILDER_SPACE}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $FLORIN_SPACE --kubeconfig $SM_CONFIG $in_cluster $FLORIN_MB_KUBECONFIG\nkubectl-kubestellar-space-get_kubeconfig $GUILDER_SPACE --kubeconfig $SM_CONFIG $in_cluster $GUILDER_MB_KUBECONFIG\n# wait until SyncerConfig, ReplicaSets and Deployments are ready\nmbxws=($FLORIN_SPACE $GUILDER_SPACE)\nfor ii in \"${mbxws[@]}\"; do\n(\nexport KUBECONFIG=\"${MY_KUBECONFIGS}/${ii}.kubeconfig\"\n# wait for SyncerConfig resource\nwhile ! kubectl get SyncerConfig the-one &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* SyncerConfig resource exists in mailbox $ii\"\n# wait for ReplicaSet resource\nwhile ! kubectl get rs &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* ReplicaSet resource exists in mailbox $ii\"\n# wait until ReplicaSet in mailbox\nwhile ! kubectl get rs -n commonstuff commond; do\nsleep 10\ndone\necho \"* commonstuff ReplicaSet in mailbox $ii\"\n)\ndone\n(\nexport KUBECONFIG=$GUILDER_MB_KUBECONFIG\n# check for deployment in guilder\nwhile ! kubectl get deploy -A &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* Deployment resource exists\"\nwhile ! kubectl get deploy -n specialstuff speciald; do\nsleep 10\ndone\necho \"* specialstuff Deployment in its mailbox\"\n# wait for crontab CRD to be established\nwhile ! kubectl get crd crontabs.stable.example.com; do sleep 10; done\nkubectl wait --for condition=Established crd crontabs.stable.example.com\n  echo \"* CronTab CRD is established in its mailbox\"\n# wait for my-new-cron-object to be in its mailbox\nwhile ! kubectl get ct -n specialstuff my-new-cron-object; do sleep 10; done\necho \"* CronTab my-new-cron-object is in its mailbox\"\n)\n</code></pre> <p>You can check that the common workload's ReplicaSet objects got to their mailbox spaces with the following command. It will list the two copies of that object, each with an annotation whose key is <code>kcp.io/cluster</code> and whose value is the kcp <code>logicalcluster.Name</code> of the mailbox space; those names appear in the \"CLUSTER\" column of the custom-columns listing near the end of the section above about the mailbox controller.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1y7wll1dz806h3sb\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1najcltzt2nqax47\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n</code></pre> <p>That display should show objects in two different mailbox spaces; the following command checks that.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet | grep \"^ *kcp.io/cluster: [0-9a-z]*$\" | sort | uniq | wc -l) -ge 2\n</code></pre> <p>The CustomResourceDefinition objects involved should also appear in the mailbox spaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | fgrep -w \"name: crontabs.stable.example.com\"\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: replicasets.apps\") -ge 2\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: deployments.apps\") -ge 1\n</code></pre> <p>The <code>APIService</code> of the special workload should also appear, along with some error messages about <code>APIService</code> not being known in the other mailbox workspaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiregistration.k8s.io --api-kind APIService 2&gt;&amp;1 | grep -v \"APIService.*the server could not find the requested resource\" | fgrep -w \"name: v1090.example.my\"\n</code></pre> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize the name of the mailbox workspace for florin (which you stored in Stage 1) here.</p> <pre><code>KUBECONFIG=$FLORIN_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    resource: configmaps\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the mailbox workspace name that you stored in Stage 1.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  clusterScope:\n  - apiVersion: v1\n    group: \"\"\nobjects:\n    - specialstuff\n    resource: namespaces\n  - apiVersion: v1\n    group: apiextensions.k8s.io\n    objects:\n    - crontabs.stable.example.com\n    resource: customresourcedefinitions\n  - apiVersion: v1\n    group: apiregistration.k8s.io\n    objects:\n    - v1090.example.my\n    resource: apiservices\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  - apiVersion: v1\n    group: stable.example.com\n    objectsByNamespace:\n    - names:\n      - my-new-cron-object\n      namespace: specialstuff\n    resource: crontabs\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - speciald\n      namespace: specialstuff\n    resource: deployments\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    - names:\n      - httpd-htdocs\n      namespace: specialstuff\n    resource: configmaps\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p>You can check for specific workload objects here with the following command.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get deployments,replicasets -A\n</code></pre> <pre><code>NAMESPACE      NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff   deployment.apps/speciald   0/0     1            0           12m\n\nNAMESPACE     NAME                      DESIRED   CURRENT   READY   AGE\ncommonstuff   replicaset.apps/commond   0         1         1       7m4s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>( KUBECONFIG=~/.kube/config\n  let tries=1\nwhile ! kubectl --context kind-florin get ns commonstuff &amp;&gt; /dev/null; do\nif (( tries &gt;= 30)); then\necho 'The commonstuff namespace failed to appear in florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n    sleep 10\ndone\nkubectl --context kind-florin get ns\n)\nsleep 15\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy,rs -A | egrep 'NAME|stuff'\nsleep 15\n</code></pre> <pre><code>NAMESPACE                            NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\nNAMESPACE                            NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                          replicaset.apps/commond                                         1         1         1       13m\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces, the Deployment, and both ReplicaSets.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy,rs -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff                          deployment.apps/speciald                              1/1     1            1           23m\nNAMESPACE                             NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                           replicaset.apps/commond                                         1         1         1       23m\nspecialstuff                          replicaset.apps/speciald-76cdbb69b5                             1         1         1       14s\n</code></pre></p> <p>Examine the <code>APIService</code> objects in the guilder cluster, find the one named <code>v1090.example.my</code>. It is broken because it refers to a Service object that we have not bothered to create.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get apiservices | grep 1090\n</code></pre> <pre><code>v1090.example.my                       my-example/my-service   False (ServiceNotFound)   2m39s\n</code></pre></p> <p>See the crontab in the guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get crontabs -n specialstuff\nsleep 15\n</code></pre> <pre><code>NAME                 AGE\nmy-new-cron-object   37m\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get rs -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8094 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>let tries=1\nwhile ! curl http://localhost:8097 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The special workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8096 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-5","title":"Stage 5","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#singleton-reported-state-return","title":"Singleton reported state return","text":"<p>The two <code>EdgePlacement</code> objects above assert that the expected number of executing copies of their matching workload objects is 1 and request return of reported state to the WDS when the number of executing copies is exactly 1.</p> <p>For the common workload, that assertion is not correct: the number of executing copies should be 2.  The assertion causes the actual number of executing copies to be reported.  Check that the reported number is 2.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG get rs -n commonstuff commond -o yaml | grep 'kubestellar.io/executing-count: \"2\"' || { kubectl --kubeconfig $WMW_C_SPACE_CONFIG get rs -n commonstuff commond -o yaml; false; }\n</code></pre> <p>For the special workload, the number of executing copies should be 1. Check that the reported number agrees.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml | grep 'kubestellar.io/executing-count: \"1\"' || { kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml; false; }\n</code></pre> <p>Look at the status section of the \"speciald\" <code>Deployment</code> and see that it has been filled in with the information from the guilder cluster.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml\n</code></pre> <p>Current status might not be there yet. The following command waits for status that reports that there is a special workload pod \"ready\".</p> <pre><code>let count=1\nwhile true; do\nrsyaml=$(kubectl --kubeconfig $WMW_S_SPACE_CONFIG get deploy -n specialstuff speciald -o yaml)\nif grep 'readyReplicas: 1' &lt;&lt;&lt;\"$rsyaml\"\nthen break\nfi\necho \"\"\necho \"Got:\"\ncat &lt;&lt;&lt;\"$rsyaml\"\nif (( count &gt; 5 )); then\necho 'Giving up!' &gt;&amp;2\nfalse\nfi\nsleep 15\nlet count=count+1\ndone\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#status-summarization-aspirational","title":"Status Summarization (aspirational)","text":"<p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the special workload, creates a status summary object in the specialstuff namespace in the special workload workspace holding a summary of the corresponding Deployment objects. In this case there is just one such object, in the mailbox workspace for the guilder cluster.</p> <p></p> <p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the common workload, creates a status summary object in the commonstuff namespace in the common workload workspace holding a summary of the corresponding Deployment objects.  Those are the <code>commond</code> Deployment objects in the two mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default imw1\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default $FLORIN_SPACE\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default $GUILDER_SPACE\nkubectl kubestellar remove wmw wmw-c\nkubectl kubestellar remove wmw wmw-s\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/","title":"KubeStellar-Syncer","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/kubestellar-syncer.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre> <p>KubeStellar-Syncer runs in the target cluster and sync kubernetes resource objects from the target cluster to a mailbox workspace and vice versa.</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#steps-to-try-the-syncer","title":"Steps to try the Syncer","text":"<p>The KubeStellar-Syncer can be exercised after setting up KubeStellar mailbox workspaces. Firstly we'll follow to similar steps in example1 until <code>The mailbox controller</code> in stage 2. </p> <p></p> <p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox spaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#your-kubeconfig-files","title":"Your kubeconfig files","text":"<p>This example (and others) involve user-managed kubeconfig files. These examples organize those files into one subdirectory. Start by making sure that directory exists without any contents left over from previous runs.</p> <pre><code>MY_KUBECONFIGS=${PWD}/my-kubeconfigs\nrm -rf \"$MY_KUBECONFIGS\"\nmkdir -p \"$MY_KUBECONFIGS\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-kind-cluster-for-space-management","title":"Create Kind cluster for space management","text":"<pre><code>kind create cluster --name sm-mgt --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 9443\n    protocol: TCP\nEOF\nkubectl create -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\nsleep 20\nkubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=360s\n\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-sm-mgt sm-mgt\nSM_CONFIG=~/.kube/config\n</code></pre> <p>The subsequent uses of <code>$SM_CONFIG</code> in this example assume that the current context is still the one just established, \"sm-mgt\".</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#the-space-manager-controller","title":"The space-manager controller","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <p><pre><code>cd space-framework\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> Next deploy the space framework CRDs in the space management cluster. <pre><code>KUBECONFIG=$SM_CONFIG kubectl apply -f config/crds/\ncd ..\n</code></pre> Finally, start the space-manager controller.</p> <pre><code>space-manager --kubeconfig $SM_CONFIG --context sm-mgt -v 4 &amp;&gt; /tmp/space-manager.log &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#get-binaries-of-kube-bind-and-dex","title":"Get binaries of kube-bind and dex","text":"<p>The command below makes kube-bind binaries and dex binary available in <code>$PATH</code>.</p> <pre><code>rm -rf kube-bind\ngit clone https://github.com/waltforme/kube-bind.git &amp;&amp; \\\npushd kube-bind &amp;&amp; \\\ngit checkout kubestellar &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd &amp;&amp; \\\ngit clone https://github.com/dexidp/dex.git &amp;&amp; \\\npushd dex &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build &amp;&amp; \\\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the KubeStellar Core Space (KCS) (formerly called the Edge Service Provider Workspace (ESPW)), which exports the KubeStellar API.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar -X init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code> and the current context is for the KubeStellar hosting cluster.</p> <p>The following variable will be used in later commands to indicate that they are not being invoked from within the hosting cluster (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>IMW1_KUBECONFIG=\"${MY_KUBECONFIGS}/imw1.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig imw1 --kubeconfig $SM_CONFIG $in_cluster $IMW1_KUBECONFIG\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"describe the florin location object\"\nKUBECONFIG=$IMW1_KUBECONFIG kubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two <code>ensure location</code> calls are equivalent to creating the following four objects.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>KUBECONFIG=$SM_CONFIG mailbox-controller --external-access -v=4 &amp;&gt; /tmp/mailbox-controller.log &amp;\nsleep 20\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox spaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox spaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>while [ $(KUBECONFIG=$SM_CONFIG kubectl get spaces -A | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into /tmp/mailbox-controller.log if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI1218 16:36:14.743434  239027 controller.go:277] \"Created missing space\" worker=0 mbsName=\"imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\"\n...\nI1218 16:36:14.759426  239027 controller.go:277] \"Created missing space\" worker=1 mbsName=\"imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\"\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox spaces.</p> <p>A mailbox space name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox spaces as follows.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <pre><code>NAMESPACE               NAME                                           AGE\nspaceprovider-default   espw                                           84s\nspaceprovider-default   imw1                                           76s\nspaceprovider-default   imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   20s\nspaceprovider-default   imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   20s\nspaceprovider-default   wmw1                                           53s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -n spaceprovider-default -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                           SYNCTARGET   CLUSTER\nespw                                           &lt;none&gt;       &lt;none&gt;\nimw1                                           &lt;none&gt;       &lt;none&gt;\nimw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   florin       &lt;none&gt;\nimw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   guilder      &lt;none&gt;\nwmw1                                           &lt;none&gt;       &lt;none&gt;\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox space by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox space name is $GUILDER_SPACE\n</code></pre> <pre><code>The guilder mailbox space name is imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\n</code></pre></p> <p><pre><code>FLORIN_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox space name is $FLORIN_SPACE\n</code></pre> <pre><code>The florin mailbox space name is imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#register-kubestellar-syncer-on-the-target-clusters","title":"Register KubeStellar-Syncer on the target clusters","text":"<p>Once KubeStellar setup is done, KubeStellar-Syncer can be deployed on the target cluster easily by the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#for-the-target-cluster-of-guilder","title":"For the target cluster of <code>guilder</code>,","text":"<p>Go to KCS and find the mailbox space name. <pre><code>espw_kubeconfig=\"${MY_KUBECONFIGS}/espw.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig espw $in_cluster --kubeconfig $SM_CONFIG $espw_kubeconfig\npvname=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io | grep guilder | awk '{print $1}'`\nstuid=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io $pvname -o jsonpath=\"{.metadata.uid}\"`\nmbs_name=\"imw1-mb-$stuid\"\necho \"mailbox space name = $mbs_name\"\n</code></pre></p> <pre><code>mailbox space name = vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\n</code></pre> <p>Go to the mailbox space and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>mbs_kubeconfig=\"${MY_KUBECONFIGS}/${mbs_name}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig ${mbs_name} $in_cluster --kubeconfig $SM_CONFIG $mbs_kubeconfig\n./bin/kubectl-kubestellar-syncer_gen --kubeconfig $mbs_kubeconfig guilder --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o guilder-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\".\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                           coredns                               2/2     2            2           35m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#for-the-target-cluster-of-florin","title":"For the target cluster of <code>florin</code>,","text":"<p>Go to KCS and find the mailbox space name. <pre><code>pvname=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io | grep florin | awk '{print $1}'`\nstuid=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io $pvname -o jsonpath=\"{.metadata.uid}\"`\nmbs_name=\"imw1-mb-$stuid\"\necho \"mailbox space name = $mbs_name\"\n</code></pre> <pre><code>mailbox space name = vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\n</code></pre></p> <p>Go to the mailbox space and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>mbs_kubeconfig=\"${MY_KUBECONFIGS}/${mbs_name}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig ${mbs_name} $in_cluster --kubeconfig $SM_CONFIG $mbs_kubeconfig\n./bin/kubectl-kubestellar-syncer_gen --kubeconfig $mbs_kubeconfig florin --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o florin-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\".\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote workload execution cluster (WEC) manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-florin-32uaph9l    kubestellar-syncer-florin-32uaph9l    1/1     1            1           42s\nkube-system                           coredns                               2/2     2            2           41m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           41m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and the space provider with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is the space provider and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deep-dive","title":"Deep-dive","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#the-details-about-the-registration-of-kubestellar-syncer-on-an-edge-cluster-and-a-workspace","title":"The details about the registration of KubeStellar-Syncer on an Edge cluster and a workspace","text":"<p>KubeStellar-Syncer is deployed on an Edge cluster easily by the following steps.</p> <ol> <li>Create SyncTarget and Location<ul> <li>Mailbox controller creates the mailbox workspace automatically </li> </ul> </li> <li>Get the mailbox workspace name</li> <li>Use the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer     <pre><code>kubectl ws &lt;mb-ws name&gt;\nbin/kubectl-kubestellar-syncer_gen &lt;Edge Sync Target name&gt; --syncer-image &lt;KubeStellar-Syncer image&gt; -o kubestellar-syncer.yaml\n</code></pre>     Here, <code>bin/kubectl-kubestellar-syncer_gen</code> refers to a special variant of KubeStellar's     kubectl plugin that includes the implementation of the functionality needed     here.  This variant, under the special name shown here, is a normal part of     the <code>bin</code> of KubeStellar.     For the KubeStellar-Syncer image, please select an official image from https://quay.io/repository/kubestellar/syncer?tab=tags. For example, <code>--syncer-image quay.io/kubestellar/syncer:git-08289ea05-clean</code>. You can also create a syncer image from the source following Build KubeStellar-Syncer Image.</li> <li>Deploy KubeStellar-Syncer on an Edge cluster</li> <li>Syncer starts to run on the Edge cluster<ul> <li>KubeStellar-Syncer starts watching and consuming SyncerConfig</li> </ul> </li> </ol> <p>The overall diagram is as follows:</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#what-kubestellar-syncer-gen-plugin-does","title":"What KubeStellar syncer-gen plugin does","text":"<p>In order for Syncer to sync resources between upstream (workspace) and downstream (workload execution cluster), access information for both is required. </p> <p>For the upstream access, Syncer's registration command (<code>kubectl kubestellar syncer-gen</code>) creates a ServiceAccount, ClusterRole, and ClusterRoleBinding in the workspace, and then generates a kubeconfig manifest from the ServiceAccount token, KCP server URL, and the server certificates. The kubeconfig manifest is embedded in a secret manifest and the secret is mounted to <code>/kcp/</code> in Syncer pod. The command generates such deployment manifest as Syncer reads <code>/kcp/</code> for the upstream Kubeconfig. </p> <p>On the other hand, for the downstream part, in addition to the deployment manifest, the command generates a ServiceAccount, Role/ClusterRole, RoleBinding/ClusterRoleBinding for Syncer to access resources on the WEC. These resources for the downstream part are the resources to be deployed to the downstream cluster. The ServiceAccount is set to <code>serviceAccountName</code> in the deployment manifest.</p> <p>Note: In addition to that, the command creates an EdgeSyncConfig CRD if it does not exist, and creates a default EdgeSyncConfig resource with the name specified in the command (<code>kubectl kubestellar syncer-gen &lt;name&gt;</code>). The default EdgeSyncConfig is no longer needed since Syncer now consumes all EdgeSyncConfigs in the workspace. Furthermore, creation of the EdgeSyncConfig CRD will also no longer be needed since we will switch to using SyncerConfig rather than EdgeSyncConfig in near future.</p> <p>The source code of the command is https://github.com/kubestellar/kubestellar/blob/main/pkg/cliplugins/kubestellar/syncer-gen/edgesync.go.</p> <p>The equivalent manual steps are as follows: </p> <p>The remainder is not fully debugged.</p> <pre><code>exit 0\n</code></pre> <p>Generate UUID for Syncer identification. <pre><code>syncer_id=\"syncer-\"`uuidgen | tr '[:upper:]' '[:lower:]'`\n</code></pre></p> <p>Go to a workspace. <pre><code>kubectl ws root\nkubectl ws create ws1 --enter\n</code></pre></p> <p>Create the following APIBinding in the workspace (Note that in the case of mailbox workspaces, it's done by mailbox controller at creating the mailbox workspace.) <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: bind-espw\nspec:\n  reference:\n    export:\n      path: root:espw\n      name: edge.kubestellar.io\nEOL\n</code></pre></p> <p>Create a serviceaccount in the workspace. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: $syncer_id\nEOL\n</code></pre></p> <p>Create clusterrole and clusterrolebinding to bind the serviceaccount to the role. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: $syncer_id\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- nonResourceURLs: [\"/\"]\n  verbs: [\"access\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: $syncer_id\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: $syncer_id\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: $syncer_id\n  namespace: default\nEOL\n</code></pre></p> <p>Get the serviceaccount token that will be set in the upstream kubeconfig manifest.  NOTE This part is outdated due to the recent denaturing of ServiceAccounts in kcp workspaces; the syncer-gen plugin will actually wait a little while and then create the Secret if nothing else has. <pre><code>secret_name=`kubectl get secret -o custom-columns=\":.metadata.name\"| grep $syncer_id`\ntoken=`kubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d`\n</code></pre></p> <p>Get the certificates that will be set in the upstream kubeconfig manifest. <pre><code>cacrt=`kubectl config view --minify --raw | yq \".clusters[0].cluster.certificate-authority-data\"`\n</code></pre></p> <p>Get server_url that will be set in the upstream kubeconfig manifest. <pre><code>server_url=`kubectl config view --minify --raw | yq \".clusters[0].cluster.server\" | sed -e 's|https://\\(.*\\):\\([^/]*\\)/.*|https://\\1:\\2|g'`\n</code></pre></p> <p>Set some other parameters. a. downstream_namespace where Syncer Pod runs <pre><code>downstream_namespace=\"kubestellar-$syncer_id\"\n</code></pre> b. Syncer image <pre><code>image=\"quay.io/kubestellar/syncer:v0.2.2\"\n</code></pre></p> <p>Download manifest template. <pre><code>curl -LO https://raw.githubusercontent.com/kubestellar/kubestellar/main/pkg/syncer/scripts/kubestellar-syncer-bootstrap.template.yaml\n</code></pre></p> <p>Generate manifests to bootstrap KubeStellar-Syncer. <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image envsubst &lt; kubestellar-syncer-bootstrap.template.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubestellar-syncer-9ee90de6-eb76-4ddb-9346-c4c8d92075e1\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n...\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deploy-workload-objects-from-kubestellar-to-edge-cluster","title":"Deploy workload objects from KubeStellar to Edge cluster","text":"<p>To deploy resources to Edge clusters, create the following in workload management workspace - workload objects   - Some objects are denatured if needed.   - Other objects are as it is - APIExport/API Schema corresponding to CRD such as Kubernetes ClusterPolicyReport.   - TBD: Conversion from CRD to APIExport/APISchema could be automated by using MutatingAdmissionWebhook on workload management workspace. This automation is already available (see the script here).  - EdgePlacement</p> <p></p> <p>After this, KubeStellar will put the following in the mailbox workspace. - Workload objects (both denatured one and not-denatured one) - SyncerConfig CR</p> <p>TODO: This is something we should clarify..e.g. which existing controller(s) in KubeStellar will cover, or just create a new controller to handle uncovered one. @MikeSpreitzer gave the following suggestions.   - The placement translator will put the workload objects and syncer config into the mailbox workspaces.   - The placement translator will create syncer config based on the EdgePlacement objects and what they match.   - The mailbox controller will put API Binding into the mailbox workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#edgesyncconfig-will-be-replaced-to-syncerconfig","title":"EdgeSyncConfig (will be replaced to SyncerConfig)","text":"<ul> <li>The example of EdgeSyncConfig CR is here. Its CRD is here.</li> <li>The CR here is used from edge syncer. </li> <li>The CR is placed at mb-ws to define</li> <li>object selector</li> <li>need of renaturing</li> <li>need of returning reported states of downsynced objects</li> <li>need of delete propagation for downsyncing</li> <li>The CR is managed by KubeStellar (placement transformer).</li> <li>At the initial implementation before KubeStellar side controller become ready, we assume SyncerConfig is on workload management workspace (WDS), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how KubeStellar-Syncer behaves, and will be extended/split/merged according to further design discussion.</li> <li>One CR is initially created by the command for KubeStellar-Syncer enablement in mb-ws (<code>kubectl kubestellar syncer-gen &lt;name&gt;</code>)</li> <li>The CR name is <code>&lt;name&gt;</code> and the contents are empty.</li> <li>This name is registered in the bootstrap manifests for KubeStellar-Syncer install and KubeStellar-Syncer is told to watch the CR of this name.</li> <li>Currently KubeStellar-Syncer watches all CRs in the workspace</li> <li>KubeStellar-Syncer merges them and decides which resources are down/up synced based on the merged information. </li> <li>This behavior may be changed to only watching the default CR once Placement Translator is to be the component that generates the CR from EdgePlacement: related issue</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#syncerconfig","title":"SyncerConfig","text":"<ul> <li>The spec is defined in https://github.com/kubestellar/kubestellar/blob/main/pkg/apis/edge/v2alpha1/syncer-config.go</li> <li><code>namespaceScope</code> field is for namespace scoped objects.<ul> <li><code>namespaces</code> is field for which namespaces to be downsynced.</li> <li><code>resources</code> is field for what resource's objects in the above namespaces are downsynced. All objects in the selected resource are downsynced.</li> </ul> </li> <li><code>clusterScope</code> field is for cluster scoped objects<ul> <li>It's an array of <code>apiVersion</code>, <code>group</code>, <code>resource</code>, and <code>objects</code>.</li> <li><code>objects</code> can be specified by wildcard (<code>*</code>) meaning all objects.</li> </ul> </li> <li><code>upsync</code> field is for upsynced objects including both namespace and cluster scoped objects.<ul> <li>It's an array of <code>apiGroup</code>, <code>resources</code>, <code>namespaces</code>, and <code>names</code>.</li> <li><code>apiGroup</code> is group.</li> <li><code>resources</code> is an array of upsynced resource.</li> <li><code>namespaces</code> is an array of namespace for namespace objects.</li> <li><code>names</code> is an array of upsynced object name. Wildcard (<code>*</code>) is available.</li> </ul> </li> <li>The example CR is https://github.com/kubestellar/kubestellar/blob/main/test/e2e/kubestellar-syncer/testdata/kyverno/syncer-config.yaml</li> <li>The CR is used from KubeStellar-Syncer</li> <li>The CR is placed in mb-ws to define</li> <li>object selector</li> <li>need of renaturing (May not scope in PoC2023q1)</li> <li>need of returning reported states of downsynced objects (May not scope in PoC2023q1)</li> <li>need of delete propagation for downsyncing (May not scope in PoC2023q1)</li> <li>The CR is managed by KubeStellar (placement translator).</li> <li>At the initial implementation before KubeStellar side controller become ready, we assume SyncerConfig is on workload management workspace (WDS), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how KubeStellar-Syncer behaves, and will be extended/split/merged according to further design discussion.</li> <li>Currently KubeStellar-Syncer watches all CRs in the workspace</li> <li>KubeStellar-Syncer merges them and decides which resources are down/up synced based on the merged information. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#downsyncing","title":"Downsyncing","text":"<ul> <li>KubeStellar-Syncer does downsyncing, which copy workload objects on mailbox workspace to Edge cluster</li> <li>If workload objects are deleted on mailbox workspace, the corresponding objects on the Edge cluster will be also deleted according to SyncerConfig. </li> <li>SyncerConfig specifies which objects should be downsynced.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation</li> <li>Cover cluster-scope objects and CRD</li> <li>CRD needs to be denatured if downsyncing is required. (May not scope in PoC2023q1 since no usage)</li> <li>Renaturing is applied if required (specified in SyncerConfig). (May not scope in PoC2023q1 since no usage)</li> <li>Current implementation is using polling to detect changes on mailbox workspace, but will be changed to use Informers. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#renaturing-may-not-scope-in-poc2023q1-since-no-usage","title":"Renaturing (May not scope in PoC2023q1 since no usage)","text":"<ul> <li>KubeStellar-Syncer does renaturing, which converts workload objects to different forms of objects on a Edge cluster. </li> <li>The conversion rules (downstream/upstream mapping) is specified in SyncerConfig.</li> <li>Some objects need to be denatured. </li> <li>CRD needs to be denatured when conflicting with APIBinding.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#return-of-reported-state","title":"Return of reported state","text":"<ul> <li>KubeStellar-Syncer return the reported state of downsynced objects at Edge cluster to the status of objects on the mailbox workspace periodically. </li> <li>TODO: Failing to returning reported state of some resources (e.g. deployment and service). Need more investigation. </li> <li>reported state returning on/off is configurable in SyncerConfig. (default is on)</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#resource-upsyncing","title":"Resource Upsyncing","text":"<ul> <li>KubeStellar-Syncer does upsyncing resources at Edge cluster to the corresponding mailbox workspace periodically. </li> <li>SyncerConfig specifies which objects should be upsynced from Edge cluster.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation (, and more such as ownership reference?)</li> <li>Upsyncing CRD is out of scope for now. This means when upsyncing a CR, corresponding APIBinding (not CRD) is available on the mailbox workspace. This limitation might be revisited later. </li> <li>~Upsynced objects can be accessed from APIExport set on the workload management workspace bound to the mailbox workspace (with APIBinding). This access pattern might be changed when other APIs such as summarization are provided in KubeStellar.~ =&gt; Upsynced objects are accessed through Mailbox informer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#feasibility-study","title":"Feasibility study","text":"<p>We will verify if the design described here could cover the following 4 scenarios.  - I can register a KubeStellar-Syncer on a Edge cluster to connect a mailbox workspace specified by name. (KubeStellar-Syncer registration) - I can deploy Kyverno and its policy from mailbox workspace to Edge cluster just by using manifests (generated from Kyverno helm chart) rather than using OLM. (workload deployment by KubeStellar-Syncer's downsyncing) - I can see the policy report generated at Edge cluster via API Export on workload management workspace. (resource upsyncing by KubeStellar-Syncer)  - I can deploy the denatured objects on mailbox workspace to Edge cluster by renaturing them automatically in KubeStellar-Syncer. (workload deployment by renaturing)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#build-kubestellar-syncer-image","title":"Build KubeStellar-Syncer image","text":"<p>Prerequisite - Install ko (https://ko.build/install/)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-build-the-image-in-your-local","title":"How to build the image in your local","text":"<ol> <li><code>make build-kubestellar-syncer-image-local</code> e.g. <pre><code>$ make build-kubestellar-syncer-image-local\n2023/04/24 11:50:37 Using base distroless.dev/static:latest@sha256:81018475098138883b80dcc9c1242eb02b53465297724b18e88591a752d2a49c for github.com/kubestellar/kubestellar/cmd/syncer\n2023/04/24 11:50:38 Building github.com/kubestellar/kubestellar/cmd/syncer for linux/arm64\n2023/04/24 11:50:39 Loading ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Loaded ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Adding tag latest\n2023/04/24 11:50:40 Added tag latest\nkubestellar-syncer image:\nko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n</code></pre> <code>ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96</code> is the image stored in your local Docker registry.</li> </ol> <p>You can also set a shell variable to the output of this Make task.</p> <p>For example <pre><code>image=`make build-kubestellar-syncer-image-local`\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-build-the-image-with-multiple-architectures-and-push-it-to-docker-registry","title":"How to build the image with multiple architectures and push it to Docker registry","text":"<ol> <li><code>make build-kubestellar-syncer-image</code></li> </ol> <p>The behavior can be modified with some make variables; their default values are what get used in a normal build.  The variables are as follows.</p> <ul> <li><code>DOCKER_REPO</code>, <code>IMAGE_TAG</code>: the built multi-platform manifest will   be pushed to <code>$DOCKER_REPO:$IMAGE_TAG</code>.  The default for   <code>DOCKER_REPO</code> is <code>quay.io/kubestellar/syncer</code>.  The default for   <code>IMAGE_TAG</code> is the concatenation of: \"git-\", a short ID of the   current git commit, \"-\", and either \"clean\" or \"dirty\" depending on   what <code>git status</code> has to say about it. It is STRONGLY recommended to NOT   override <code>IMAGE_TAG</code> unless the override also identifies the git commit   and cleanliness, as this tag is the ONLY place that this is recorded (for the   sake of reproducable builds, the <code>go build</code> command is told to not include   git and time metadata). To add tags, use your container runtime's and/or   image registry's additional functionality.</li> <li><code>SYNCER_PLATFORMS</code>: a   comma-separated list of <code>docker build</code> \"platforms\".  The default is   \"linux/amd64,linux/arm64,linux/s390x\".</li> <li><code>ADDITIONAL_ARGS</code>: a word that will be added into the <code>ko build</code> command line.   The default is the empty string.</li> </ul> <p>For example, <pre><code>$ make build-kubestellar-syncer-image DOCKER_REPO=quay.io/mspreitz/syncer SYNCER_PLATFORMS=linux/amd64,linux/arm64 \n2023/11/06 13:46:15 Using base cgr.dev/chainguard/static:latest@sha256:d3465871ccaba3d4aefe51d6bb2222195850f6734cbbb6ef0dd7a3da49826159 for github.com/kubestellar/kubestellar/cmd/syncer\n2023/11/06 13:46:16 Building github.com/kubestellar/kubestellar/cmd/syncer for linux/amd64\n2023/11/06 13:46:16 Building github.com/kubestellar/kubestellar/cmd/syncer for linux/arm64\n2023/11/06 13:46:43 Publishing quay.io/mspreitz/syncer:git-a4250b7ee-dirty\n2023/11/06 13:46:44 pushed blob: sha256:250c06f7c38e52dc77e5c7586c3e40280dc7ff9bb9007c396e06d96736cf8542\n2023/11/06 13:46:44 pushed blob: sha256:24e67d450bd33966f28c92760ffcb5eae57e75f86ce1c0e0266a5d3c159d1798\n2023/11/06 13:46:44 pushed blob: sha256:cd93f0a485889b13c1b34307d8dde4b989b45b7ebdd1f13a2084c89c87cb2fbf\n2023/11/06 13:46:44 pushed blob: sha256:aa2769d82ae2f06035ceb26ce127c604bc0797f3e9a09bfc0dc010afff25d5c6\n2023/11/06 13:46:44 pushed blob: sha256:9ac3b3732a57658f71e51d440eba76d27be0fac6db083c3e227585d5d7b0be94\n2023/11/06 13:46:44 pushed blob: sha256:512f2474620de277e19ecc783e8e2399f54cb2669873db4b54159ac3c47a1914\n2023/11/06 13:46:44 pushed blob: sha256:f2ae5118c0fadc41f16d463484970c698e9640de5d574b0fd29d4065e6d92795\n2023/11/06 13:46:44 pushed blob: sha256:836fc9b0d92a362f818a04d483219a5254b4819044506b26d2a78c27a49d8421\n2023/11/06 13:46:44 pushed blob: sha256:74256082c076ec34b147fa439ebdafffb10043cb418abe7531c49964cc2e9376\n2023/11/06 13:46:44 quay.io/mspreitz/syncer:sha256-905d0fda05d7f9312c0af44856e9af5004ed6e2369f38b71469761cb3f9da2d1.sbom: digest: sha256:d40e5035236f888f8a1a784c4c630998dd92ee66c1b375bf379f1c915c4f296d size: 374\n2023/11/06 13:46:44 Published SBOM quay.io/mspreitz/syncer:sha256-905d0fda05d7f9312c0af44856e9af5004ed6e2369f38b71469761cb3f9da2d1.sbom\n2023/11/06 13:46:44 quay.io/mspreitz/syncer:sha256-18045f17222f9d0ec4fa3f736eaba891041d2980d1fb8c9f7f0a7a562172c9e5.sbom: digest: sha256:cad30541f2af79b74f87a37d284fa508fefd35cb130ee35745cfe31d85318fe9 size: 373\n2023/11/06 13:46:44 Published SBOM quay.io/mspreitz/syncer:sha256-18045f17222f9d0ec4fa3f736eaba891041d2980d1fb8c9f7f0a7a562172c9e5.sbom\n2023/11/06 13:46:44 quay.io/mspreitz/syncer:sha256-25d71940766653861e3175feec34fd2a6faff4f4c4f7bd55784f035a860d3be2.sbom: digest: sha256:877daabfb8593ce25c377446f9ec07782eb89b1ff15afdf9a2dfe882b7f87b06 size: 374\n2023/11/06 13:46:44 Published SBOM quay.io/mspreitz/syncer:sha256-25d71940766653861e3175feec34fd2a6faff4f4c4f7bd55784f035a860d3be2.sbom\n2023/11/06 13:46:45 pushed blob: sha256:0757eb0b6bd5eb800545762141ea55fae14a3f421aa84ac0414bbf51ffd95509\n2023/11/06 13:46:45 pushed blob: sha256:9b50f69553a78acc0412f1fba1e27553f47a0f1cc76acafaad983320fb4d2edd\n2023/11/06 13:46:54 pushed blob: sha256:15df213e4830817c1a38d97fda67c3e8459c17bc955dc36ac7f2fbdea26a12d4\n2023/11/06 13:46:54 quay.io/mspreitz/syncer@sha256:905d0fda05d7f9312c0af44856e9af5004ed6e2369f38b71469761cb3f9da2d1: digest: sha256:905d0fda05d7f9312c0af44856e9af5004ed6e2369f38b71469761cb3f9da2d1 size: 1211\n2023/11/06 13:46:54 pushed blob: sha256:6144db4c37348e2bdba9e850652e46f260dbab377e4f62d29bcdb84fcceaca00\n2023/11/06 13:46:55 quay.io/mspreitz/syncer@sha256:25d71940766653861e3175feec34fd2a6faff4f4c4f7bd55784f035a860d3be2: digest: sha256:25d71940766653861e3175feec34fd2a6faff4f4c4f7bd55784f035a860d3be2 size: 1211\n2023/11/06 13:46:55 quay.io/mspreitz/syncer:git-a4250b7ee-dirty: digest: sha256:18045f17222f9d0ec4fa3f736eaba891041d2980d1fb8c9f7f0a7a562172c9e5 size: 986\n2023/11/06 13:46:55 Published quay.io/mspreitz/syncer:git-a4250b7ee-dirty@sha256:18045f17222f9d0ec4fa3f736eaba891041d2980d1fb8c9f7f0a7a562172c9e5\necho KO_DOCKER_REPO=quay.io/mspreitz/syncer GOFLAGS=-buildvcs=false ko build --platform=linux/amd64,linux/arm64 --bare --tags git-a4250b7ee-dirty  ./cmd/syncer\nKO_DOCKER_REPO=quay.io/mspreitz/syncer GOFLAGS=-buildvcs=false ko build --platform=linux/amd64,linux/arm64 --bare --tags git-a4250b7ee-dirty ./cmd/syncer\nquay.io/mspreitz/syncer:git-a4250b7ee-dirty@sha256:18045f17222f9d0ec4fa3f736eaba891041d2980d1fb8c9f7f0a7a562172c9e5\n</code></pre></p> <p>The last line of the output shows the full image reference, including both tag and digest of the \"image\" (technically it is a multi-platform manifest).</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-the-environment_1","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-bare-processes_1","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and the space provider with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-kubernetes-workload_1","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is the space provider and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/","title":"KubeStellar Mailbox Controller","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/mailbox-controller.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#linking-synctarget-with-mailbox-workspace","title":"Linking SyncTarget with Mailbox Workspace","text":"<p>For a given SyncTarget T, the mailbox controller currently chooses the name of the corresponding workspace to be the concatenation of the following:</p> <ul> <li>the ID of the logical cluster containing T</li> <li>the string \"-mb-\"</li> <li>T's UID</li> </ul> <p>The mailbox workspace gets an annotation whose key is <code>edge.kubestellar.io/sync-target-name</code> and whose value is the name of the workspace object (as seen in its parent workspace, the edge service provider workspace).</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#usage","title":"Usage","text":"<p>The mailbox controller needs three Kubernetes client configurations. One --- concerned with reading inventory --- is to access the APIExport view of the <code>edge.kubestellar.io</code> API group, to read the <code>SyncTarget</code> objects.  This must be a client config that is pointed at the workspace (which is always <code>root:espw</code>, as far as I know) that has this APIExport and is authorized to read its view.  Another client config is needed to give read/write access to all the mailbox workspaces, so that the controller can create <code>APIBinding</code> objects to the edge APIExport in those workspaces; this should be a client config that is able to read/write in all clusters.  For example, that is in the kubeconfig context named <code>base</code> in the kubeconfig created by <code>kcp start</code>.  Finally, the controller also needs a kube client config that is pointed at the root workspace and is authorized to consume the <code>Workspace</code> objects from there.</p> <p>The command line flags, beyond the basics, are as follows.</p> <pre><code>      --concurrency int                  number of syncs to run in parallel (default 4)\n--espw-path string                 the pathname of the edge service provider workspace (default \"root:espw\")\n--mbws-cluster string              The name of the kubeconfig cluster to use for access to mailbox workspaces (really all clusters)\n--mbws-context string              The name of the kubeconfig context to use for access to mailbox workspaces (really all clusters) (default \"base\")\n--mbws-kubeconfig string           Path to the kubeconfig file to use for access to mailbox workspaces (really all clusters)\n--mbws-user string                 The name of the kubeconfig user to use for access to mailbox workspaces (really all clusters)\n--server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10203)\n--root-cluster string              The name of the kubeconfig cluster to use for access to the root workspace\n      --root-context string              The name of the kubeconfig context to use for access to the root workspace (default \"root\")\n--root-kubeconfig string           Path to the kubeconfig file to use for access to the root workspace\n      --root-user string                 The name of the kubeconfig user to use for access to the root workspace\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#try-out-the-mailbox-controller","title":"Try out the mailbox controller","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#pull-the-kcp-and-kubestellar-source-code-build-the-kubectl-ws-binary-and-start-kcp","title":"Pull the kcp and KubeStellar source code, build the kubectl-ws binary, and start kcp","text":"<p>Open a terminal window(1) and clone the latest KubeStellar source:</p> <p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window). Set your <code>KUBECONFIG</code> environment variable to name the kubernetes client config file that <code>kcp</code> generates. <pre><code>kcp start &amp;&gt; /dev/null &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#create-the-edge-service-provider-workspace-espw","title":"Create the Edge Service Provider Workspace (ESPW)","text":"<p>Open another terminal window(2) and point <code>$KUBECONFIG</code> to the admin kubeconfig for the kcp server and include the location of kubectl-ws in <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>Next, use the command that makes sure the Edge Service Provider Workspace (ESPW), which is <code>root:espw</code>, is properly set up.</p> <pre><code>kubestellar init\n</code></pre> <p>After that, a run of the controller should look like the following.</p> <pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>I0305 18:06:20.046741   85556 main.go:110] \"Command line flag\" add_dir_header=\"false\"\nI0305 18:06:20.046954   85556 main.go:110] \"Command line flag\" alsologtostderr=\"false\"\nI0305 18:06:20.046960   85556 main.go:110] \"Command line flag\" concurrency=\"4\"\nI0305 18:06:20.046965   85556 main.go:110] \"Command line flag\" inventory-context=\"root\"\nI0305 18:06:20.046971   85556 main.go:110] \"Command line flag\" inventory-kubeconfig=\"\"\nI0305 18:06:20.046976   85556 main.go:110] \"Command line flag\" log_backtrace_at=\":0\"\nI0305 18:06:20.046980   85556 main.go:110] \"Command line flag\" log_dir=\"\"\nI0305 18:06:20.046985   85556 main.go:110] \"Command line flag\" log_file=\"\"\nI0305 18:06:20.046989   85556 main.go:110] \"Command line flag\" log_file_max_size=\"1800\"\nI0305 18:06:20.046993   85556 main.go:110] \"Command line flag\" logtostderr=\"true\"\nI0305 18:06:20.046997   85556 main.go:110] \"Command line flag\" one_output=\"false\"\nI0305 18:06:20.047002   85556 main.go:110] \"Command line flag\" server-bind-address=\":10203\"\nI0305 18:06:20.047006   85556 main.go:110] \"Command line flag\" skip_headers=\"false\"\nI0305 18:06:20.047011   85556 main.go:110] \"Command line flag\" skip_log_headers=\"false\"\nI0305 18:06:20.047015   85556 main.go:110] \"Command line flag\" stderrthreshold=\"2\"\nI0305 18:06:20.047019   85556 main.go:110] \"Command line flag\" v=\"2\"\nI0305 18:06:20.047023   85556 main.go:110] \"Command line flag\" vmodule=\"\"\nI0305 18:06:20.047027   85556 main.go:110] \"Command line flag\" workload-context=\"\"\nI0305 18:06:20.047031   85556 main.go:110] \"Command line flag\" workload-kubeconfig=\"\"\nI0305 18:06:20.070071   85556 main.go:247] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\nI0305 18:06:20.072088   85556 shared_informer.go:282] Waiting for caches to sync for mailbox-controller\nI0305 18:06:20.172169   85556 shared_informer.go:289] Caches are synced for mailbox-controller\nI0305 18:06:20.172196   85556 main.go:210] \"Informers synced\"\n</code></pre> <p>In a separate terminal window(3), create an inventory management workspace as follows.</p> <pre><code>kubectl ws \\~\nkubectl ws create imw --enter\nkubectl kcp bind apiexport root:espw:edge.kubestellar.io\n</code></pre> <p>Then in that workspace, run the following command to create a <code>SyncTarget</code> object.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\n  name: stest1\nspec:\n  cells:\n    foo: bar\nEOF\n</code></pre> <p>That should provoke logging like the following from the mailbox controller.</p> <pre><code>I0305 18:07:20.490417   85556 main.go:369] \"Created missing workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>And you can verify that as follows:</p> <pre><code>kubectl ws .\nkubectl get synctargets.edge.kubestellar.io\n</code></pre> <p><pre><code>kubectl ws root\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl ws tree kubectl get workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\nniqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368   universal            Ready   https://192.168.58.123:6443/clusters/0ay27fcwuo2sv6ht   22s\n</code></pre></p> <p>FYI, if you look inside that workspace you will see an <code>APIBinding</code> named <code>bind-edge</code> that binds to the <code>APIExport</code> named <code>edge.kubestellar.io</code> from the edge service provider workspace (and this is why the controller needs to know the pathname of that workspace), so that the edge API is available in the mailbox workspace.</p> <p>Next, <code>kubectl delete</code> that workspace, and watch the mailbox controller wait for it to be gone and then re-create it.</p> <pre><code>I0305 18:08:15.428884   85556 main.go:369] \"Created missing workspace\" worker=2 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>Finally, go back to your inventory workspace to delete the <code>SyncTarget</code>:</p> <p><pre><code>kubectl ws \\~\nkubectl ws .\nkubectl ws imw\nkubectl ws .\nkubectl get synctargets.edge.kubestellar.io\nkubectl delete synctargets.edge.kubestellar.io stest1\n</code></pre> and watch the mailbox controller react as follows.</p> <pre><code>I0305 18:08:44.380421   85556 main.go:352] \"Deleted unwanted workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and the space provider with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is the space provider and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/outline/","title":"Details","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-of-this-memo","title":"Status of this memo","text":"<p>This summarizes the design for a Proof-of-Concept intended to be completed in early 2023.  It has been completed except for the summarization feature.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#introduction","title":"Introduction","text":"<p>This is a quick demo of a fragment of what we think is needed for edge multi-cluster.  It is intended to demonstrate the following points.</p> <ul> <li>Separation of infrastructure and workload management.</li> <li>The focus here is on workload management, and that strictly reads   an inventory of infrastructure.</li> <li>What passes from inventory to workload management is kcp TMC   Location and SyncTarget objects.</li> <li>Use of a kcp workspace as the container for the central spec of a workload.</li> <li>Propagation of desired state from center to edge, as directed by   EdgePlacement objects and the Location and SyncTarget objects they reference.</li> <li>Interfaces designed for a large number of edge clusters.</li> <li>Interfaces designed with the intention that edge clusters operate   independently of each other and the center (e.g., can tolerate only   occasional connectivity) and thus any \"service providers\" (in the   technical sense from kcp) in the center or elsewhere.</li> <li>Rule-based customization of desired state.</li> <li>Propagation of reported state from edge to center.</li> <li>Summarization of reported state in the center.</li> <li>Return and/or summarization of state from associated objects (e.g.,   ReplicaSet or Pod objects associated with a given Deployment   object).</li> <li>The edge opens connections to the center, not vice-versa.</li> <li>An edge computing platform \"product\" that can be deployed (as   opposed to a service that is used).</li> </ul> <p>Some important things that are not attempted in this PoC include the following.</p> <ul> <li>An implementation that supports a large number of edge clusters or   any other thing that requires sharding for scale. In this PoC we   will use a single kcp server to hold all the workspaces, and will   not shard any controller.</li> <li>More than one SyncTarget per Location.</li> <li>A hierarchy with more than two levels.</li> <li>User control over ordering of propagation from center to edge,   either among destinations or kinds of objects.</li> <li>More than baseline security (baseline being, e.g., HTTPS, Secret   objects, non-rotating bearer token based service authentication).</li> <li>A good design for bootstrapping the workload management in the edge   clusters.</li> <li>Very strong isolation between tenants in the edge computing   platform.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#development-roadmap-for-this-poc","title":"Development Roadmap for this PoC","text":"<p>Some features will get implemented later than others, so that we can start being able to run interesting end-to-end scenarios relatively soon.  Following is a list of features that will not be implemented at first.</p> <p>Of the following features, customization will be needed before the others.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#customization","title":"Customization","text":"<p>We can have a complete system that ignores customization, as long as it is only used for workloads that need no customization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#summarization","title":"Summarization","text":"<p>We can omit summarization at first.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#return-andor-summarization-of-state-from-associated-objects","title":"Return and/or summarization of state from associated objects","text":"<p>This will involve both defining a scalable interface for declaring what should be returned as well as implementing it.  This will certainly affect the syncer between mailbox workspace and edge cluster, and the summarization part will affect the status summarizer.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#good-handling-of-workload-conflicts","title":"Good handling of workload conflicts","text":"<p>We could start by handling workload conflicts in a very simple way: treating each as an error.  Later development can handle them better, as outlined later.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#denaturingrenaturing","title":"Denaturing/renaturing","text":"<p>We could start by not doing this.  For some resources, the effect of leaving these resources natured in the center is only to add authorizations in the center that are not needed and are undesired in a well-secured environment but tolerable in early demonstrations --- provided that there is not a conflict with an object of the same name that is positively desired in the center.  In particular, these are: <code>ClusterRole</code>, <code>ClusterRoleBinding</code>, <code>Role</code>, <code>RoleBinding</code>, and, depending on the Kubernetes release and usage style, <code>ServiceAccount</code>. The extra consideration for <code>ServiceAccount</code> is when an associated <code>Secret</code> is a natural consequence.  However, that is not a practical problem because such <code>Secret</code> objects are recognized as system infrastructure (see below). Oops, oversight here: the controller that makes the associated Secret objects also insists that the ServiceAccount to refer to them; see later about denaturing ServiceAccounts. Another consideration for <code>ServiceAccount</code> objects, as for <code>Secret</code> and <code>ConfigMap</code> objects, is that some are in some sense \"reverse-natured\": some are created by some other thing as part of the nature of that other thing (object or external system).  Another way of looking at these particular objects is that they are system infrastructure.</p> <p>For some kinds of object, lack of denaturing/renaturing means that KubeStellar will simply not be able to support workloads that contain such objects.  These are: <code>MutatingWebhookConfiguration</code>, <code>ValidatingWebhookConfiguration</code>, <code>LimitRange</code>, <code>ResourceQuota</code>.</p> <p>For some resources, the need to denature is only a matter of anticipation.  <code>FlowSchema</code> and <code>PriorityLevelConfiguration</code> currently are not interpreted by kcp servers and so are effectively already denatured there.  Hopefully they will be given interpretations in the future, and then those resources will join the previous category.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#roles-and-responsibilities","title":"Roles and Responsibilities","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developersdeployersadminsusers-of-the-infrastructure-management-layer","title":"Developers/deployers/admins/users of the infrastructure management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developers-of-the-workload-management-layer","title":"Developers of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#deployersadmins-of-the-workload-management-layer","title":"Deployers/admins of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#users-of-the-workload-management-layer","title":"Users of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#design-overview","title":"Design overview","text":"<p>In very brief: the design approach is to achieve the multicast semantics of edge placement by two layers of activity.  Between the two layers sit mailbox workspaces: these exist in the center, and there is one for each edge cluster.  One layer of activity runs in the center and relates the edge placement problems to mailbox workspace contents.  The other layer is syncers, one in each edge cluster, that relate the corresponding mailbox contents with their local clusters.</p> <p>As in TMC, in this design we have downsync and upsync --- but they are a little more complicated here.  Downsync involves propagation of desired state from workload management workspace through mailbox workspaces to edge and return/summarization of reported state.  Upsync involves return/summarization of desired and reported state of objects born on the edge clusters.  On the inward path, the reported or full state goes from edge to the mailbox workspace and then is summarized to the workload management workspace.  State propagation is maintained in an eventually consistent way, it is not just one-and-done.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#note-on-milieu","title":"Note on milieu","text":"<p>We have made a terminoloogy shift since this document was originally written, but not yet thoroughly updated this document to use the new terminology. Both old and new terminology and their relation is documented in the glossary.</p> <p>This design is set in the context of kcp and currently in the midst of a transition away from that. This design uses kcp workspaces as spaces, and eschews the use of kcp APIExport+APIBinding in favor of using kube-bind.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#kube-bind-object-id-mapping","title":"Kube-bind object ID mapping","text":"<p>With kube-bind, a provider sees a different ID on an object than the consumer does. The namespace or name is modified so that the objects from all the consumers can be copied into the provider. This object ID mapping can be inverted with the following interface.</p> <pre><code>// AnalyzeClusterScopedObject examines a cluster-scoped object in a kube-bind service provider cluster\n// and returns the object's name as known in the service consumer cluster and the kube-bind space id for\n// the consumer, or an error if the object does not appear to be a provider's copy of a consumer's object.\nfunc AnalyzeClusterScopedObject(obj metav1.Object) (name, kbSpaceID string, err error) {..}\n// ComposeClusterScopedName translates the name of a cluster-scoped object\n// from what appears in the service consumer cluster to what appears in the\n// service provider cluster. A Namespace is an example of a cluster-scoped\n// object.\n// For namespaced objects, kube-bind translates the name of their namespace.\nfunc ComposeClusterScopedName(kbSpaceID string, name string) string {..}\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#kube-bind-space-id-mapping","title":"Kube-bind space ID mapping","text":"<p>The kube-bind module has its own identifiers for spaces. These are distinct from the ways that kcp identifies spaces. In kube-bind, for a given provider and consumer, the identifier is the \"cluster namespace\" in the provider cluster that corresponds to the consuer. We will not presume that these cluster namespace names are globally unique (across providers).</p> <p>In kcp there are two ways of identifying most spaces: (1) <code>logicalcluster.Name</code> and (2) the pathname of the corresponding <code>Workspace</code> object. In this design the controllers have been using <code>logicalcluster.Name</code> as the space identifier (using the <code>Path</code> method of <code>logicalcluster.Name</code> when passing a <code>Name</code> to a func that takes a <code>Path</code>).</p> <p>In kube-bind, for a given provider, there is a 1:1 relation between the kube-bind identifier for a consumer (the \"cluster namespace\" in the provider) and the kcp identifier for that consumer (the <code>logicalcluster.Name</code>). This relation is queried through the following interface.</p> <pre><code>// KubeBindSpaceRelation is specific to one kube-bind service provider\n// and reveals the 1:1 relation between the kube-bind identifier for\n// a consumer (i.e., the name of the \"cluster namespace\" for that consumer\n// in the provider) and the underlying space identifier (i.e., the\n// `logicalcluster.Name`).\ntype KubeBindSpaceRelation interface {\n// SpaceIDToKubeBind maps an underlying space ID to a kube-bind space ID.\n// Returns empty string if there is no relationship.\nSpaceIDToKubeBind(spaceID string) string\n// SpaceIDFromKubeBind maps a kube-bind space ID to an underlying space ID.\n// Returns empty string if there is no relationship.\nSpaceIDFromKubeBind(kubeBindID string) string\n}\n// NewKubeBindSpaceRelation creates a KubeBindSpaceRelation given a\n// Kubernetes client for the provider's space and a context bounding\n// the relation's lifetime.\nfunc NewKubeBindSpaceRelation(ctx context.Context, client kubernetes.Interface) KubeBindSpaceRelation {..}\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#implementation-of-space-identifier-mapping","title":"Implementation of space identifier mapping","text":"<p>The underlying information is provided to our layers at just one point in time: when the kube-bind binding operation (<code>kubectl bind</code>) is done. At this time there is available: the <code>logicalcluster.Name</code> of the provider space, the <code>logicalcluster.Name</code> of the consumer space, and the kube-bind identifier of the consumer in relation to the provider. This triple of information is written to a <code>ConfigMap</code> object. That object is in the <code>kubestellar</code> namespace in the provider space, the object's name contains the <code>logicalcluster.Name</code> of the consumer space, and the object has a label whose key is <code>kubestellar.io/kube-bind-id</code> and whose value is the kube-bind identifier of that consumer for that provider. Putting the content in a label makes it easy to get a tabular listing: <code>kubectl get -n kubestellar cm -l kubestellar.io/kube-bind-id -L kubestellar.io/kube-bind-id</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-management-workspaces-imw","title":"Inventory Management workspaces (IMW)","text":"<p>Now called Inventory Space (IS).</p> <p>In this design the primary interface between infrastructure management and workload management is API objects in inventory management workspaces.  We use the <code>Location</code> and <code>SyncTarget</code> object types from KubeStellar's edge.kubestellar.io api group (created by <code>kubestellar init</code>) for this purpose.  The people doing infrastructure management are responsible for creating the inventory management workspaces and populating them with <code>Location</code> and <code>SyncTarget</code> objects, one <code>Location</code> and one <code>SyncTarget</code> per edge cluster.  These inventory management workspaces use APIServiceBindings to APIServiceExports defining <code>Location</code> and <code>SyncTarget</code> so that the workload management layer can use one informer factory on the KCS (formerly ESPW) to read those objects.</p> <p>To complete the plumbing of the syncers, each inventory workspace that contains a SyncTarget needs to also contain the following associated objects.  FYI, these are the things that <code>kubectl kcp workload sync</code> directly creates besides the SyncTarget.  Ensuring their presence is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ol> <li>A ServiceAccount that the syncer will authenticate as.</li> <li>A ClusterRole manipulating that SyncTarget and the    APIResourceImports (what are these?).</li> <li>A ClusterRoleBinding that links that ServiceAccount with that    ClusterRole.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-service-provider-workspace-espw","title":"Edge Service Provider workspace (ESPW)","text":"<p>Now called the KubeStellar Core Space (KCS).</p> <p>The edge multi-cluster service is provided by one workspace that includes the following things.</p> <ul> <li>An APIServiceExport of the edge API group.</li> <li>The edge controllers: Where Resolver, Placement Translator, Mailbox   Controller, and Status Summarizer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-management-workspaces-wmw","title":"Workload Management workspaces (WMW)","text":"<p>Now called a Workload Description Space (WDS).</p> <p>The users of edge multi-cluster primarily maintain these.  Each one of these has both control (API objects that direct the behavior of the edge computing platform) and data (API objects that hold workload desired and reported state).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#data-objects","title":"Data objects","text":"<p>The workload desired state is represented by kube-style API objects, in the way that is usual in the Kubernetes milieu.  For edge computing we need to support both cluster-scoped (AKA non-namespaced) kinds as well as namespaced kinds of objects.</p> <p>The use of a workspace as a mere container presents a challenge, because some kinds of kubernetes API objects at not merely data but also modify the behavior of the apiserver holding them.  To resolve this dilemma, the edge users of such a workspace will use a special view of the workspace that holds only data objects.  The ones that modify apiserver behavior will be translated by the view into \"denatured\" versions of those objects in the actual workspace so that they have no effect on it.  And for these objects, the transport from center-to-edge will do the inverse: translate the denatured versions into the regular (\"natured\"?) versions for appearance in the edge cluster.  Furthermore, for some kinds of objects that modify apiserver behavior we want them \"natured\" at both center and edge.  There are thus a few categories of kinds of objects.  Following is a listing, with with the particular kinds that appear in kcp or plain kubernetes.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-denatured-in-center-natured-in-edge","title":"Needs to be denatured in center, natured in edge","text":"<p>For these kinds of objects: clients of the real workload management workspace can manipulate some such objects that will modify the behavior of the workspace, while clients of the edge computing view will manipulate distinct objects that have no effect on the behavior of the workspace.  These are kinds of objects to which kcp normally associates some behavior.  To be fully precise, the concern here is with behavior that is externally visible (including externally visible behavior of the server itself); we do not care to dissociate server-internal behavior such as storing encrypted at rest.  The edge computing platform will have to implement that view which dissociates the normal kcp behavior.</p> APIVERSION KIND NAMESPACED admissionregistration.k8s.io/v1 MutatingWebhookConfiguration false admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration false apiregistration.k8s.io/v1 APIService false flowcontrol.apiserver.k8s.io/v1beta2 FlowSchema false flowcontrol.apiserver.k8s.io/v1beta2 PriorityLevelConfiguration false rbac.authorization.k8s.io/v1 ClusterRole false rbac.authorization.k8s.io/v1 ClusterRoleBinding false rbac.authorization.k8s.io/v1 Role true rbac.authorization.k8s.io/v1 RoleBinding true v1 LimitRange true v1 ResourceQuota true v1 ServiceAccount true <p>NOTE: The denaturing described here is not implemented yet.  The kinds of objects listed above can be put into a workload management workspace and it will give them its usual interpretation. For objects that add authorizations, this will indeed add authorizations but not otherwise break something. The kcp server does not implement <code>FlowSchema</code> nor <code>PriorityLevelConfiguration</code>; those will indeed be uninterpreted. For objects that configure calls to other servers, these will fail unless the user arranges for them to work when made in the center as well in the edge clusters.</p> <p>In kcp v0.11.0 the Token controller (for ServiceAccounts) creates a token Secret for each ServiceAccount that does not currently reference one created by that controller and updates the ServiceAccount to reference the Secret just created. Should another controller undo that change to that ServiceAccount, the two controllers will fight continually --- leading to an ever growning collection of Secret objects. To prevent the placement translator from fighting with a mailbox space, users must mark a downsynced ServiceAccount as \"create-only\". To prevent some non-KubeStellar controller from fighting with a WDS, users must use feature(s) of that non-KubeStellar controller. In later versions of kcp and Kubernetes there is not such automatic Secret creation.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-and-edge","title":"Needs to be natured in center and edge","text":"<p>These should have their usual effect in both center and edge; they need no distinct treatment.</p> <p>Note, however, that they do have some sequencing implications.  They should be created before any dependent objects, deleted after all dependent objects. Ordering violations only cause log noise.</p> <p>These are also exceptional in their reported state. It is managed by the apiserver. Leaving them natured in the WDS and the mailbox workspace means that they can not hold their reported state from the WEC.</p> APIVERSION KIND NAMESPACED apiextensions.k8s.io/v1 CustomResourceDefinition false v1 Namespace false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-not-destined-for-edge","title":"Needs to be natured in center, not destined for edge","text":"APIVERSION KIND NAMESPACED apis.kcp.io/v1alpha1 APIBinding false kube-bind.io/v1alpha1 APIServiceBinding false <p>A workload management workspace needs APIServiceBindings to the APIServiceExports of the relevant control resources from the edge service provider workspace, in order to be able to contain EdgePlacement and related objects.  These objects and those APIServiceBindings are not destined for the edge clusters.</p> <p>The edge clusters are not presumed to be kcp workspaces nor participate in kube-bind, so APIBindings and APIServiceBindings do not propagate to the edge clusters.  However, it is possible that APIBindings or APIServiceBindings for workload APIs may exist in a workload management workspace and be selected for downsync to mailbox workspaces while the edge clusters have the same resources defined by CRDs (as mentioned later in the discussion of built-in resources and namespaces).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#for-features-not-supported","title":"For features not supported","text":"<p>These are part of k8s or kcp APIs that are not supported by the edge computing platform.</p> APIVERSION KIND NAMESPACED apiresource.kcp.io/v1alpha1 APIResourceImport false apiresource.kcp.io/v1alpha1 NegotiatedAPIResource false apis.kcp.io/v1alpha1 APIConversion false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#not-destined-for-edge","title":"Not destined for edge","text":"<p>These kinds of objects are concerned with either (a) TMC control or (b) workload data that should only exist in the edge clusters.  These will not be available in the view used by edge clients to maintain their workload desired and reported state.</p> APIVERSION KIND NAMESPACED kube-bind.io/v1alpha1 APIServiceExport false apis.kcp.io/v1alpha1 APIExport false apis.kcp.io/v1alpha1 APIExportEndpointSlice false apis.kcp.io/v1alpha1 APIResourceSchema false apps/v1 ControllerRevision true authentication.k8s.io/v1 TokenReview false authorization.k8s.io/v1 LocalSubjectAccessReview true authorization.k8s.io/v1 SelfSubjectAccessReview false authorization.k8s.io/v1 SelfSubjectRulesReview false authorization.k8s.io/v1 SubjectAccessReview false certificates.k8s.io/v1 CertificateSigningRequest false coordination.k8s.io/v1 Lease true core.kcp.io/v1alpha1 LogicalCluster false core.kcp.io/v1alpha1 Shard false events.k8s.io/v1 Event true ~~scheduling.kcp.io/v1alpha1~~ ~~Location~~ ~~false ~~ scheduling.kcp.io/v1alpha1 Placement false tenancy.kcp.io/v1alpha1 ClusterWorkspace false tenancy.kcp.io/v1alpha1 Workspace false tenancy.kcp.io/v1alpha1 WorkspaceType false topology.kcp.io/v1alpha1 Partition false topology.kcp.io/v1alpha1 PartitionSet false v1 Binding true v1 ComponentStatus false v1 Event true v1 Node false ~~workload.kcp.io/v1alpha1~~ ~~SyncTarget~~ ~~false~~"},{"location":"Coding%20Milestones/PoC2023q1/outline/#already-denatured-in-center-want-natured-in-edge","title":"Already denatured in center, want natured in edge","text":"<p>These are kinds of objects that kcp already gives no interpretation to, and that is what KubeStellar needs from the center workspaces.</p> <p>This is the default category of kind of object --- any kind of data object not specifically listed in another category is implicitly in this category.  Following are the kinds from k8s and kcp that fall in this category.</p> APIVERSION KIND NAMESPACED apps/v1 DaemonSet true apps/v1 Deployment true apps/v1 ReplicaSet true apps/v1 StatefulSet true autoscaling/v2 HorizontalPodAutoscaler true batch/v1 CronJob true batch/v1 Job true networking.k8s.io/v1 Ingress true networking.k8s.io/v1 IngressClass false networking.k8s.io/v1 NetworkPolicy true node.k8s.io/v1 RuntimeClass false policy/v1 PodDisruptionBudget true scheduling.k8s.io/v1 PriorityClass false storage.k8s.io/v1 CSIDriver false storage.k8s.io/v1 CSINode false storage.k8s.io/v1 CSIStorageCapacity true storage.k8s.io/v1 StorageClass false storage.k8s.io/v1 VolumeAttachment false v1 ConfigMap true v1 Endpoints true v1 PersistentVolume false v1 PersistentVolumeClaim true v1 Pod true v1 PodTemplate true v1 ReplicationController true v1 Secret true v1 Service true <p>Note that some <code>ConfigMap</code> and <code>Secret</code> objects are treated differently, as explained in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#system-infrastructure-objects","title":"System infrastructure objects","text":"<p>Even in a kcp workspace, some certain objects --- called here \"system infrastructure objects\" --- are created as a consequence of certain other objects or things.  The system infrastructure objects are tolerated in the center and do not propagate toward the edge.  Here is an initial list of system infrastructure objects:</p> <ul> <li><code>Secret</code> objects whose type is   <code>kubernetes.io/service-account-token</code> (these are automatically   created to support a <code>ServiceAccount</code> in some circumstances) or   <code>bootstrap.kubernetes.io/token</code>;</li> <li><code>ConfigMap</code> objects named <code>kube-root-ca.crt</code>;</li> <li><code>ServiceAcount</code> objects named <code>default</code> (these are automatically   created as a consequence of a namespace being created).</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#built-in-resources-and-namespaces","title":"Built-in resources and namespaces","text":"<p>An edge cluster has some built-in resources (i.e, kinds of objects) and namespaces.  A resource may be built-in by any of several ways: it can be built-in to the apiserver, it can be defined by a CRD, its API group can be delegated by an APIService to a custom external server (each of the latter two is sometimes called \"aggregation\").  Note also that a resource may be defined in edge clusters one way (e.g., by being built into kube-apiserver) and in the workload management workspace another way (e.g., by a CustomResourceDefinition).</p> <p>In this PoC, all edge clusters are considered to have the same built-in resources and namespaces.</p> <p>As a matter of scoping the work here, it is also assumed that each API group built into the edge clusters supports the API versions chosen by the conflict resolution rules below when they are applied to the workload sources.</p> <p>At deployment time the workload management platform is configured with lists of resources and namespaces built into the edge clusters.</p> <p>Propagation from center to edge does not attempt to manage the resource and namespace definitions that are built into the edge clusters.</p> <p>The mailbox workspaces will have built-in resources and namespaces that are a subset of those built into the edge clusters.  The propagation from workload management workspace to mailbox workspace does not attempt to manage the resource and namespace definitions that are built into the mailbox workspaces.</p> <p>The above wording is deliberately restrained, for the sake of flexibility regarding resources that are defined one way in the edge clusters and another way in workload management workspace.  For example, the following scenario is allowed.</p> <ul> <li>Some central team owns an API group and produces some   CustomResourceDefinition (CRD) objects that populate that API group.</li> <li>That team derives APIServiceExports of their CRDs.</li> <li>That team maintains a kcp workspace holding those APIServiceExports.</li> <li>Some workload management workspaces have APIServiceBindings to those   APIServiceExports, and EdgePlacement objects that (1) select those   APIServiceBinding objects for downsync and (2) select objects of   kinds defined through those APIServiceBindings for either downsync   or upsync (possibly letting 1 happen as an implicit consequence of   2).</li> <li>Those resources are built into the edge clusters by pre-deploying   the aforementioned CRDs there.</li> <li>Those resources are not built into the mailbox workspaces.  In   this case the APIServiceBindings would propagate from workload management   workspace to mailbox workspaces but not edge clusters.</li> <li>As a consequence of those propagated APIServiceBindings, the   APIServiceExport space gets kube-bound copies of all of the objects   (in workload management workspaces, in mailbox workspaces, and in   any other workspaces where they appear) whose kind is defined   through those APIServiceBindings.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#control-objects","title":"Control objects","text":"<p>These are the EdgePlacement objects, their associated SinglePlacementSlice objects, and the objects that direct customization and summarization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edgeplacement-objects","title":"EdgePlacement objects","text":"<p>One of these is a binding between a \"what\" predicate (with downsync and upsync parts) and a \"where\" predicate.</p> <p>Overlaps between EdgePlacement objects are explicitly allowed.  Two EdgePlacement objects may have \"where\" predicates that both match some of the same destinations.  Two EdgePlacement objects may have \"what\" predicates that match some of the same workload descriptions.  Two EdgePlacement objects may overlap in both ways.</p> <p>An EdgePlacement object binds \"what\" and \"where\", with one adverb at present and more coming in the future. This requires clear definition of how these adverbs combine in the case of overlapping bindings.</p> <p>Conflicts are possible.  This is because the user controls the IDs --- that is, the names --- of the parts of the workload.  In full, a Kubernetes API object is identified by API group, API major version, Kind (equivalently, resource name), namespace if relevant, and name. For simplicity in this PoC we will not worry about differences in API major version; each API group in Kubernetes and/or kcp currently has only one major version.</p> <p>Two different workload descriptions can have objects with the same ID (i.e., if they appear in different workspaces).  These objects, when rendered to the same API version, might have different values.  And the objects may be available in different API versions in different source workspaces.  See client-go for what an API server says about which versions it can serve for a given API group, and meta/v1 for the supporting details on an APIGroup struct.</p> <p>When multiple workload objects with the same APIGroup, Kind, namespace (if namespaced), and name are directed to the same edge cluster, they are merged with conflicts handled by (a) a rule for resolution and (b) reporting via both error logging and Kubernetes Event creation.  These conflicts are serious matters: they mean user expectations are not being met (because they are inconsistent); this is why the placement translator tries hard to make the user aware.</p> <p>The first part of merging a set of objects is to read them all at the same API version.  The placement translator solves the problem of picking API version at the level of API groups rather than object-by-object.  The API version for an given API group is chosen as follows.  First, take the intersection of the supported versions from the various sources.  If this intersection is empty then this is a conflict.  It is resolved by throwing out the APIGroup with the lowest version and repeating with the reduced set of APIGroup structs. Next, take the union of the preferred versions. If this union has a non-empty intersection with the intersection of the supported versions, take the following steps with this intersection; otherwise proceed with just the intersection of the supported versions.  When first (since process startup) presented with an instance of this problem, the placement translator picks the highest version from this intersection.  Subsequently for the same API group, the placement translator sticks with its previous decision as long as that is still in the intersection.  If the previous choice is no longer available, the highest version is picked.  This preference for highest version is based on the expectation that rolling forward will be more common than rolling back; using the intersection ensures that both work (as long as the collection of sources has an overlap in supported versions, which is basic sanity).</p> <p>A workload prescription object that is in the process of graceful deletion (i.e., with <code>DeletionTimestamp</code> set to something) is considered here to already be gone.</p> <p>Once they have been read at a consistent API version, merging of multiple objects is done as follows.  Different parts of the object are handled differently.</p> <ul> <li>TypeMeta.  This can not conflict because it is part of what   identifies an object.</li> <li>ObjectMeta.</li> <li>Labels and Annotations.  These are handled on a key-by-key     basis.  Distinct keys do not conflict.  When multiple objects have     a label or annotation with the same key, the corresponding value     in the result is the value from the most recently updated of those     objects.</li> <li>OwnerReferences.  This is handled analogously to labels and     annotations.  The key is the combination of APIVersion, Kind, and     Name.</li> <li>Finalizers.  This is simply a set of strings.  The result of     merging is the union of the sets.</li> <li>ManagedFields.  This is metadata that is not propagated.</li> <li>Spec.  Beyond TypeMeta and ObjectMeta, the remaining object   fields are specific to the kind of object.  Many have a field named   \"Spec\" in the Go language source, \"spec\" in the JSON representation.   For objects that have Spec fields, merging has a conflict if those   field values are not all equal when considered as JSON data, and the   resolution is to take the value from the most recently updated   object.</li> <li>Status.  Status is handled analogously to Spec.  For both, we   consider a missing field to be the same as a field with a value of   <code>nil</code>.  That is expected to be the common case for the Status of   these workload prescription objects.</li> <li>Other fields.  If all the values are maps (objects in   JavaScript) then they are merged key-by-key, as for labels and   annotations.  Otherwise they are treated as monoliths, as for Spec   and Status.</li> </ul> <p>For the above, the most recently updated object is determined by parsing the ResourceVersion as an <code>int64</code> and picking the highest value.  This is meaningful under the assumption that all the source workspaces are from the same kcp server --- which will be true for this PoC but is not a reasonable assumption in general.  Also: interpreting ResourceVersion breaks a rule for Kubernetes clients --- but this is dismayingly common.  Beyond this PoC we could hope to do better by looking at the ManagedFields.  But currently kcp does not include https://github.com/kubernetes/kubernetes/pull/110058 so the ManagedFields often do not properly reflect the update times.  Even so, those timestamps have only 1-second precision --- so conflicts will remain possible (although, hopefully, unlikely).</p> <p>There is special handling for Namespace objects.  When a workload includes namespaced objects, the propagation has to include ensuring that the corresponding Namespace object exists in the destination.  An EdgePlacement's \"what\" predicate MAY fail to match a relevant Namespace object.  This is taken to mean that this EdgePlacement is not requesting propagation of the details (Spec, labels, etc.) of that Namespace object but only expects propagation to somehow ensure that the namespace exists.  When merging overlapping workloads that have namespaces in common, only the Namespace objects that come from matching a \"what\" predicate need to be merged.</p> <p>The above also provide an answer to the question of what version is used when writing to the mailbox workspace and edge cluster.  The version used for that is the version chosen above.  In the case of no conflicts, this means that the writes are done using the preferred version from the API group from the workload management workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-workspaces","title":"Mailbox workspaces","text":"<p>The mailbox controller maintains one mailbox workspace for each SyncTarget.  A mailbox workspace acts as a workload source for the corresponding syncer, prescribing the workload to go to the corresponding edge cluster and the <code>SyncerConfig</code> object that guides the syncer.</p> <p>A mailbox workspace contains the following items.</p> <ol> <li>APIServiceBindings (maintained by the mailbox controller) to    APIServiceExports of workload object types.</li> <li>Workload objects, post customization in the case of downsynced    objects.</li> <li>A <code>SyncerConfig</code> object.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-cluster","title":"Edge cluster","text":"<p>Also called workload execution cluster.</p> <p>One of these contains the following items.  FYI, these are the things in the YAML output by <code>kubectl kcp workload edge-sync</code>.  The responsibility for creating and maintaining these objects is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ul> <li>A namespace that holds the syncer and associated objects.</li> <li>A ServiceAccount that the syncer authenticates as when accessing the   views of the center and when accessing the edge cluster.</li> <li>A Secret holding that ServiceAccount's authorization token.</li> <li>A ClusterRole listing the non-namespaced privileges that the   syncer will use in the edge cluster.</li> <li>A ClusterRoleBinding linking the syncer's ServiceAccount and ClusterRole.</li> <li>A Role listing the namespaced privileges that the syncer will use in   the edge cluster.</li> <li>A RoleBinding linking the syncer's ServiceAccount and Role.</li> <li>A Secret holding the kubeconfig that the syncer will use to access   the edge cluster.</li> <li>A Deployment of the syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-controller","title":"Mailbox Controller","text":"<p>This controller maintains one mailbox workspace per SyncTarget.  Each of these mailbox workspaces is used for a distinct syncing problem: downsynced objects go here from their workload management workspaces, and upsynced objects go here from the edge cluster.  These workspaces are all children of the edge service provider workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#where-resolver","title":"Where Resolver","text":"<p>This controller monitors the EdgePlacement, Location, and SyncTarget objects and maintains the results of matching.  For each EdgePlacement object this controller maintains an associated collection of SinglePlacementSlice objects holding the matches for that EdgePlacement.  These SinglePlacementSlice objects appear in the same workspace as the corresponding EdgePlacement, and identify SyncTarget and Location objects by the identifiers of their original copies (not the copies in the KCS) --- with the exception of the UID of the SyncTarget, that comes from the copy in the KCS; the remainder of how they are linked is TBD.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#placement-translator","title":"Placement Translator","text":"<p>This controller continually monitors all the EdgePlacement objects, SinglePlacementSlice objects, and related workload objects, and maintains the proper projections of those into mailbox workspace contents.  The customization, if any, is done in this process.  Note also that everything that has to be denatured in the workload management workspace also has to be denatured in the mailbox workspace.</p> <p>The job of the placement translator can be broken down into the following five parts.</p> <ul> <li>Resolve each EdgePlacement's \"what\" part to a list of particular   workspace items (namespaces and non-namespaced objects).</li> <li>Monitor the SinglePlacementSlice objects that report the Where Resolver's   resolutions of the \"where\" part of the EdgePlacement objects.</li> <li>Maintain the association between the resolved \"where\" (from the   Where Resolver) and the resolved what.</li> <li>Maintain the copies, with customization, of the workload objects   from source workspace to mailbox workspaces.</li> <li>Maintain the SyncerConfig object in each mailbox workspace to direct   the corresponding syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#syncers","title":"Syncers","text":"<p>In this PoC there is a 1:1:1 relation between edge cluster, mailbox workspace, and syncer.  The syncer runs in the edge cluster and does downsync from and upsync to the mailbox workspace.  The syncer monitors a SyncerConfig object in the mailbox workspace to know what to downsync and upsync.</p> <p>For those familiar with kcp's TMC syncer, note that the edge syncer differs in the following ways.</p> <ul> <li>Create self-sufficient edge clusters.</li> <li>Re-nature objects that KubeStellar forcibly denatures at the center.</li> <li>Return reported state from associated objects.</li> <li>Does not access the SyncTarget object.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-summarizer","title":"Status Summarizer","text":"<p>For each EdgePlacement object and related objects this controller maintains the directed status summary objects.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#usage-scenario","title":"Usage Scenario","text":"<p>The usage scenario breaks, at the highest level, into two parts: inventory and workload.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-usage","title":"Inventory Usage","text":"<p>A user with infrastructure authority creates one or more inventory management workspaces.  Each such workspace needs to have the following items, which that user will create if they are not pre-populated by the workspace type.</p> <ul> <li>APIServiceBindings to the APIServiceExports of <code>SyncTarget</code> and   <code>Location</code>.</li> <li>A ServiceAccount (with associated token-bearing Secret) (details   TBD) that the mailbox controller authenticates as.</li> <li>A ClusterRole and ClusterRoleBinding that authorize said   ServiceAccount to do what the mailbox controller needs to do.</li> </ul> <p>This user also creates one or more edge clusters.</p> <p>For each of those edge clusters, this user creates the following.</p> <ul> <li>a corresponding SyncTarget, in one of those inventory management   workspaces;</li> <li>a Location, in the same workspace, that matches only that   SyncTarget.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-usage","title":"Workload usage","text":"<p>A user with workload authority starts by creating one or more workload management workspaces.  Each needs to have the following, which that user creates if the workload type did not already provide.</p> <ul> <li>APIServiceBindings to the APIServiceExports of the relevant   resources from the edge service provider workspace.</li> <li>For each of the Where Resolver, the Placement Translator, and the   Status Summarizer:</li> <li>A ServiceAccount for that controller to authenticate as;</li> <li>A ClusterRole granting the privileges needed by that controller;</li> <li>A ClusterRoleBinding that binds those two.</li> </ul> <p>This user also uses the edge-workspace-as-container view of each such workspace to describe the workload desired state.</p> <p>This user creates one or more EdgePlacement objects to say which workload goes where.  These may be accompanied by API objects that specify rule-based customization, specify how status is to be summarized.</p> <p>The KubeStellar implementation propagates the desired state from center to edge and collects the specified information from edge to center.</p> <p>The edge user monitors status summary objects in their workload management workspaces.</p> <p>The status summaries may include limited-length lists of broken objects.</p> <p>Full status from the edge is available in the mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/","title":"KubeStellar Placement Translator","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/placement-translator.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre> <p>The placement translator runs in the center and translates EMC placement problems into edge sync problems.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#status","title":"Status","text":"<p>The placement translator is a work in progress.  It maintains <code>SyncerConfig</code> objects and downsynced objects in mailbox workspaces, albeit with limitations discussed in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#additional-design-details","title":"Additional Design Details","text":"<p>The placement translator maintains one <code>SyncerConfig</code> object in each mailbox workspace.  That object is named <code>the-one</code>.  Other <code>SyncerConfig</code> objects may exist; the placement translator ignores them.</p> <p>The placement translator responds to each resource discovery independently.  This makes the behavior jaggy and the logging noisy. For example, it means that the <code>SyncerConfig</code> objects may be rewritten for each resource discovery.  But eventually the right things happen.</p> <p>The placement translator does not yet attempt the full prescribed technique for picking the API version to use when reading and writing. Currently it looks only at the preferred version reported in each workload management workspace, and only succeeds if they all agree.</p> <p>One detail left vague in the design outline is what constitutes the \"desired state\" that propagates from center to edge.  The easy obvious answer is the \"spec\" section of downsynced objects, but that answer ignores some issues.  Following is the current full answer.</p> <p>When creating a workload object in a mailbox workspace, the placement translator uses a copy of the object read from the workload management workspace but with the following changes.</p> <ul> <li>The <code>metadata.managedFields</code> is emptied.</li> <li>The <code>metadata.resourceVersion</code> is emptied.</li> <li>The <code>metadata.selfLlink</code> is emptied.</li> <li>The <code>metadata.uid</code> is emptied.</li> <li>The <code>metadata.ownerReferences</code> is emptied.  (Doing better would   require tracking UID mappings from WMW to MBWS.)</li> <li>In <code>metadata.labels</code>, <code>edge.kubestellar.io/projected=yes</code> is added.</li> </ul> <p>The placement translator does not react to changes to the workload objects in the mailbox workspace.</p> <p>When downsyncing desired state and the placement translator finds the object already exists in the mailbox workspace, the placement translator does an HTTP PUT (<code>Update</code> in the <code>k8s.io/client-go/dynamic</code> package) using an object value --- called below the \"destination\" object --- constructed by reading the object from the MBWS and making the following changes.</p> <ul> <li>For top-level sections in the source object other than <code>apiVersion</code>,   <code>kind</code>, <code>metadata</code>, and <code>status</code>, the destination object gets the   same contents for that section.</li> <li>If the source object has some annotations then they are merged into   the destination object annotations as follows.</li> <li>A destination annotation that has no corresponding annotation in     the source is unchanged.</li> <li>A destination annotation that has the same value as the     corresponding annotation in the source is unchanged.</li> <li>A \"system\" annotation is unchanged.  The system annotations are     those whose key (a) starts with <code>kcp.io/</code> or other stuff followed     by <code>.kcp.io/</code> and (b) does not start with <code>edge.kubestellar.io/</code>.</li> <li>The source object's labels are merged into the destination object   using the same rules as for annotations, and <code>edge.kubestellar.io/projected</code>   is set to <code>yes</code>.</li> <li>The remainder of the <code>metadata</code> is unchanged.</li> </ul> <p>For objects --- other than <code>Namespace</code> objects --- that exist in a mailbox workspace and whose API GroupResource has been relevant to the placement translator since it started, ones that have the <code>edge.kubestellar.io/projected=yes</code> label but are not currently desired are deleted.  The exclusion for <code>Namespace</code> objects is there because the placement translator does not take full ownership of them, rather it takes the position that there might be other parties that create <code>Namespace</code> objects or rely on their existence.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#usage","title":"Usage","text":"<p>The placement translator needs two kube client configurations.  One points to the edge service provider workspace and provides authority to (a) read the APIExport view of the edge API and (b) write into the mailbox workspaces.  The other points to the kcp server base (i.e., does not identify a particular logical cluster nor <code>*</code>) and is authorized to read all clusters.  In the kubeconfig created by <code>kcp start</code> that is satisfied by the context named <code>system:admin</code>.</p> <p>The command line flags, beyond the basics, are as follows.  For a string parameter, if no default is explicitly stated then the default is the empty string, which usually means \"not specified here\".  For both kube client configurations, the usual rules apply: first consider command line parameters, then <code>$KUBECONFIG</code>, then <code>~/.kube/config</code>.</p> <pre><code>      --allclusters-cluster string       The name of the kubeconfig cluster to use for access to all clusters\n      --allclusters-context string       The name of the kubeconfig context to use for access to all clusters (default \"system:admin\")\n--allclusters-kubeconfig string    Path to the kubeconfig file to use for access to all clusters\n      --allclusters-user string          The name of the kubeconfig user to use for access to all clusters\n\n--espw-cluster string              The name of the kubeconfig cluster to use for access to the edge service provider workspace\n      --espw-context string              The name of the kubeconfig context to use for access to the edge service provider workspace\n      --espw-kubeconfig string           Path to the kubeconfig file to use for access to the edge service provider workspace\n      --espw-user string                 The name of the kubeconfig user to use for access to the edge service provider workspace\n\n--root-cluster string              The name of the kubeconfig cluster to use for access to root workspace\n      --root-context string              The name of the kubeconfig context to use for access to root workspace (default \"root\")\n--root-kubeconfig string           Path to the kubeconfig file to use for access to root workspace\n      --root-user string                 The name of the kubeconfig user to use for access to root workspace\n\n--server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10204)\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#try-it","title":"Try It","text":"<p>The nascent placement translator can be exercised following the scenario in example1.  You will need to run the where resolver and mailbox controller long enough for them to create what this scenario calls for, but they can be terminated after that.</p> <p></p> <p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox spaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#your-kubeconfig-files","title":"Your kubeconfig files","text":"<p>This example (and others) involve user-managed kubeconfig files. These examples organize those files into one subdirectory. Start by making sure that directory exists without any contents left over from previous runs.</p> <pre><code>MY_KUBECONFIGS=${PWD}/my-kubeconfigs\nrm -rf \"$MY_KUBECONFIGS\"\nmkdir -p \"$MY_KUBECONFIGS\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#get-binaries-of-kube-bind-and-dex","title":"Get binaries of kube-bind and dex","text":"<p>The command below makes kube-bind binaries and dex binary available in <code>$PATH</code>.</p> <pre><code>rm -rf kube-bind\ngit clone https://github.com/waltforme/kube-bind.git &amp;&amp; \\\npushd kube-bind &amp;&amp; \\\ngit checkout kubestellar &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd &amp;&amp; \\\ngit clone https://github.com/dexidp/dex.git &amp;&amp; \\\npushd dex &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build &amp;&amp; \\\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the KubeStellar Core Space (KCS) (formerly called the Edge Service Provider Workspace (ESPW)), which exports the KubeStellar API.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar -X init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code> and the current context is for the KubeStellar hosting cluster.</p> <p>The following variable will be used in later commands to indicate that they are not being invoked from within the hosting cluster (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>IMW1_KUBECONFIG=\"${MY_KUBECONFIGS}/imw1.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig imw1 --kubeconfig $SM_CONFIG $in_cluster $IMW1_KUBECONFIG\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"describe the florin location object\"\nKUBECONFIG=$IMW1_KUBECONFIG kubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two <code>ensure location</code> calls are equivalent to creating the following four objects.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p> <p>Continue to follow the steps until the start of Stage 3 of the exercise.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>KUBECONFIG=$SM_CONFIG mailbox-controller --external-access -v=4 &amp;&gt; /tmp/mailbox-controller.log &amp;\nsleep 20\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox spaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox spaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>while [ $(KUBECONFIG=$SM_CONFIG kubectl get spaces -A | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into /tmp/mailbox-controller.log if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI1218 16:36:14.743434  239027 controller.go:277] \"Created missing space\" worker=0 mbsName=\"imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\"\n...\nI1218 16:36:14.759426  239027 controller.go:277] \"Created missing space\" worker=1 mbsName=\"imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\"\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox spaces.</p> <p>A mailbox space name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox spaces as follows.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <pre><code>NAMESPACE               NAME                                           AGE\nspaceprovider-default   espw                                           84s\nspaceprovider-default   imw1                                           76s\nspaceprovider-default   imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   20s\nspaceprovider-default   imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   20s\nspaceprovider-default   wmw1                                           53s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -n spaceprovider-default -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                           SYNCTARGET   CLUSTER\nespw                                           &lt;none&gt;       &lt;none&gt;\nimw1                                           &lt;none&gt;       &lt;none&gt;\nimw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   florin       &lt;none&gt;\nimw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   guilder      &lt;none&gt;\nwmw1                                           &lt;none&gt;       &lt;none&gt;\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox space by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox space name is $GUILDER_SPACE\n</code></pre> <pre><code>The guilder mailbox space name is imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\n</code></pre></p> <p><pre><code>FLORIN_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox space name is $FLORIN_SPACE\n</code></pre> <pre><code>The florin mailbox space name is imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#connect-guilder-edge-cluster-with-its-mailbox-space","title":"Connect guilder edge cluster with its mailbox space","text":"<p>The following command will (a) create, in the mailbox space for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster guilder\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\".\n\nWrote workload execution cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\n\nto verify the syncer pod is running.\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system                                           coredns                                               2/2     2            2           4m1s\nkubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   1/1     1            1           0s\nlocal-path-storage                                    local-path-provisioner                                1/1     1            1           3m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#connect-florin-edge-cluster-with-its-mailbox-space","title":"Connect florin edge cluster with its mailbox space","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster florin\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\".\n\nWrote workload execution cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\n\nto verify the syncer pod is running.\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nserviceaccount/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\ndeployment.apps/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.  These commands create the WMW, makes KubeStellar edge APIs available for the WMW via kube-bind, and optionally makes some Kubernetes APIs available for the WMW by populating a set of CRDs into the WMW.</p> <pre><code># TODO: Needs to pass --with-kube false or true\nWMW_C_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-c.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-c --with-kube $kube_needed --output-kubeconfig \"$WMW_C_SPACE_CONFIG\"\nsleep 15\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code># TODO: needs to set --with-kube false/true\nWMW_S_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-s.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-s --with-kube $kube_needed --output-kubeconfig \"$WMW_S_SPACE_CONFIG\"\n</code></pre> <p>In this workload we will also demonstrate how to downsync objects whose kind is defined by a <code>CustomResourceDefinition</code> object. We will use the one from the Kubernetes documentation for CRDs, modified so that the resource it defines is in the category <code>all</code>. First, create the definition object with the following command.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;\n  name: crontabs.stable.example.com\nspec:\n  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;\n  group: stable.example.com\n  # list of versions supported by this CustomResourceDefinition\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                cronSpec:\n                  type: string\n                image:\n                  type: string\n                replicas:\n                  type: integer\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;\n    plural: crontabs\n    # singular name to be used as an alias on the CLI and for display\n    singular: crontab\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: CronTab\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - ct\n    categories:\n    - all\nEOF\n</code></pre> <p>Next, use the following command to wait for the apiserver to process that definition.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG wait --for condition=Established crd crontabs.stable.example.com\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n  annotations: {just-for: fun}\n---\napiVersion: \"stable.example.com/v1\"\nkind: CronTab\nmetadata:\n  name: my-new-cron-object\n  namespace: specialstuff\nspec:\n  cronSpec: \"* * * * */5\"\n  image: my-awesome-cron-image\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p>The \"what predicate\" explicitly includes the <code>Namespace</code> object named \"specialstuff\", which causes all of its desired state (including labels and annotations) to be downsynced. This contrasts with the common EdgePlacement, which does not explicitly mention the <code>commonstuff</code> namespace, relying on the implicit creation of namespaces as needed in the WECs.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    objectNames: [ v1090.example.my ]\n  - apiGroup: stable.example.com\n    resources: [ crontabs ]\n    namespaces: [ specialstuff ]\n    objectNames: [ my-new-cron-object ]\n  - apiGroup: \"\"\n    resources: [ namespaces ]\n    objectNames: [ specialstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar-where-resolver --external-access -v=4 &amp;&gt; /tmp/where-resolver.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>while ! kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nwhile ! kubectl --kubeconfig $WMW_S_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I1219 16:40:49.814564  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-florin\" location=\"kube-bind-sx6pl-florin\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n...\nI1219 16:40:49.797689  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-guilder\" location=\"kube-bind-sx6pl-guilder\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p> <p>Finally run the placement translator from the command line.  That should look like the following (possibly including some complaints, which do not necessarily indicate real problems).</p> <pre><code>kubectl ws root:espw\nplacement-translator &amp;\nsleep 120\n</code></pre> <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\nI0412 15:15:57.970014   94634 shared_informer.go:289] Caches are synced for what-resolver\nI0412 15:15:57.970178   94634 shared_informer.go:282] Waiting for caches to sync for where-resolver\nI0412 15:15:57.970192   94634 shared_informer.go:289] Caches are synced for where-resolver\n...\nI0412 15:15:57.972185   94634 map-types.go:338] \"Put\" map=\"where\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val=\"[&amp;{SinglePlacementSlice edge.kubestellar.io/v2alpha1} {edge-placement-s    e1b1033d-49f2-45e8-8a90-6d0295b644b6 1184 1 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:r0bdh9oumjkoag3s] [{edge.kubestellar.io/v2alpha1 EdgePlacement edge-placement-s 0e718a31-db21-47f1-b789-cd55835b1418 &lt;nil&gt; &lt;nil&gt;}] []  [{where-resolver Update edge.kubestellar.io/v2alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"0e718a31-db21-47f1-b789-cd55835b1418\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\nI0412 15:15:57.973740   94634 map-types.go:338] \"Put\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val=\"[&amp;{SinglePlacementSlice edge.kubestellar.io/v2alpha1} {edge-placement-c    c446ca9b-8937-4751-89ab-058bcfb079c1 1183 3 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:1i1weo8uoea04wxr] [{edge.kubestellar.io/v2alpha1 EdgePlacement edge-placement-c c1e038b9-8bd8-4d22-8ab8-916e40c794d1 &lt;nil&gt; &lt;nil&gt;}] []  [{where-resolver Update edge.kubestellar.io/v2alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"c1e038b9-8bd8-4d22-8ab8-916e40c794d1\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-f sync-target-f e6efb8bd-6755-45ac-b44d-5d38f978f990} {1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\n...\nI0412 15:15:58.173974   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[{APIGroup: Resource:namespaces Name:commonstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:group1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group2.test Resources:[cogs] Namespaces:[] Names:[William]}]}\nI0412 15:15:58.180380   94634 map-types.go:338] \"Put\" map=\"what\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val={Downsync:map[{APIGroup: Resource:namespaces Name:specialstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:group1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group3.test Resources:[widgets] Namespaces:[] Names:[*]}]}\n...\n</code></pre> <p>The \"Put\" log entries with <code>map=\"what\"</code> show what the \"what resolver\" is reporting.  This reports mappings from <code>ExternalName</code> of an <code>EdgePlacement</code> object to the workload parts that that <code>EdgePlacement</code> says to downsync and upsync.</p> <p>The \"Put\" log entries with <code>map=\"where\"</code> show the <code>SinglePlacementSlice</code> objects associated with each <code>EdgePlacement</code>.</p> <p>Next, using a separate shell, examine the SyncerConfig objects in the mailbox workspaces.  Make sure to use the same kubeconfig as you use to run the placement translator, or any other that is pointed at the edge service provider workspace. The following with switch the focus to mailbox workspace(s).</p> <p>You can get a listing of mailbox workspaces, as follows.</p> <p><pre><code>kubectl ws root\nkubectl get workspace\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19   universal            Ready   https://192.168.58.123:6443/clusters/12zzf3frkqz2yj39   36m\n1xpg93182scl85te-mb-e6efb8bd-6755-45ac-b44d-5d38f978f990   universal            Ready   https://192.168.58.123:6443/clusters/2v6wl3x41zxmpmhr   36m\n</code></pre></p> <p>Next switch to one of the mailbox workspaces (in my case I picked the one for the guilder cluster) and examine the <code>SyncerConfig</code> object. That should look like the following.</p> <p><pre><code>kubectl ws $(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>Current workspace is \"root:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml                           </code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\ngeneration: 2\nname: the-one\n  resourceVersion: \"1249\"\nuid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: events\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n  upsync:\n  - apiGroup: group2.test\n    names:\n    - William\n    resources:\n    - cogs\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre></p> <p>At this point you might veer off from the example scenario and try tweaking things.  For example, try deleting an EdgePlacement as follows.</p> <p><pre><code>kubectl ws root:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:work-c\"\n</code></pre> <pre><code>kubectl delete EdgePlacement edge-placement-c\n</code></pre> <pre><code>edgeplacement.edge.kubestellar.io \"edge-placement-c\" deleted\n</code></pre></p> <p>That will cause the placement translator to log updates, as follows.</p> <pre><code>I0412 15:20:43.129842   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[] Upsync:[]}\nI0412 15:20:43.241674   94634 map-types.go:342] \"Delete\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\"\n</code></pre> <p>After that, the SyncerConfig in the florin mailbox should be empty, as in the following (you mailbox workspace names may be different).</p> <p><pre><code>kubectl ws root\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl ws $(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\n</code></pre> <pre><code>Current workspace is \"root:2lplrryirmv4xug3-mb-89c08764-01ae-4117-8fb0-6b752e76bc2f\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 2cow9p3xogak4n0u\n  creationTimestamp: \"2023-04-11T04:34:22Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"2130\"\nuid: 2b66b4bc-4130-4bf0-8524-73d6885f2ad8\nspec:\n  namespaceScope: {}\nstatus: {}\n</code></pre></p> <p>And the SyncerConfig in the guilder mailbox workspace should reflect only the special workload.  That would look something like the following.</p> <p><pre><code>kubectl ws root\nkubectl ws $(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>Current workspace is \"root:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml                           </code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1254\"\nuid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: events\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and the space provider with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is the space provider and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/","title":"Use Cases Affecting the Roadmap for PoC2023q1","text":""},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#background","title":"Background","text":"<p>The outline mentions features that need not be implement at first.  In the following sections we consider some particular use cases.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mvi","title":"MVI","text":"<p>MVI needs customization.  We can demo an MVI scenario without: self-sufficient edge clusters, summarization, upsync (Return and/or summarization of reported state from associated objects), sophisticated handling of workload conflicts.</p> <p>What about denaturing?</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#compliance-to-policy","title":"Compliance-to-Policy","text":"<p>I am not sure what is workable here.  Following are some possibilities.  They vary in two dimensions.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#how-c2p-controller-consumes-reports","title":"How C2P controller consumes reports","text":""},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-workload-apiexport-and-workload-apibindings","title":"Through view of workload APIExport and workload APIBindings","text":"<p>As outlined in PR 241: - the C2P team maintains CRDs, APIResourceSchemas, and an APIExport for   the policy and report resources; - the C2P team puts those APIResourceSchemas and that APIExport in a   kcp workspace of their choice; - the workload management workspace has an APIBinding to that APIExport; - the EdgePlacement selects that APIBinding for downsync; - the APIBinding goes to the mailbox workspace but not the edge cluster; - those CRDs are pre-installed on the edge clusters; - the APIExport's view shows the report objects in the mailbox   workspaces (as well as anywhere else they exist).</p> <p>This is not a great choice because of \"those CRDs are pre-installed on the edge clusters\".</p> <p>It is also not a great choice because it requires the C2P team to maintain two copies of the Kyverno resource definitions.</p> <p>This is a bad choice because it is not consistent with the preferred way to demonstrate installation of Kyverno, which is to have Helm install Kyverno into the workload management workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-a-new-kind-of-view","title":"Through a new kind of view","text":"<p>We could define a new kind of view that does what we want.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-emc-apiexport-and-mailbox-apibindings","title":"Through view of EMC APIExport and mailbox APIBindings","text":"<p>This approach also uses APIExport and APIBinding objects but in a different way than above.  In this approach the placement translator maintains one APIExport in the edge service provider workspace and a corresponding APIBinding object in each mailbox workspace, and they work together as follows.</p> <p>The APIExport has an empty LatestResourceSchemas but a large dynamic PermissionClaims slice.  In particular, there is a PermissionClaim for every resource involved in downsync or upsync in any EdgePlacement object.  Some day we might try something more granular, but today is not that day.</p> <p>In each mailbox workspace, the corresponding APIBinding's list of accepted PermissionClaims has an entry for every resource downsynced or upsynced to that workspace.</p> <p>As a consequence, the APIExport's view holds all the objects whose kind/resource is defined by those APIBindings.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-an-api-for-consuming-from-mailboxes","title":"Through an API for consuming from mailboxes","text":"<p>The C2P Controller uses the API proposed in PR 240 to read the report objects from the mailbox workspaces.  This has the downside of exposing the mailbox workspaces as part of the KubeStellar interface --- which they were NOT originally intended to be.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#c2p-controller-consumes-report-summaries-prepared-by-kubestellar","title":"C2P Controller consumes report summaries prepared by KubeStellar","text":"<p>In this scenario: - we have defined and implemented summarization in KubeStellar; - that summarization is adequate for the needs of the C2P Controller; - that controller consumes summaries rather than the reports themselves.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mailbox-vs-edge","title":"Mailbox vs. Edge","text":"<p>The latest plan is to use full EMC.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#using-the-current-tmc-syncer","title":"Using the current TMC syncer","text":"<p>In this scenario the edge clusters are not self-sufficient; the workload containers in an edge cluster use kube api services from the corresponding mailbox workspace.  The key insight here is that from an outside perspective, a pair of (edge cluster, corresponding mailbox workspace) operates as a unit and the rest of the world does not care about internal details of that unit.  But that is only true if you do not require too much from the networking.  In this scenario, a workload container runs in the edge cluster and a workload Service object is about proxying/load-balancing in the edge cluster.  An admission control webhook normally directs the apiserver to call out to a virtual IP address associated with a Service; that is a problem in this scenario because the apiserver in question is the one holding the mailbox workspace but the Service that gets connections to the workload containers is in the edge cluster.  This scenario will work if the C2P workload does not include admission control webhooks.  Note that Kubernetes release 1.26 introduces CEL-based validating admission control policies, so using them would not involve webhooks.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#expand-tmc-to-support-webhooks","title":"Expand TMC to support webhooks","text":"<p>The problem with webhooks would go away if TMC were expanded to support them, perhaps through some sort of tunneling so that a client in the center can open connections to a Service at the edge.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#pre-deploy-controllers-and-resources-on-edge-clusters","title":"Pre-deploy controllers and resources on edge clusters","text":"<p>In this scenario, the PVP/PEP is pre-deployed on the edge clusters, and the policy and report resources (which are cluster-scoped) are predefined there too.  This scenario would continue to use the TMC syncer, but only need it to downsync the policies and upsync the reports.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#use-full-emc","title":"Use full EMC","text":"<p>No shortcuts here, no limitations.</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/","title":"KubeStellar Where Resolver","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/where-resolver.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#usage-of-the-where-resolver","title":"Usage of the Where Resolver","text":"<p>The Where Resolver needs two Kubernetes client configurations.</p> <p>The first is needed to access the APIExport view of the <code>edge.kubestellar.io</code> API group. It must point to the edge service provider workspace that has this APIExport and is authorized to read its view for edge APIs.</p> <p>The second is needed to maintain <code>SinglePlacementSlice</code> objects in all workload management workspaces; this should be a client config that is able to read/write in all clusters. For example, there is a kubeconfig context named <code>base</code> in the kubeconfig created by <code>kcp start</code> which satisfies these requirements.</p> <p>The command line flags, beyond the basics, are as follows.</p> <pre><code>      --espw-cluster string                  The name of the kubeconfig cluster to use for access to the edge service provider workspace\n      --espw-context string                  The name of the kubeconfig context to use for access to the edge service provider workspace\n      --espw-kubeconfig string               Path to the kubeconfig file to use for access to the edge service provider workspace\n      --espw-user string                     The name of the kubeconfig user to use for access to the edge service provider workspace\n\n--base-cluster string                  The name of the kubeconfig cluster to use for access to all logical clusters as kcp-admin (default \"base\")\n--base-context string                  The name of the kubeconfig context to use for access to all logical clusters as kcp-admin\n      --base-kubeconfig string               Path to the kubeconfig file to use for access to all logical clusters as kcp-admin\n      --base-user string                     The name of the kubeconfig user to use for access to all logical clusters as kcp-admin (default \"kcp-admin\")\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#steps-to-try-the-where-resolver","title":"Steps to try the Where Resolver","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#pull-the-kcp-source-code-build-kcp-and-start-kcp","title":"Pull the kcp source code, build kcp, and start kcp","text":"<p>At this point you should have cloned the KubeStellar repo and <code>cd</code>ed into it as directed above.</p> <p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window). Set your <code>KUBECONFIG</code> environment variable to name the kubernetes client config file that <code>kcp</code> generates. <pre><code>kcp start &amp;&gt; /dev/null &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#build-and-initialize-kubestellar","title":"Build and initialize KubeStellar","text":"<p>First build KubeStellar and add the result to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>Next, use the command that makes sure the Edge Service Provider Workspace (ESPW), which is <code>root:espw</code>, is properly set up.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#create-the-workload-management-workspace-wmw-and-bind-it-to-the-espw-apis","title":"Create the Workload Management Workspace (WMW) and bind it to the ESPW APIs","text":"<p>Use the user home workspace (\\~) as the workload management workspace (WMW). <pre><code>kubectl ws \\~\n</code></pre></p> <p>Bind APIs. <pre><code>kubectl apply -f config/imports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#run-the-kubestellar-where-resolver-against-the-espw","title":"Run the KubeStellar Where Resolver against the ESPW","text":"<p>Go to the <code>root:espw</code> workspace and run the Where Resolver.</p> <pre><code>kubectl ws root:espw\nkubestellar-where-resolver -v 2 &amp;\nsleep 45\n</code></pre> <p>The outputs from the Where Resolver should be similar to: <pre><code>I0605 10:53:00.156100   29786 main.go:212] \"Found APIExport view\" exportName=\"edge.kubestellar.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/jxch2kyb3c1h6bac/edge.kubestellar.io\"\nI0605 10:53:00.157874   29786 main.go:212] \"Found APIExport view\" exportName=\"scheduling.kcp.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/root/scheduling.kcp.io\"\nI0605 10:53:00.159242   29786 main.go:212] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/root/workload.kcp.io\"\nI0605 10:53:00.261128   29786 controller.go:201] \"starting controller\" controller=\"where-resolver\"\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#create-the-inventory-management-workspace-imw-and-populate-it-with-locations-and-synctargets","title":"Create the Inventory Management Workspace (IMW) and populate it with locations and synctargets","text":"<p>Use workspace <code>root:compute</code> as the Inventory Management Workspace (IMW). <pre><code>kubectl ws root:compute\nkubectl kcp bind apiexport root:espw:edge.kubestellar.io\n</code></pre></p> <p>Create two Locations and two SyncTargets. <pre><code>kubectl create -f config/samples/location_prod.yaml\nkubectl create -f config/samples/location_dev.yaml\nkubectl create -f config/samples/synctarget_prod.yaml\nkubectl create -f config/samples/synctarget_dev.yaml\nsleep 5\n</code></pre></p> <p>Note that kcp automatically creates a Location <code>default</code>. So there are 3 Locations and 2 SyncTargets in <code>root:compute</code>. <pre><code>kubectl get locations,synctargets\n</code></pre> <pre><code>NAME                                 RESOURCE      AVAILABLE   INSTANCES   LABELS   AGE\nlocation.edge.kubestellar.io/default   synctargets   0           2                    2m12s\nlocation.edge.kubestellar.io/dev       synctargets   0           1                    2m39s\nlocation.edge.kubestellar.io/prod      synctargets   0           1                    3m13s\n\nNAME                              AGE\nsynctarget.edge.kubestellar.io/dev    110s\nsynctarget.edge.kubestellar.io/prod   2m12s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#create-some-edgeplacements-in-the-wmw","title":"Create some EdgePlacements in the WMW","text":"<p>Go to Workload Management Workspace (WMW) and create an EdgePlacement <code>all2all</code>. <pre><code>kubectl ws \\~\nkubectl create -f config/samples/edgeplacement_all2all.yaml\nsleep 3\n</code></pre></p> <p>The Where Resolver maintains a SinglePlacementSlice for an EdgePlacement in the same workspace. <pre><code>kubectl get sps all2all -oyaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\ndestinations:\n- cluster: 1yotsgod0d2p3xa5\n  locationName: prod\n  syncTargetName: prod\n  syncTargetUID: 13841ffd-33f2-4cf4-9114-6156f73aa5c8\n- cluster: 1yotsgod0d2p3xa5\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\n- cluster: 1yotsgod0d2p3xa5\n  locationName: default\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\n- cluster: 1yotsgod0d2p3xa5\n  locationName: default\n  syncTargetName: prod\n  syncTargetUID: 13841ffd-33f2-4cf4-9114-6156f73aa5c8\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-06-05T14:55:20Z\"\ngeneration: 1\nname: all2all\n  ownerReferences:\n  - apiVersion: edge.kubestellar.io/v2alpha1\n    kind: EdgePlacement\n    name: all2all\n    uid: 31915018-6a25-4f01-943e-b8a0a0ed35ba\n  resourceVersion: \"875\"\nuid: a2b8224d-5feb-40a1-adb2-67c07965f13b\n</code></pre> EdgePlacement <code>all2all</code> selects all the 3 Locations in <code>root:compute</code>.</p> <p>Create a more specific EdgePlacement which selects Locations labeled by <code>env: dev</code>. <pre><code>kubectl create -f config/samples/edgeplacement_dev.yaml\nsleep 3\n</code></pre></p> <p>The corresponding SinglePlacementSlice has a shorter list of <code>destinations</code>: <pre><code>kubectl get sps dev -oyaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\ndestinations:\n- cluster: 1yotsgod0d2p3xa5\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-06-05T14:57:00Z\"\ngeneration: 1\nname: dev\n  ownerReferences:\n  - apiVersion: edge.kubestellar.io/v2alpha1\n    kind: EdgePlacement\n    name: dev\n    uid: 1ac4b7f5-5521-4b5a-a0fa-cc2ec87b458b\n  resourceVersion: \"877\"\nuid: c9c0c2fc-d721-4c73-9788-e10711bad23a\n</code></pre></p> <p>Feel free to change the Locations, SyncTargets, and EdgePlacements and see how the Where Resolver reacts.</p> <p>Your next step is to deliver a workload to a mailbox (that represents an edge location).  Go here to take the next step... (TBD)</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and the space provider with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is the space provider and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/_index/","title":"Environments","text":"<p>There will be 2 environments designed, created, and maintained for KubeStellar: - A cloud environment (cloud-env) which will be used to deploy, scale, and measure metrics associated with small, medium, and large-scale KubeStellar experiments, - A development environment (dev-env) which will be used to deploy a small local KubeStellar installation that is sized for daily use and experimentation on a laptop.</p>"},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/","title":"Cloud-Environment (cloud-env)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#monitoring-tools-for-kubestellar-prometheus-grafana-and-node-exporter","title":"Monitoring Tools for KubeStellar (Prometheus, Grafana and Node Exporter)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#description","title":"Description","text":"<p>This example shows how to deploy monitoring tools (prometheus, grafana and node exporter) for KubeStellar components (core and edge regions) - see architecture image above. Prometheus server is deployed in the core region running the KCP server alongside the components for KubeStellar. A Prometheus agent is deployed in the edge regions running the workload execution clusters.</p> <ol> <li>Create your hosts file with the list of target hosts (KCP server &amp; workload execution cluster)</li> </ol> <pre><code>[kcp-server]\n192.168.56.2\n\n[workload-execution-clusters]\n192.160.56.10\n192.160.56.12\n</code></pre> <ol> <li>Configure the prometheus targets endpoints:</li> </ol> <p>a) Prometheus Server: edit the file roles/prometheus/templates/prometheus-config.yaml.j2</p> <pre><code>global:\n  evaluation_interval: 5s\n  external_labels:\n    env: dev\n  scrape_interval: 30s\nscrape_configs:\n- job_name: mailbox-controller\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:10203'\n\n- job_name: node-exporter\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:9100'\n\n- job_name: kcp\n  scrape_interval: 15s\n  scheme: https\n  metrics_path: /metrics\n  tls_config:\n    insecure_skip_verify: true\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:6443'\n</code></pre> <p>b) Prometheus Agent: edit the file roles/prometheus/templates/prometheus-agent-config.yaml.j2</p> <pre><code># my global config\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s \nscrape_configs:\n  - job_name: \"prometheus-agent\"\n\n    static_configs:\n      - targets: [\"{ ansible_default_ipv4.address }:&lt;port_number&gt;\"]\nremote_write:\n  - url: \"http://&lt;prometheus-server-ip-address&gt;:&lt;port_number&gt;/api/v1/write\"\n</code></pre> <ol> <li>Install prometheus server, grafana and node exporter to the KCP server host using the following playbook:</li> </ol> <pre><code>- hosts: kcp-server\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n    - grafana\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-kcpServer.yaml\n</code></pre> <ol> <li>Install prometheus agent and node exporter to a workload execution cluster using the following playbook:</li> </ol> <pre><code>- hosts: workload-execution-clusters\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  vars:\n   agent: 'yes'\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-workload-execution-cluster.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/","title":"Development-Environment (dev-env)","text":"<p>Mostly under construction - coming soon</p>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#hosting-kubestellar-in-a-kind-cluster","title":"Hosting KubeStellar in a Kind cluster","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#create-a-kind-cluster-with-a-port-mapping","title":"Create a Kind cluster with a port mapping","text":"<p>Create a Kind cluster with the <code>extraPortMappings</code> for the Ingress controller, which will listen at port 443 on the one kind node.  We pick a port number here that does not run afoul of the usual prohibition of ordinary user processes listening at low port numbers.</p> <pre><code>kind create cluster --name ks-host --config=- &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 1119\n    protocol: TCP\nEOF\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#create-an-nginx-ingress-controller-with-ssl-passthrough","title":"Create an nginx Ingress controller with SSL passthrough","text":"<p>Create an <code>nginx-ingress</code> with SSL passthrough. Following Kind NGINX ingress instructions, we have modified the YAML at https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml to include the <code>--enable-ssl-passthrough=true</code> argument. This is the link to our raw modified nginx controller deployment YAML.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\n</code></pre> <p>Wait for the ingress to be ready:</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=90s\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#load-a-locally-built-container-image-into-the-kind-cluster","title":"Load a locally-built container image into the kind cluster","text":"<p>Remember that you can do this.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/","title":"Example1 post espw","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>KUBECONFIG=$SM_CONFIG mailbox-controller --external-access -v=4 &amp;&gt; /tmp/mailbox-controller.log &amp;\nsleep 20\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox spaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox spaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>while [ $(KUBECONFIG=$SM_CONFIG kubectl get spaces -A | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into /tmp/mailbox-controller.log if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI1218 16:36:14.743434  239027 controller.go:277] \"Created missing space\" worker=0 mbsName=\"imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\"\n...\nI1218 16:36:14.759426  239027 controller.go:277] \"Created missing space\" worker=1 mbsName=\"imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\"\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox spaces.</p> <p>A mailbox space name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox spaces as follows.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <pre><code>NAMESPACE               NAME                                           AGE\nspaceprovider-default   espw                                           84s\nspaceprovider-default   imw1                                           76s\nspaceprovider-default   imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   20s\nspaceprovider-default   imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   20s\nspaceprovider-default   wmw1                                           53s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -n spaceprovider-default -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                           SYNCTARGET   CLUSTER\nespw                                           &lt;none&gt;       &lt;none&gt;\nimw1                                           &lt;none&gt;       &lt;none&gt;\nimw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   florin       &lt;none&gt;\nimw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   guilder      &lt;none&gt;\nwmw1                                           &lt;none&gt;       &lt;none&gt;\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox space by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox space name is $GUILDER_SPACE\n</code></pre> <pre><code>The guilder mailbox space name is imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\n</code></pre></p> <p><pre><code>FLORIN_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox space name is $FLORIN_SPACE\n</code></pre> <pre><code>The florin mailbox space name is imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#connect-guilder-edge-cluster-with-its-mailbox-space","title":"Connect guilder edge cluster with its mailbox space","text":"<p>The following command will (a) create, in the mailbox space for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster guilder\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\".\n\nWrote workload execution cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\n\nto verify the syncer pod is running.\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system                                           coredns                                               2/2     2            2           4m1s\nkubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   1/1     1            1           0s\nlocal-path-storage                                    local-path-provisioner                                1/1     1            1           3m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#connect-florin-edge-cluster-with-its-mailbox-space","title":"Connect florin edge cluster with its mailbox space","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster florin\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\".\n\nWrote workload execution cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\n\nto verify the syncer pod is running.\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nserviceaccount/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\ndeployment.apps/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.  These commands create the WMW, makes KubeStellar edge APIs available for the WMW via kube-bind, and optionally makes some Kubernetes APIs available for the WMW by populating a set of CRDs into the WMW.</p> <pre><code># TODO: Needs to pass --with-kube false or true\nWMW_C_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-c.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-c --with-kube $kube_needed --output-kubeconfig \"$WMW_C_SPACE_CONFIG\"\nsleep 15\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code># TODO: needs to set --with-kube false/true\nWMW_S_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-s.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-s --with-kube $kube_needed --output-kubeconfig \"$WMW_S_SPACE_CONFIG\"\n</code></pre> <p>In this workload we will also demonstrate how to downsync objects whose kind is defined by a <code>CustomResourceDefinition</code> object. We will use the one from the Kubernetes documentation for CRDs, modified so that the resource it defines is in the category <code>all</code>. First, create the definition object with the following command.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;\n  name: crontabs.stable.example.com\nspec:\n  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;\n  group: stable.example.com\n  # list of versions supported by this CustomResourceDefinition\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                cronSpec:\n                  type: string\n                image:\n                  type: string\n                replicas:\n                  type: integer\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;\n    plural: crontabs\n    # singular name to be used as an alias on the CLI and for display\n    singular: crontab\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: CronTab\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - ct\n    categories:\n    - all\nEOF\n</code></pre> <p>Next, use the following command to wait for the apiserver to process that definition.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG wait --for condition=Established crd crontabs.stable.example.com\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n  annotations: {just-for: fun}\n---\napiVersion: \"stable.example.com/v1\"\nkind: CronTab\nmetadata:\n  name: my-new-cron-object\n  namespace: specialstuff\nspec:\n  cronSpec: \"* * * * */5\"\n  image: my-awesome-cron-image\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p>The \"what predicate\" explicitly includes the <code>Namespace</code> object named \"specialstuff\", which causes all of its desired state (including labels and annotations) to be downsynced. This contrasts with the common EdgePlacement, which does not explicitly mention the <code>commonstuff</code> namespace, relying on the implicit creation of namespaces as needed in the WECs.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    objectNames: [ v1090.example.my ]\n  - apiGroup: stable.example.com\n    resources: [ crontabs ]\n    namespaces: [ specialstuff ]\n    objectNames: [ my-new-cron-object ]\n  - apiGroup: \"\"\n    resources: [ namespaces ]\n    objectNames: [ specialstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar-where-resolver --external-access -v=4 &amp;&gt; /tmp/where-resolver.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>while ! kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nwhile ! kubectl --kubeconfig $WMW_S_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I1219 16:40:49.814564  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-florin\" location=\"kube-bind-sx6pl-florin\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n...\nI1219 16:40:49.797689  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-guilder\" location=\"kube-bind-sx6pl-guilder\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox spaces and create <code>SyncerConfig</code> objects there.</p> <p>If you have deployed the KubeStellar core as workload in a Kubernetes cluster then the placement translator is running in a Pod there. If instead you are running the core controllers as bare processes then use the following commands to launch the placement translator; it requires the ESPW to be current at start time.</p> <pre><code>KUBECONFIG=$SM_CONFIG placement-translator --external-access -v=4  &amp;&gt; /tmp/placement-translator.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait for the placement translator to get its job done for this example.</p> <pre><code>FLORIN_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${FLORIN_SPACE}.kubeconfig\"\nGUILDER_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${GUILDER_SPACE}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $FLORIN_SPACE --kubeconfig $SM_CONFIG $in_cluster $FLORIN_MB_KUBECONFIG\nkubectl-kubestellar-space-get_kubeconfig $GUILDER_SPACE --kubeconfig $SM_CONFIG $in_cluster $GUILDER_MB_KUBECONFIG\n# wait until SyncerConfig, ReplicaSets and Deployments are ready\nmbxws=($FLORIN_SPACE $GUILDER_SPACE)\nfor ii in \"${mbxws[@]}\"; do\n(\nexport KUBECONFIG=\"${MY_KUBECONFIGS}/${ii}.kubeconfig\"\n# wait for SyncerConfig resource\nwhile ! kubectl get SyncerConfig the-one &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* SyncerConfig resource exists in mailbox $ii\"\n# wait for ReplicaSet resource\nwhile ! kubectl get rs &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* ReplicaSet resource exists in mailbox $ii\"\n# wait until ReplicaSet in mailbox\nwhile ! kubectl get rs -n commonstuff commond; do\nsleep 10\ndone\necho \"* commonstuff ReplicaSet in mailbox $ii\"\n)\ndone\n(\nexport KUBECONFIG=$GUILDER_MB_KUBECONFIG\n# check for deployment in guilder\nwhile ! kubectl get deploy -A &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* Deployment resource exists\"\nwhile ! kubectl get deploy -n specialstuff speciald; do\nsleep 10\ndone\necho \"* specialstuff Deployment in its mailbox\"\n# wait for crontab CRD to be established\nwhile ! kubectl get crd crontabs.stable.example.com; do sleep 10; done\nkubectl wait --for condition=Established crd crontabs.stable.example.com\n  echo \"* CronTab CRD is established in its mailbox\"\n# wait for my-new-cron-object to be in its mailbox\nwhile ! kubectl get ct -n specialstuff my-new-cron-object; do sleep 10; done\necho \"* CronTab my-new-cron-object is in its mailbox\"\n)\n</code></pre> <p>You can check that the common workload's ReplicaSet objects got to their mailbox spaces with the following command. It will list the two copies of that object, each with an annotation whose key is <code>kcp.io/cluster</code> and whose value is the kcp <code>logicalcluster.Name</code> of the mailbox space; those names appear in the \"CLUSTER\" column of the custom-columns listing near the end of the section above about the mailbox controller.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1y7wll1dz806h3sb\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1najcltzt2nqax47\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n</code></pre> <p>That display should show objects in two different mailbox spaces; the following command checks that.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet | grep \"^ *kcp.io/cluster: [0-9a-z]*$\" | sort | uniq | wc -l) -ge 2\n</code></pre> <p>The CustomResourceDefinition objects involved should also appear in the mailbox spaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | fgrep -w \"name: crontabs.stable.example.com\"\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: replicasets.apps\") -ge 2\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: deployments.apps\") -ge 1\n</code></pre> <p>The <code>APIService</code> of the special workload should also appear, along with some error messages about <code>APIService</code> not being known in the other mailbox workspaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiregistration.k8s.io --api-kind APIService 2&gt;&amp;1 | grep -v \"APIService.*the server could not find the requested resource\" | fgrep -w \"name: v1090.example.my\"\n</code></pre> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize the name of the mailbox workspace for florin (which you stored in Stage 1) here.</p> <pre><code>KUBECONFIG=$FLORIN_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    resource: configmaps\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the mailbox workspace name that you stored in Stage 1.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  clusterScope:\n  - apiVersion: v1\n    group: \"\"\nobjects:\n    - specialstuff\n    resource: namespaces\n  - apiVersion: v1\n    group: apiextensions.k8s.io\n    objects:\n    - crontabs.stable.example.com\n    resource: customresourcedefinitions\n  - apiVersion: v1\n    group: apiregistration.k8s.io\n    objects:\n    - v1090.example.my\n    resource: apiservices\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  - apiVersion: v1\n    group: stable.example.com\n    objectsByNamespace:\n    - names:\n      - my-new-cron-object\n      namespace: specialstuff\n    resource: crontabs\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - speciald\n      namespace: specialstuff\n    resource: deployments\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    - names:\n      - httpd-htdocs\n      namespace: specialstuff\n    resource: configmaps\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p>You can check for specific workload objects here with the following command.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get deployments,replicasets -A\n</code></pre> <pre><code>NAMESPACE      NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff   deployment.apps/speciald   0/0     1            0           12m\n\nNAMESPACE     NAME                      DESIRED   CURRENT   READY   AGE\ncommonstuff   replicaset.apps/commond   0         1         1       7m4s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>( KUBECONFIG=~/.kube/config\n  let tries=1\nwhile ! kubectl --context kind-florin get ns commonstuff &amp;&gt; /dev/null; do\nif (( tries &gt;= 30)); then\necho 'The commonstuff namespace failed to appear in florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n    sleep 10\ndone\nkubectl --context kind-florin get ns\n)\nsleep 15\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy,rs -A | egrep 'NAME|stuff'\nsleep 15\n</code></pre> <pre><code>NAMESPACE                            NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\nNAMESPACE                            NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                          replicaset.apps/commond                                         1         1         1       13m\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces, the Deployment, and both ReplicaSets.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy,rs -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff                          deployment.apps/speciald                              1/1     1            1           23m\nNAMESPACE                             NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                           replicaset.apps/commond                                         1         1         1       23m\nspecialstuff                          replicaset.apps/speciald-76cdbb69b5                             1         1         1       14s\n</code></pre></p> <p>Examine the <code>APIService</code> objects in the guilder cluster, find the one named <code>v1090.example.my</code>. It is broken because it refers to a Service object that we have not bothered to create.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get apiservices | grep 1090\n</code></pre> <pre><code>v1090.example.my                       my-example/my-service   False (ServiceNotFound)   2m39s\n</code></pre></p> <p>See the crontab in the guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get crontabs -n specialstuff\nsleep 15\n</code></pre> <pre><code>NAME                 AGE\nmy-new-cron-object   37m\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get rs -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8094 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>let tries=1\nwhile ! curl http://localhost:8097 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The special workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8096 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/","title":"Example1 post provider","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/#get-binaries-of-kube-bind-and-dex","title":"Get binaries of kube-bind and dex","text":"<p>The command below makes kube-bind binaries and dex binary available in <code>$PATH</code>.</p> <pre><code>rm -rf kube-bind\ngit clone https://github.com/waltforme/kube-bind.git &amp;&amp; \\\npushd kube-bind &amp;&amp; \\\ngit checkout kubestellar &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd &amp;&amp; \\\ngit clone https://github.com/dexidp/dex.git &amp;&amp; \\\npushd dex &amp;&amp; \\\nIGNORE_GO_VERSION=1 make build &amp;&amp; \\\nexport PATH=$(pwd)/bin:$PATH &amp;&amp; \\\npopd\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the KubeStellar Core Space (KCS) (formerly called the Edge Service Provider Workspace (ESPW)), which exports the KubeStellar API.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar -X init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code> and the current context is for the KubeStellar hosting cluster.</p> <p>The following variable will be used in later commands to indicate that they are not being invoked from within the hosting cluster (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-provider/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>IMW1_KUBECONFIG=\"${MY_KUBECONFIGS}/imw1.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig imw1 --kubeconfig $SM_CONFIG $in_cluster $IMW1_KUBECONFIG\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nKUBECONFIG=$IMW1_KUBECONFIG kubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"describe the florin location object\"\nKUBECONFIG=$IMW1_KUBECONFIG kubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two <code>ensure location</code> calls are equivalent to creating the following four objects.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-pre-provider/","title":"Example1 pre provider","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox spaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-pre-provider/#your-kubeconfig-files","title":"Your kubeconfig files","text":"<p>This example (and others) involve user-managed kubeconfig files. These examples organize those files into one subdirectory. Start by making sure that directory exists without any contents left over from previous runs.</p> <pre><code>MY_KUBECONFIGS=${PWD}/my-kubeconfigs\nrm -rf \"$MY_KUBECONFIGS\"\nmkdir -p \"$MY_KUBECONFIGS\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-pre-provider/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kcp/","title":"Example1 provider kcp","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kcp/#deploy-kcp-and-kubestellar","title":"Deploy kcp and KubeStellar","text":"<p>You need kcp and KubeStellar and can deploy them in either of two ways: as bare processes on whatever host you are using to run this example, or as workload in a Kubernetes cluster (an OpenShift cluster qualifies).  Do one or the other, not both.</p> <p>KubeStellar only works with release <code>v0.11.0</code> of kcp.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kcp/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kcp/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.  See the start of the kcp quickstart for instructions on that, but get release v0.11.0 rather than the latest (the downloadable assets appear after the long list of changes/features/etc).</p> <p>Clone the v0.11.0 branch of the kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> and build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Running the kcp server creates a hidden subdirectory named <code>.kcp</code> to hold all sorts of state related to the server. If you have run it before and want to start over from scratch then you should <code>rm -rf .kcp</code> first.</p> <p>Use the following commands to: (a) run the kcp server in a forked command, (b) update your <code>KUBECONFIG</code> environment variable to configure <code>kubectl</code> to use the kubeconfig produced by the kcp server, and (c) wait for the kcp server to get through some initialization. The choice of <code>-v=3</code> for the kcp server makes it log a line for every HTTP request (among other things).</p> <pre><code>kcp start -v=3 &amp;&gt; /tmp/kcp.log &amp;\nPROVIDER_KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\n# wait until KCP is ready checking availability of ws resource\nwhile ! KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubectl ws tree &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that the kcp server creates. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kcp/#create-a-space-provider-description-for-kcp","title":"Create a space provider description for KCP","text":"<p>Space provider for KCP will allow you to use KCP as backend provider for spaces. Use the following commands to create a provider secret for KCP access and a space provider definition.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl create secret generic kcpsec --from-file=kubeconfig=$PROVIDER_KUBECONFIG --from-file=incluster=$PROVIDER_KUBECONFIG\nKUBECONFIG=$SM_CONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: space.kubestellar.io/v1alpha1\nkind: SpaceProviderDesc\nmetadata:\n  name: default\nspec:\n  ProviderType: \"kcp\"\n  SpacePrefixForDiscovery: \"ks-\"\n  secretRef:\n    namespace: default\n    name: kcpsec\nEOF\n</code></pre> <p>Next, use the following command to wait for the space-manger to process the provider.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl wait --for=jsonpath='{.status.Phase}'=Ready spaceproviderdesc/default --timeout=90s\n</code></pre> <p>The following variable will be used in later commands to indicate that they are being invoked close enough to the provider's apiserver to use the more efficient networking (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"--in-cluster\"\nkube_needed=true\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kubeflex/","title":"Example1 provider kubeflex","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kubeflex/#deploy-kubeflex-and-kubestellar-as-bare-processes","title":"Deploy KubeFlex and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kubeflex/#initialize-kubeflex","title":"Initialize KubeFlex","text":"<pre><code>wget https://github.com/kubestellar/kubeflex/releases/download/v0.3.3/kubeflex_0.3.3_linux_amd64.tar.gz\nmkdir kubeflex\ntar xf kubeflex_0.3.3_linux_amd64.tar.gz -C kubeflex\nkubeflex/bin/kflex --kubeconfig $SM_CONFIG init\nrm kubeflex_0.3.3_linux_amd64.tar.gz\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-provider-kubeflex/#create-a-space-provider-description-for-kubeflex","title":"Create a space provider description for KubeFlex","text":"<p>Space provider for KubeFlex will allow you to use KubeFlex as backend provider for spaces. Use the following commands to create a provider secret for KubeFlex access and a space provider definition.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl --context sm-mgt create secret generic kfsec --from-file=kubeconfig=$SM_CONFIG --from-file=incluster=$SM_CONFIG\nKUBECONFIG=$SM_CONFIG kubectl --context sm-mgt apply -f - &lt;&lt;EOF\napiVersion: space.kubestellar.io/v1alpha1\nkind: SpaceProviderDesc\nmetadata:\n  name: default\nspec:\n  ProviderType: \"kubeflex\"\n  SpacePrefixForDiscovery: \"ks-\"\n  secretRef:\n    namespace: default\n    name: kfsec\nEOF\n</code></pre> <p>Next, use the following command to wait for the space-manger to process the provider.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl --context sm-mgt wait --for=jsonpath='{.status.Phase}'=Ready spaceproviderdesc/default --timeout=90s\n</code></pre> <p>The following variable will be used in later commands to indicate that they are being invoked close enough to the provider's apiserver to use the more efficient networking (see doc on \"in-cluster\").</p> <pre><code>in_cluster=\"--in-cluster\"\nkube_needed=false\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-space-manager/","title":"Example1 space manager","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-space-manager/#create-kind-cluster-for-space-management","title":"Create Kind cluster for space management","text":"<pre><code>kind create cluster --name sm-mgt --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 9443\n    protocol: TCP\nEOF\nkubectl create -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\nsleep 20\nkubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=360s\n\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-sm-mgt sm-mgt\nSM_CONFIG=~/.kube/config\n</code></pre> <p>The subsequent uses of <code>$SM_CONFIG</code> in this example assume that the current context is still the one just established, \"sm-mgt\".</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-space-manager/#the-space-manager-controller","title":"The space-manager controller","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <p><pre><code>cd space-framework\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> Next deploy the space framework CRDs in the space management cluster. <pre><code>KUBECONFIG=$SM_CONFIG kubectl apply -f config/crds/\ncd ..\n</code></pre> Finally, start the space-manager controller.</p> <pre><code>space-manager --kubeconfig $SM_CONFIG --context sm-mgt -v 4 &amp;&gt; /tmp/space-manager.log &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1a/","title":"Example1 stage 1a","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1a/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>KUBECONFIG=$SM_CONFIG mailbox-controller --external-access -v=4 &amp;&gt; /tmp/mailbox-controller.log &amp;\nsleep 20\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox spaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox spaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>while [ $(KUBECONFIG=$SM_CONFIG kubectl get spaces -A | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into /tmp/mailbox-controller.log if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI1218 16:36:14.743434  239027 controller.go:277] \"Created missing space\" worker=0 mbsName=\"imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\"\n...\nI1218 16:36:14.759426  239027 controller.go:277] \"Created missing space\" worker=1 mbsName=\"imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\"\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox spaces.</p> <p>A mailbox space name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox spaces as follows.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <pre><code>NAMESPACE               NAME                                           AGE\nspaceprovider-default   espw                                           84s\nspaceprovider-default   imw1                                           76s\nspaceprovider-default   imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   20s\nspaceprovider-default   imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   20s\nspaceprovider-default   wmw1                                           53s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -n spaceprovider-default -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                           SYNCTARGET   CLUSTER\nespw                                           &lt;none&gt;       &lt;none&gt;\nimw1                                           &lt;none&gt;       &lt;none&gt;\nimw1-mb-11c4c16a-82f9-48ad-b036-879aba135959   florin       &lt;none&gt;\nimw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b   guilder      &lt;none&gt;\nwmw1                                           &lt;none&gt;       &lt;none&gt;\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox space by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox space name is $GUILDER_SPACE\n</code></pre> <pre><code>The guilder mailbox space name is imw1-mb-ace9fd79-1c35-410d-879d-d73ec6f9fe6b\n</code></pre></p> <p><pre><code>FLORIN_SPACE=$(KUBECONFIG=$SM_CONFIG kubectl get space -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox space name is $FLORIN_SPACE\n</code></pre> <pre><code>The florin mailbox space name is imw1-mb-11c4c16a-82f9-48ad-b036-879aba135959\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/","title":"Example1 stage 1b","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/#connect-guilder-edge-cluster-with-its-mailbox-space","title":"Connect guilder edge cluster with its mailbox space","text":"<p>The following command will (a) create, in the mailbox space for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster guilder\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\".\n\nWrote workload execution cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\" kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df\n\nto verify the syncer pod is running.\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system                                           coredns                                               2/2     2            2           4m1s\nkubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   kubestellar-syncer-kube-bind-sx6pl-guilder-2laxc4df   1/1     1            1           0s\nlocal-path-storage                                    local-path-provisioner                                1/1     1            1           3m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/#connect-florin-edge-cluster-with-its-mailbox-space","title":"Connect florin edge cluster with its mailbox space","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw imw1 $in_cluster florin\n</code></pre> <pre><code>Creating service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\nCreating cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to give service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n1. write and sync access to the synctarget \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to bind service account \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" to cluster role \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\".\n\nWrote workload execution cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\". Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;wec-config&gt; kubectl get deployment -n \"kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\" kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj\n\nto verify the syncer pod is running.\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nserviceaccount/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\nsecret/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\ndeployment.apps/kubestellar-syncer-kube-bind-sx6pl-florin-1pa812aj created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/","title":"Example1 stage 2","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.  These commands create the WMW, makes KubeStellar edge APIs available for the WMW via kube-bind, and optionally makes some Kubernetes APIs available for the WMW by populating a set of CRDs into the WMW.</p> <pre><code># TODO: Needs to pass --with-kube false or true\nWMW_C_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-c.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-c --with-kube $kube_needed --output-kubeconfig \"$WMW_C_SPACE_CONFIG\"\nsleep 15\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code># TODO: needs to set --with-kube false/true\nWMW_S_SPACE_CONFIG=${MY_KUBECONFIGS}/wmw-s.kubeconfig\nkubectl kubestellar ensure wmw $in_cluster wmw-s --with-kube $kube_needed --output-kubeconfig \"$WMW_S_SPACE_CONFIG\"\n</code></pre> <p>In this workload we will also demonstrate how to downsync objects whose kind is defined by a <code>CustomResourceDefinition</code> object. We will use the one from the Kubernetes documentation for CRDs, modified so that the resource it defines is in the category <code>all</code>. First, create the definition object with the following command.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;\n  name: crontabs.stable.example.com\nspec:\n  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;\n  group: stable.example.com\n  # list of versions supported by this CustomResourceDefinition\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                cronSpec:\n                  type: string\n                image:\n                  type: string\n                replicas:\n                  type: integer\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;\n    plural: crontabs\n    # singular name to be used as an alias on the CLI and for display\n    singular: crontab\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: CronTab\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - ct\n    categories:\n    - all\nEOF\n</code></pre> <p>Next, use the following command to wait for the apiserver to process that definition.</p> <pre><code>kubectl --kubeconfig $WMW_S_SPACE_CONFIG wait --for condition=Established crd crontabs.stable.example.com\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n  annotations: {just-for: fun}\n---\napiVersion: \"stable.example.com/v1\"\nkind: CronTab\nmetadata:\n  name: my-new-cron-object\n  namespace: specialstuff\nspec:\n  cronSpec: \"* * * * */5\"\n  image: my-awesome-cron-image\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p>The \"what predicate\" explicitly includes the <code>Namespace</code> object named \"specialstuff\", which causes all of its desired state (including labels and annotations) to be downsynced. This contrasts with the common EdgePlacement, which does not explicitly mention the <code>commonstuff</code> namespace, relying on the implicit creation of namespaces as needed in the WECs.</p> <pre><code># TODO: verify that APIservices is supported for Kubeflex control plan\nkubectl --kubeconfig $WMW_S_SPACE_CONFIG apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    objectNames: [ v1090.example.my ]\n  - apiGroup: stable.example.com\n    resources: [ crontabs ]\n    namespaces: [ specialstuff ]\n    objectNames: [ my-new-cron-object ]\n  - apiGroup: \"\"\n    resources: [ namespaces ]\n    objectNames: [ specialstuff ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\nsleep 10\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver.</p> <pre><code>KUBECONFIG=$SM_CONFIG kubestellar-where-resolver --external-access -v=4 &amp;&gt; /tmp/where-resolver.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>while ! kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nwhile ! kubectl --kubeconfig $WMW_S_SPACE_CONFIG get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I1219 16:40:49.814564  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-florin\" location=\"kube-bind-sx6pl-florin\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n...\nI1219 16:40:49.797689  325368 reconcile_on_location.go:213] \"updated SinglePlacementSlice\" controller=\"where-resolver\" triggeringKind=\"Location\" key=\"kube-bind-sx6pl-guilder\" location=\"kube-bind-sx6pl-guilder\" singlePlacementSlice=\"kube-bind-g72bg-edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl --kubeconfig $WMW_C_SPACE_CONFIG get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-3/","title":"Example1 stage 3","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-3/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox spaces and create <code>SyncerConfig</code> objects there.</p> <p>If you have deployed the KubeStellar core as workload in a Kubernetes cluster then the placement translator is running in a Pod there. If instead you are running the core controllers as bare processes then use the following commands to launch the placement translator; it requires the ESPW to be current at start time.</p> <pre><code>KUBECONFIG=$SM_CONFIG placement-translator --external-access -v=4  &amp;&gt; /tmp/placement-translator.log &amp;\nsleep 10\n</code></pre> <p>The following commands wait for the placement translator to get its job done for this example.</p> <pre><code>FLORIN_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${FLORIN_SPACE}.kubeconfig\"\nGUILDER_MB_KUBECONFIG=\"${MY_KUBECONFIGS}/${GUILDER_SPACE}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $FLORIN_SPACE --kubeconfig $SM_CONFIG $in_cluster $FLORIN_MB_KUBECONFIG\nkubectl-kubestellar-space-get_kubeconfig $GUILDER_SPACE --kubeconfig $SM_CONFIG $in_cluster $GUILDER_MB_KUBECONFIG\n# wait until SyncerConfig, ReplicaSets and Deployments are ready\nmbxws=($FLORIN_SPACE $GUILDER_SPACE)\nfor ii in \"${mbxws[@]}\"; do\n(\nexport KUBECONFIG=\"${MY_KUBECONFIGS}/${ii}.kubeconfig\"\n# wait for SyncerConfig resource\nwhile ! kubectl get SyncerConfig the-one &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* SyncerConfig resource exists in mailbox $ii\"\n# wait for ReplicaSet resource\nwhile ! kubectl get rs &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* ReplicaSet resource exists in mailbox $ii\"\n# wait until ReplicaSet in mailbox\nwhile ! kubectl get rs -n commonstuff commond; do\nsleep 10\ndone\necho \"* commonstuff ReplicaSet in mailbox $ii\"\n)\ndone\n(\nexport KUBECONFIG=$GUILDER_MB_KUBECONFIG\n# check for deployment in guilder\nwhile ! kubectl get deploy -A &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* Deployment resource exists\"\nwhile ! kubectl get deploy -n specialstuff speciald; do\nsleep 10\ndone\necho \"* specialstuff Deployment in its mailbox\"\n# wait for crontab CRD to be established\nwhile ! kubectl get crd crontabs.stable.example.com; do sleep 10; done\nkubectl wait --for condition=Established crd crontabs.stable.example.com\n  echo \"* CronTab CRD is established in its mailbox\"\n# wait for my-new-cron-object to be in its mailbox\nwhile ! kubectl get ct -n specialstuff my-new-cron-object; do sleep 10; done\necho \"* CronTab my-new-cron-object is in its mailbox\"\n)\n</code></pre> <p>You can check that the common workload's ReplicaSet objects got to their mailbox spaces with the following command. It will list the two copies of that object, each with an annotation whose key is <code>kcp.io/cluster</code> and whose value is the kcp <code>logicalcluster.Name</code> of the mailbox space; those names appear in the \"CLUSTER\" column of the custom-columns listing near the end of the section above about the mailbox controller.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1y7wll1dz806h3sb\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\n    kcp.io/cluster: 1najcltzt2nqax47\n    ... (lots of other details) ...\n  name: commond\n  namespace: commonstuff\nspec:\n  ... (the customized spec) ...\nstatus:\n  ... (may be filled in by the time you look) ...\n</code></pre> <p>That display should show objects in two different mailbox spaces; the following command checks that.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apps --api-kind ReplicaSet | grep \"^ *kcp.io/cluster: [0-9a-z]*$\" | sort | uniq | wc -l) -ge 2\n</code></pre> <p>The CustomResourceDefinition objects involved should also appear in the mailbox spaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | fgrep -w \"name: crontabs.stable.example.com\"\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: replicasets.apps\") -ge 2\ntest $(KUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiextensions.k8s.io --api-kind CustomResourceDefinition | grep -cw \"name: deployments.apps\") -ge 1\n</code></pre> <p>The <code>APIService</code> of the special workload should also appear, along with some error messages about <code>APIService</code> not being known in the other mailbox workspaces.</p> <pre><code># TODO: kubestellar-list-syncing-objects has kcp dependencies. Will remove when controllers support spaces.\nKUBECONFIG=\"$PROVIDER_KUBECONFIG\" kubestellar-list-syncing-objects --api-group apiregistration.k8s.io --api-kind APIService 2&gt;&amp;1 | grep -v \"APIService.*the server could not find the requested resource\" | fgrep -w \"name: v1090.example.my\"\n</code></pre> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize the name of the mailbox workspace for florin (which you stored in Stage 1) here.</p> <pre><code>KUBECONFIG=$FLORIN_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    resource: configmaps\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the mailbox workspace name that you stored in Stage 1.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  clusterScope:\n  - apiVersion: v1\n    group: \"\"\nobjects:\n    - specialstuff\n    resource: namespaces\n  - apiVersion: v1\n    group: apiextensions.k8s.io\n    objects:\n    - crontabs.stable.example.com\n    resource: customresourcedefinitions\n  - apiVersion: v1\n    group: apiregistration.k8s.io\n    objects:\n    - v1090.example.my\n    resource: apiservices\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  - apiVersion: v1\n    group: stable.example.com\n    objectsByNamespace:\n    - names:\n      - my-new-cron-object\n      namespace: specialstuff\n    resource: crontabs\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - speciald\n      namespace: specialstuff\n    resource: deployments\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    - names:\n      - httpd-htdocs\n      namespace: specialstuff\n    resource: configmaps\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p>You can check for specific workload objects here with the following command.</p> <p><pre><code>KUBECONFIG=$GUILDER_MB_KUBECONFIG kubectl get deployments,replicasets -A\n</code></pre> <pre><code>NAMESPACE      NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff   deployment.apps/speciald   0/0     1            0           12m\n\nNAMESPACE     NAME                      DESIRED   CURRENT   READY   AGE\ncommonstuff   replicaset.apps/commond   0         1         1       7m4s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-4/","title":"Example1 stage 4","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-4/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>( KUBECONFIG=~/.kube/config\n  let tries=1\nwhile ! kubectl --context kind-florin get ns commonstuff &amp;&gt; /dev/null; do\nif (( tries &gt;= 30)); then\necho 'The commonstuff namespace failed to appear in florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n    sleep 10\ndone\nkubectl --context kind-florin get ns\n)\nsleep 15\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy,rs -A | egrep 'NAME|stuff'\nsleep 15\n</code></pre> <pre><code>NAMESPACE                            NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\nNAMESPACE                            NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                          replicaset.apps/commond                                         1         1         1       13m\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces, the Deployment, and both ReplicaSets.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy,rs -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff                          deployment.apps/speciald                              1/1     1            1           23m\nNAMESPACE                             NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                           replicaset.apps/commond                                         1         1         1       23m\nspecialstuff                          replicaset.apps/speciald-76cdbb69b5                             1         1         1       14s\n</code></pre></p> <p>Examine the <code>APIService</code> objects in the guilder cluster, find the one named <code>v1090.example.my</code>. It is broken because it refers to a Service object that we have not bothered to create.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get apiservices | grep 1090\n</code></pre> <pre><code>v1090.example.my                       my-example/my-service   False (ServiceNotFound)   2m39s\n</code></pre></p> <p>See the crontab in the guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get crontabs -n specialstuff\nsleep 15\n</code></pre> <pre><code>NAME                 AGE\nmy-new-cron-object   37m\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get rs -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8094 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>let tries=1\nwhile ! curl http://localhost:8097 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The special workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8096 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-teardown/","title":"Example1 teardown","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default imw1\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default $FLORIN_SPACE\nKUBECONFIG=$SM_CONFIG kubectl delete space -n spaceprovider-default $GUILDER_SPACE\nkubectl kubestellar remove wmw wmw-c\nkubectl kubestellar remove wmw wmw-s\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-teardown/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-teardown/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-0-deploy-florin/","title":"Kubestellar syncer 0 deploy florin","text":"<p>Go to KCS and find the mailbox space name. <pre><code>pvname=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io | grep florin | awk '{print $1}'`\nstuid=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io $pvname -o jsonpath=\"{.metadata.uid}\"`\nmbs_name=\"imw1-mb-$stuid\"\necho \"mailbox space name = $mbs_name\"\n</code></pre> <pre><code>mailbox space name = vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\n</code></pre></p> <p>Go to the mailbox space and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>mbs_kubeconfig=\"${MY_KUBECONFIGS}/${mbs_name}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig ${mbs_name} $in_cluster --kubeconfig $SM_CONFIG $mbs_kubeconfig\n./bin/kubectl-kubestellar-syncer_gen --kubeconfig $mbs_kubeconfig florin --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o florin-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\".\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote workload execution cluster (WEC) manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-florin-32uaph9l    kubestellar-syncer-florin-32uaph9l    1/1     1            1           42s\nkube-system                           coredns                               2/2     2            2           41m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           41m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-0-deploy-guilder/","title":"Kubestellar syncer 0 deploy guilder","text":"<p>Go to KCS and find the mailbox space name. <pre><code>espw_kubeconfig=\"${MY_KUBECONFIGS}/espw.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig espw $in_cluster --kubeconfig $SM_CONFIG $espw_kubeconfig\npvname=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io | grep guilder | awk '{print $1}'`\nstuid=`kubectl --kubeconfig $espw_kubeconfig get synctargets.edge.kubestellar.io $pvname -o jsonpath=\"{.metadata.uid}\"`\nmbs_name=\"imw1-mb-$stuid\"\necho \"mailbox space name = $mbs_name\"\n</code></pre></p> <pre><code>mailbox space name = vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\n</code></pre> <p>Go to the mailbox space and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>mbs_kubeconfig=\"${MY_KUBECONFIGS}/${mbs_name}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig ${mbs_name} $in_cluster --kubeconfig $SM_CONFIG $mbs_kubeconfig\n./bin/kubectl-kubestellar-syncer_gen --kubeconfig $mbs_kubeconfig guilder --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o guilder-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\".\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                           coredns                               2/2     2            2           35m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-1-syncer-gen-plugin/","title":"Kubestellar syncer 1 syncer gen plugin","text":"<p>The remainder is not fully debugged.</p> <pre><code>exit 0\n</code></pre> <p>Generate UUID for Syncer identification. <pre><code>syncer_id=\"syncer-\"`uuidgen | tr '[:upper:]' '[:lower:]'`\n</code></pre></p> <p>Go to a workspace. <pre><code>kubectl ws root\nkubectl ws create ws1 --enter\n</code></pre></p> <p>Create the following APIBinding in the workspace (Note that in the case of mailbox workspaces, it's done by mailbox controller at creating the mailbox workspace.) <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: bind-espw\nspec:\n  reference:\n    export:\n      path: root:espw\n      name: edge.kubestellar.io\nEOL\n</code></pre></p> <p>Create a serviceaccount in the workspace. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: $syncer_id\nEOL\n</code></pre></p> <p>Create clusterrole and clusterrolebinding to bind the serviceaccount to the role. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: $syncer_id\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- nonResourceURLs: [\"/\"]\n  verbs: [\"access\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: $syncer_id\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: $syncer_id\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: $syncer_id\n  namespace: default\nEOL\n</code></pre></p> <p>Get the serviceaccount token that will be set in the upstream kubeconfig manifest.  NOTE This part is outdated due to the recent denaturing of ServiceAccounts in kcp workspaces; the syncer-gen plugin will actually wait a little while and then create the Secret if nothing else has. <pre><code>secret_name=`kubectl get secret -o custom-columns=\":.metadata.name\"| grep $syncer_id`\ntoken=`kubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d`\n</code></pre></p> <p>Get the certificates that will be set in the upstream kubeconfig manifest. <pre><code>cacrt=`kubectl config view --minify --raw | yq \".clusters[0].cluster.certificate-authority-data\"`\n</code></pre></p> <p>Get server_url that will be set in the upstream kubeconfig manifest. <pre><code>server_url=`kubectl config view --minify --raw | yq \".clusters[0].cluster.server\" | sed -e 's|https://\\(.*\\):\\([^/]*\\)/.*|https://\\1:\\2|g'`\n</code></pre></p> <p>Set some other parameters. a. downstream_namespace where Syncer Pod runs <pre><code>downstream_namespace=\"kubestellar-$syncer_id\"\n</code></pre> b. Syncer image <pre><code>image=\"quay.io/kubestellar/syncer:v0.2.2\"\n</code></pre></p> <p>Download manifest template. <pre><code>curl -LO https://raw.githubusercontent.com/kubestellar/kubestellar/main/pkg/syncer/scripts/kubestellar-syncer-bootstrap.template.yaml\n</code></pre></p> <p>Generate manifests to bootstrap KubeStellar-Syncer. <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image envsubst &lt; kubestellar-syncer-bootstrap.template.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubestellar-syncer-9ee90de6-eb76-4ddb-9346-c4c8d92075e1\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n...\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller-subs/mailbox-controller-process-start-without-cd-kubestellar/","title":"Mailbox controller process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\ngo run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller-subs/mailbox-controller-process-start/","title":"Mailbox controller process start","text":"<pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator-subs/placement-translator-process-start-without-cd-kubestellar/","title":"Placement translator process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\nplacement-translator &amp;\nsleep 120\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator-subs/placement-translator-process-start/","title":"Placement translator process start","text":"<pre><code>kubectl ws root:espw\ncd ../kubestellar\ngo run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-0-pull-kcp-and-kubestellar-source-and-start-kcp/","title":"Where resolver 0 pull kcp and kubestellar source and start kcp","text":"<p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window). Set your <code>KUBECONFIG</code> environment variable to name the kubernetes client config file that <code>kcp</code> generates. <pre><code>kcp start &amp;&gt; /dev/null &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-1-build-kubestellar/","title":"Where resolver 1 build kubestellar","text":"<pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-2-ws-root-and-ws-create-edge/","title":"Where resolver 2 ws root and ws create edge","text":"<p>Next, use the command that makes sure the Edge Service Provider Workspace (ESPW), which is <code>root:espw</code>, is properly set up.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-exports/","title":"Where resolver exports","text":"<pre><code>kubectl ws root:espw\nkubectl apply -f ../kubestellar/config/exports/\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-imports/","title":"Where resolver imports","text":"<p>Use the user home workspace (\\~) as the workload management workspace (WMW). <pre><code>kubectl ws \\~\n</code></pre></p> <p>Bind APIs. <pre><code>kubectl apply -f config/imports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-process-start-without-cd-kubestellar/","title":"Where resolver process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\ngo run cmd/kubestellar-where-resolver/main.go -v 2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-process-start/","title":"Where resolver process start","text":"<pre><code>kubectl ws root:espw\nkubestellar-where-resolver -v 2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q4/outline/","title":"Details","text":"<p>Want to get involved? Check out our good-first-issue list.</p> <p>TODO: draw new picture</p>"},{"location":"Coding%20Milestones/PoC2023q4/outline/#status-of-this-memo","title":"Status of this memo","text":"<p>This document outlines near-term plans for building on what was produced in the work on the PoC for 2023q1.</p>"},{"location":"Coding%20Milestones/PoC2023q4/outline/#introduction","title":"Introduction","text":"<p>PoC2023q1 was defined with the over-arching goal of supporting edge computing scenarios.  Since then we have realized that the technical problems that we took on are not that specific, they appear in other multi-cluster scenarios as well.</p> <p>The goals of this PoC are as follows.  Ones that are substantially different from what has been accomplished for PoC2023q1 are highlighted.</p> <ul> <li>Separation of infrastructure and workload management.</li> <li>The focus here is on workload management, and that strictly reads   an inventory of infrastructure.</li> <li>What passes from inventory to workload management is kcp TMC   Location and SyncTarget objects.</li> <li>Compared to PoC2023q1, decoupling from kcp TMC by making our own   copy of the definitions of SyncTarget and Location.</li> <li>Potentially: switch from using SyncTarget and Location to some   other representation of inventory.</li> <li>Compared to PoC2023q1, decoupling from kcp core by (1) introducing   an abstraction layer that delivers the essential functionality of   kcp's logical clusters based on a variety of implementations and (2)   using kube-bind instead of   kcp's APIExport/APIBinding.  Where PoC2023q1 used the concept of a   kcp workspace, PoC2023q4 uses the abstract concept that we call a   \"space\".</li> <li>Use of a space as the container for the central spec of a workload.</li> <li>Propagation of desired state from center outward, as directed by   EdgePlacement objects and the referenced inventory objects.</li> <li>Interfaces designed for a large number of workload execution clusters.</li> <li>Interfaces designed with the intention that workload execution   clusters operate independently of each other and the center (e.g.,   can tolerate only occasional connectivity) and thus any \"service   providers\" (in the technical sense from kcp) in the center or   elsewhere.</li> <li>Rule-based customization of desired state.</li> <li>Propagation of reported state from workload execution clusters to center.</li> <li>Summarization of reported state in the center.</li> <li>Exact, not summarized, reported state returned to workload   description space in the case of placement on exactly 1 workload   execution cluster.</li> <li>Return and/or summarization of state from associated objects (e.g.,   ReplicaSet or Pod objects associated with a given Deployment   object).</li> <li>The TCP connections are opened in the inward direction, not outward.</li> <li>A platform \"product\" that can be deployed (as opposed to a service   that is used).</li> <li>Codified support for scenarios where some KubeStellar clients and   the syncers in some of the workload execution clusters have to go   through load balancers and/or other proxies to reach the central   server(s).</li> <li>Compared to PoC2023q1, codification of closer to production grade   deployment technique(s).</li> <li>A hierarchy with more than two levels.</li> </ul> <p>Some important things that are not attempted in this PoC include the following.</p> <ul> <li>An implementation that supports a very large volume of reported   state (which could come from either a large number of workload   execution clusters and/or a large amount of reported state in each   one of those).</li> <li>User control over ordering of propagation from center outward,   either among destinations or kinds of objects.</li> <li>More than baseline security (baseline being, e.g., HTTPS, Secret   objects, non-rotating bearer token based service authentication).</li> <li>A good design for bootstrapping the workload management in the   workload execution clusters.</li> <li>Very strong isolation between tenants of this platform.</li> </ul>"},{"location":"Community/_index/","title":"Join the KubeStellar community","text":""},{"location":"Community/_index/#kubestellar-is-an-open-source-project-that-anyone-in-the-community-can-use-improve-and-enjoy-join-us-heres-a-few-ways-to-find-out-whats-happening-and-get-involved","title":"KubeStellar is an open source project that anyone in the community can use, improve, and enjoy. Join us! Here's a few ways to find out what's happening and get involved","text":""},{"location":"Community/_index/#learn-and-connect","title":"Learn and Connect","text":""},{"location":"Community/_index/#using-or-want-to-use-kubestellar-find-out-more-here","title":"Using or want to use KubeStellar? Find out more here:","text":"<ul> <li>User mailing list: Discussion and help from your fellow users</li> <li>YouTube Channel: Follow us on YouTube to view recordings of past KubeStellar community meetings and demo days</li> <li>LinkedIn: See what others are saying about the community</li> <li>Medium Blog Series: Follow us on Medium to read about community developments</li> </ul>"},{"location":"Community/_index/#develop-and-contribute","title":"Develop and Contribute","text":""},{"location":"Community/_index/#if-you-want-to-get-more-involved-by-contributing-to-kubestellar-join-us-here","title":"If you want to get more involved by contributing to KubeStellar, join us here:","text":"<ul> <li>GitHub: Development takes place here!</li> <li>#kubestellar-dev Slack channel in the Kubernetes slack workspace: Chat with other project developers</li> <li>Developer mailing list: Discuss development issues around the project</li> <li>You can find out how to contribute to KubeStellar in our Contribution Guidelines</li> </ul>"},{"location":"Community/_index/#community-meetings","title":"Community Meetings","text":"<ol> <li>Join our Developer mailing list to get your community meeting invitation.</li> <li>You can also directly subscribe to the community calendar, or view our calendar</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Sign up to discuss a topic in the KubeStellar Community Meeting Agenda</li> </ol>"},{"location":"Community/_index/#other-resources","title":"Other Resources","text":"<ul> <li>Google Drive</li> </ul>"},{"location":"Community/partners/argocd/","title":"ArgoCD","text":"<p>This document explains how to add KubeStellar's 'workspaces' as Argo CD's 'clusters'.</p>"},{"location":"Community/partners/argocd/#add-kubestellars-workspaces-to-argo-cd-as-clusters","title":"Add KubeStellar's workspaces to Argo CD as clusters","text":"<p>As of today, the 'workspaces', aka 'logical clusters' used by KubeStellar are not identical with ordinary Kubernetes clusters. Thus, in order to add them as Argo CD's 'clusters', there are a few more steps to take.</p> <p>For KubeStellar's Inventory Management Workspace (IMW) and Workload Management Workspace (WMW). The steps are similar. Let's take WMW as an example:</p> <ol> <li>Create `kube-system` namespace in the workspace.</li> <li>Make sure necessary apibindings exist in the workspace.  For WMW, we need one for Kubernetes and one for KubeStellar's edge API.</li> <li>Exclude `ClusterWorkspace` from discovery and sync.  <pre><code>kubectl -n argocd edit cm argocd-cm\n</code></pre>  Make sure `resource.exclusions` exists in the `data` field of the `argocd-cm` configmap as follows: <pre><code>data:\nresource.exclusions: |\n- apiGroups:\n- \"tenancy.kcp.io\"\nkinds:\n- \"ClusterWorkspace\"\nclusters:\n- \"*\"\n</code></pre>  Restart the Argo CD server. <pre><code>kubectl -n argocd rollout restart deployment argocd-server\n</code></pre>  Argo CD's documentation mentions this feature as [Resource Exclusion/Inclusion](https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion). </li> <li>Make sure the current context uses WMW, then identify the admin.kubeconfig. The command and output should be similar to <pre><code>$ argocd cluster add --name wmw --kubeconfig ./admin.kubeconfig workspace.kcp.io/current\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `workspace.kcp.io/current` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" updated\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" updated\nCluster 'https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo' added\n</code></pre>  ### Create Argo CD Applications Once KubeStellar's workspaces are added, Argo CD Applications can be created as normal. There are a few examples listed [here](https://github.com/edge-experiments/gitops-source/tree/main/kubestellar), and the commands to use the examples are listed as follows.  #### Create Argo CD Applications against KubeStellar's IMW Create two Locations. The command and output should be similar to <pre><code>$ argocd app create locations \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/locations/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'locations' created\n</code></pre>  Create two SyncTargets. The command and output should be similar to <pre><code>$ argocd app create synctargets \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/synctargets/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'synctargets' created\n</code></pre>  #### Create Argo CD Application against KubeStellar's WMW Create a Namespace. The command and output should be similar to <pre><code>$ argocd app create namespace \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/namespaces/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'namespace' created\n</code></pre>  Create a Deployment for 'cpumemload'. The command and output should be similar to <pre><code>$ argocd app create cpumemload \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/workloads/cpumemload/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'cpumemload' created\n</code></pre>  Create an EdgePlacement. The command and output should be similar to <pre><code>$ argocd app create edgeplacement \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/placements/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'edgeplacement' created\n</code></pre> </li> </ol>"},{"location":"Community/partners/argocd/#other-resources","title":"Other Resources","text":"<p>Medium - Sync 10,000 ArgoCD Applications in One Shot Medium - Sync 10,000 ArgoCD Applications in One Shot, by Yourself Medium - GitOpsCon - here we come</p>"},{"location":"Community/partners/argocd/#argocd-scale-experiment-kubestellar-community-demo-day","title":"ArgoCD Scale Experiment - KubeStellar Community Demo Day","text":""},{"location":"Community/partners/argocd/#gitopscon-2023-a-quantitative-study-on-argo-scalability-andrew-anderson-jun-duan-ibm","title":"GitOpsCon 2023 - A Quantitative Study on Argo Scalability - Andrew Anderson &amp; Jun Duan, IBM","text":""},{"location":"Community/partners/argocd/#argocd-and-kubestellar-in-the-news","title":"ArgoCD and KubeStellar in the news","text":""},{"location":"Community/partners/fluxcd/","title":"FluxCD","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/kyverno/","title":"Check out KubeStellar working with Kyverno:","text":"<p>Medium - Syncing Objects from one Kubernetes cluster to another Kubernetes cluster</p>"},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-demo-day","title":"Kyverno and KubeStellar Demo Day","text":""},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-in-the-news","title":"Kyverno and KubeStellar in the news","text":""},{"location":"Community/partners/kyverno/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/","title":"Check out KubeStellar working with IBM's Maximo Visual Inspection (MVI):","text":"<p>Medium - Deployment and configuration of MVI-Edge using KubeStellar</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-demo-day","title":"MVI and KubeStellar Demo Day","text":""},{"location":"Community/partners/mvi/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-in-the-news","title":"MVI and KubeStellar in the news","text":""},{"location":"Community/partners/openziti/","title":"OpenZiti","text":""},{"location":"Community/partners/turbonomic/","title":"Check out KubeStellar working with Turbonomic:","text":"<p>Medium - Make Multi-Cluster Scheduling a No-Brainer</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-demo-day","title":"Turbonomic and KubeStellar Demo Day","text":""},{"location":"Community/partners/turbonomic/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>As we can see from the blog and the demo, Turbonomic talks to KubeStellar via GitOps. The scheduling decisions are passed from Turbonomic to KubeStellar in two steps: 1. Turbo -&gt; GitHub repository. 2. GitHub repository -&gt; KubeStellar.</p> <p>For the 1st step (Turbonomic -&gt; GitHub repository), a controller named \"change reconciler\" creates PRs against the GitHub repository, where the PRs contains changes to scheduling decisions.</p> <p>There's also a piece of code which intercepts Turbonomic actions and creates CRs for the above change reconciler.</p> <p>For the 2nd step (GitHub repository-&gt; KubeStellar), we can use Argo CD. The detailed procedure to integrate Argo CD with KubeStellar is documented here.</p> <p>As we can see from the blog and the demo, Turbonomic collects data from edge clusters. This is made possible by installing kubeturbo into each of the edge clusters.</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-in-the-news","title":"Turbonomic and KubeStellar in the news","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through this document and familiarize yourself with our code of conduct. If you have any inquiries, please feel free to reach out to us on the KubeStellar-dev Slack channel.</p> <p>We can't wait to collaborate with you!</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#contributing-code","title":"Contributing Code","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#prerequisites","title":"Prerequisites","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#go","title":"Go","text":"<p>Install Go 1.19+.  See this gist for another way to install Go.   Please note that the go language version numbers in these files must exactly agree:</p> <pre><code>Your local go/go.mod file, kcp/.ci-operator.yaml, and in all the kcp/.github/workflows yaml files that specify go-version.\n\n- In ./ci-operator.yaml the go version is indicated by the \"tag\" attribute.\n- In go.mod it is indicated by the \"go\" directive.\n- In the .github/workflows yaml files it is indicated by \"go-version\"\n</code></pre> <p>Check out our QuickStart Guide</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#other-packages","title":"Other packages","text":"<ul> <li>GNU make</li> <li>ko (required for compiling KubeStellar Syncer)</li> <li>slsa-verifier needed in Ubuntu for ko signing</li> </ul>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#issues","title":"Issues","text":"<p>Prioritization for pull requests is given to those that address and resolve existing GitHub issues. Utilize the available issue labels to identify meaningful and relevant issues to work on.</p> <p>If you believe that there is a need for a fix and no existing issue covers it, feel free to create a new one.</p> <p>As a new contributor, we encourage you to start with issues labeled as good first issues.</p> <p>Your assistance in improving documentation is highly valued, regardless of your level of experience with the project.</p> <p>To claim an issue that you are interested in, kindly leave a comment on the issue and request the maintainers to assign it to you.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#committing","title":"Committing","text":"<p>We encourage all contributors to adopt best practices in git commit management to facilitate efficient reviews and retrospective analysis. Your git commits should provide ample context for reviewers and future codebase readers.</p> <p>A recommended format for final commit messages is as follows:</p> <pre><code>{Short Title}: {Problem this commit is solving and any important contextual information} {issue number if applicable}\n</code></pre>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>When submitting a pull request, clear communication is appreciated. This can be achieved by providing the following information:</p> <ul> <li>Detailed description of the problem you are trying to solve, along with links to related GitHub issues</li> <li>Explanation of your solution, including links to any design documentation and discussions</li> <li>Information on how you tested and validated your solution</li> <li>Updates to relevant documentation and examples, if applicable</li> </ul> <p>The pull request template has been designed to assist you in communicating this information effectively.</p> <p>Smaller pull requests are typically easier to review and merge than larger ones. If your pull request is big, it is always recommended to collaborate with the maintainers to find the best way to divide it.</p> <p>Approvers will review your PR within a business day. A PR requires both an /lgtm and then an /approve in order to get merged. You may /approve your own PR but you may not /lgtm it. Automation will add the PR it to the OpenShift PR merge queue. The OpenShift Tide bot will automatically merge your work when it is available.</p> <p>Congratulations! Your pull request has been successfully merged! \ud83d\udc4f</p> <p>If you have any questions about contributing, don't hesitate to reach out to us on the KubeStellar-dev Slack channel.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-locally","title":"Testing Locally","text":"<p>Our QuickStart  guide shows a user how to install a local space provider and install the KubeStellar components and run an example.  As a contributor you will want a different setup flow, including <code>git clone</code> of this repo instead of fetching and unpacking a release archive.  The same example usage should work for you, and there is a larger example at this link.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-changes-to-the-kubestellar-central-container-image","title":"Testing changes to the KubeStellar central container image","text":"<p>If you make a change that affects the container image holding the central components then you will need to build a new image; perhaps surprisingly, this is not included in <code>make build</code>.  The regular way to build this image is with the following command.  It builds a multi-platform image, for all the platforms that KubeStellar can run its central components on, and pushes it to quay.io.  Read the remarks below before trying this.</p> <pre><code>make kubestellar-image\n</code></pre> <p>The set of target platforms can be specified by setting the <code>CORE_PLATFORMS</code> variable. The following command is equivalent to the default behavior.</p> <pre><code>make kubestellar-image CORE_PLATFORMS=linux/amd64,linux/arm64,linux/ppc64le\n</code></pre> <p>NOTE VERY SHARP AND BURIED EDGE: IF the target platforms include   <code>linux/amd64</code> --- either because you explicitly set that or you let   the default setting apply --- then you MUST issue this command on a   machine (real or virtual) with the x86-64-v2   instructions. \"x86-64-v2\" is a shorthand for a bundle of instruction   set features that have been appearing in x86 chips for many years   now (any real machine that you are likely to use today has them) but   still do not all appear by default in some common emulators. See   QEMU configuration   recommendations,   for example. If the machine lacks the v2 instructions then the build   will fail when it tries to use the glibc in the redhat/ubi9   image. Cross-platform building when the builder is NOT x86 and the   target IS x86 is beyond the ken of modern technology (see   here and   here). If   you somehow succeed to build for the target platform   <code>linux/amd64/v2</code> and successfully test on real x86 hardware you   still are not done: when you try to use this image in OpenShift on   x86 you may get inexplicable failures to pull the image.</p> <p>The command shown above will only succeed if you have done <code>docker login</code> to quay.io with credentials authorized to write to the <code>kubestellar/kubestellar</code> repository. Look on quay.io to find the image you just pushed, you will soon need to use one of its tags. This make target pushes the image with two tags, one based on build timestamp and one based on git metadata.</p> <p>If you are not authorized to write to <code>quay.io/kubestellar/kubestellar</code> then you can specify an alternate image repository: put it in the make variable named <code>CORE_IMAGE_REPO</code>.  For example, you might invoke <code>make kubestellar-image CORE_IMAGE_REPO=docker.io/myacct/ksctr</code>.</p> <p>Another variable that you might like to use is <code>EXTRA_CORE_TAG</code>. This causes the make command to push the image with a third tag that you supply in that variable.  For example, if you want to tag the image with a release tag you might invoke <code>make kubestellar-image EXTRA_CORE_TAG=v0.42.7</code>.</p> <p>For a less pushy alternative you can build a single-platform image and not push it, using the following command. It also supports the <code>CORE_IMAGE_REPO</code> and <code>EXTRA_CORE_TAG</code> variables.  But it only builds for your local \"platform\"; you can use this if you have podman pretending to be docker.</p> <pre><code>make kubestellar-image-local\n</code></pre> <p>Follow that with <code>docker images</code> to find the tag of the image you just built.  Get that image:tag known where you are going to run the central container; for example, if that will be in a local <code>kind</code> cluster then you can use kind load.</p> <p>To get the image you just built used in your testing, edit <code>scripts/outer/kubectl-kubestellar-deploy</code> and update the line that defines <code>$image_tag</code>; follow this with your <code>make build</code>.  For the sake of future users of a merged change, your last edit like this should refer to a tag that you pushed to quay.io/kubestellar/kubestellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-changes-to-the-bootstrap-script","title":"Testing changes to the bootstrap script","text":"<p>The quickstart says to fetch the bootstrap script from the main branch of the KubeStellar repo; if you want to contribute a change to that script then you will need to test your changed version.  Just run your local copy (perhaps in a special testing directory, just to be safe) and be sure to add the downloaded <code>bin</code> at the front of your <code>$PATH</code> (contrary to what the scripting currently tells you) so that your <code>git clone</code>'s <code>bin</code> does not shadow the one being tested.</p> <p>Note that changes to the bootstrap script start being used by users as soon as your PR merges.  Since this script can only fetch a released version of the executables, changes to this script can not rely on any behavior of those executables that is not in the currently latest release.  Also, a change that restricts the range of usable releases needs to add checking for use of incompatible releases.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-the-bootstrap-script-against-an-upcoming-release","title":"Testing the bootstrap script against an upcoming release","text":"<p>Prior to making a new release, there needs to be testing that the current bootstrap script works with the executable behavior that will appear in the new release.  To support this we will add an option to the bootstrap script that enables it to use a local release archive instead of fetching an archive of an actual release from github.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>KubeStellar is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p> <p>Please read the following guide if you're interested in contributing to KubeStellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p>"},{"location":"Contribution%20guidelines/LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following    boilerplate notice, with the fields enclosed by brackets \"[]\"    replaced with your own identifying information. (Don't include    the brackets!)  The text should be enclosed in the appropriate    comment syntax for the file format. We also recommend that a    file or class name and description of purpose be included on the    same \"printed page\" as the copyright notice for easier    identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"Contribution%20guidelines/coc/","title":"Code of Conduct","text":"<p>This project is following the CNCF Code of Conduct. </p>"},{"location":"Contribution%20guidelines/coc/#kubestellar-community-code-of-conduct","title":"KubeStellar Community Code of Conduct","text":"<p>As contributors, maintainers, and participants in the CNCF community, and in the interest of fostering an open and welcoming community, we pledge to respect all people who participate or contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, attending conferences or events, or engaging in other community or project activities.</p> <p>We are committed to making participation in the CNCF community a harassment-free experience for everyone, regardless of age, body size, caste, disability, ethnicity, level of experience, family status, gender, gender identity and expression, marital status, military or veteran status, nationality, personal appearance, race, religion, sexual orientation, socieconomic status, tribe, or any other dimension of diversity.</p>"},{"location":"Contribution%20guidelines/coc/#scope","title":"Scope","text":"<p>This code of conduct applies: * within project and community spaces, * in other spaces when an individual CNCF community participant's words or actions are directed at or are about a CNCF project, the CNCF community, or another CNCF community participant.</p>"},{"location":"Contribution%20guidelines/coc/#cncf-events","title":"CNCF Events","text":"<p>CNCF events that are produced by the Linux Foundation with professional events staff are governed by the Linux Foundation Events Code of Conduct available on the event page. This is designed to be used in conjunction with the CNCF Code of Conduct.</p>"},{"location":"Contribution%20guidelines/coc/#our-standards","title":"Our Standards","text":"<p>The CNCF Community is open, inclusive and respectful. Every member of our community has the right to have their identity respected.</p> <p>Examples of behavior that contributes to a positive environment include but are not limited to:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> <li>Using welcoming and inclusive language</li> </ul> <p>Examples of unacceptable behavior include but are not limited to:</p> <ul> <li>The use of sexualized language or imagery</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment in any form</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Violence, threatening violence, or encouraging others to engage in violent behavior</li> <li>Stalking or following someone without their consent</li> <li>Unwelcome physical contact</li> <li>Unwelcome sexual or romantic attention or advances</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul> <p>The following behaviors are also prohibited: * Providing knowingly false or misleading information in connection with a Code of Conduct investigation or otherwise intentionally tampering with an investigation. * Retaliating against a person because they reported an incident or provided information about an incident as a witness.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct.  By adopting this Code of Conduct, project maintainers commit themselves to fairly and consistently applying these principles to every aspect of managing a CNCF project.  Project maintainers who do not follow or enforce the Code of Conduct may be temporarily or permanently removed from the project team.</p>"},{"location":"Contribution%20guidelines/coc/#reporting","title":"Reporting","text":"<p>For incidents occurring in the KubeStellar community, contact the KubeStellar Code of Conduct Committee of Conduct Committee. You can expect a response within three business days.</p> <p>For other projects, or for incidents that are project-agnostic or impact multiple CNCF projects, please contact the CNCF Code of Conduct Committee via conduct@cncf.io.  Alternatively, you can contact any of the individual members of the CNCF Code of Conduct Committee to submit your report. For more detailed instructions on how to submit a report, including how to submit a report anonymously, please see our Incident Resolution Procedures. You can expect a response within three business days.</p> <p>For incidents ocurring at CNCF event that is produced by the Linux Foundation, please contact eventconduct@cncf.io.</p>"},{"location":"Contribution%20guidelines/coc/#enforcement","title":"Enforcement","text":"<p>Upon review and investigation of a reported incident, the CoC response team that has jurisdiction will determine what action is appropriate based on this Code of Conduct and its related documentation. </p> <p>For information about which Code of Conduct incidents are handled by project leadership, which incidents are handled by the CNCF Code of Conduct Committee, and which incidents are handled by the Linux Foundation (including its events team), see our Jurisdiction Policy.</p>"},{"location":"Contribution%20guidelines/coc/#amendments","title":"Amendments","text":"<p>Consistent with the CNCF Charter, any substantive changes to this Code of Conduct must be approved by the Technical Oversight Committee.</p>"},{"location":"Contribution%20guidelines/coc/#acknowledgements","title":"Acknowledgements","text":"<p>This Code of Conduct is adapted from the Contributor Covenant (http://contributor-covenant.org), version 2.0 available at http://contributor-covenant.org/version/2/0/code_of_conduct/</p>"},{"location":"Contribution%20guidelines/governance/","title":"Governance","text":""},{"location":"Contribution%20guidelines/governance/#kubestellar-project-governance","title":"KubeStellar Project Governance","text":"<p>The KubeStellar project is dedicated to solving challenges stemming from multi-cluster configuration management for edge, multi-cloud, and hybrid cloud.  This governance explains how the project is run.</p> <ul> <li>Manifesto</li> <li>Values</li> <li>Maintainers</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifying this Charter</li> </ul>"},{"location":"Contribution%20guidelines/governance/#manifesto","title":"Manifesto","text":"<ul> <li>KubeStellar Maintainers strive to be good citizens in the Kubernetes project.</li> <li>KubeStellar Maintainers see KubeStellar always as part of the Kubernetes ecosystem and always     strive to keep that ecosystem united. In particular, this means:</li> <li>KubeStellar strives to not divert from Kubernetes, but strives to extend its       use-cases to non-container control planes while keeping the ecosystems of       libraries and tooling united.</li> <li>KubeStellar \u2013 as a consumer of Kubernetes API Machinery \u2013 will strive to stay 100%       compatible with the semantics of Kubernetes APIs, while removing container       orchestration specific functionality.</li> <li>KubeStellar strives to upstream changes to Kubernetes code as much as possible.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#values","title":"Values","text":"<p>The KubeStellar and its leadership embrace the following values:</p> <ul> <li>Openness: Communication and decision-making happens in the open and is     discoverable for future reference. As much as possible, all discussions and     work take place in public forums and open repositories.</li> <li>Fairness: All stakeholders have the opportunity to provide feedback and     submit contributions, which will be considered on their merits.</li> <li>Community over Product or Company: Sustaining and growing our community     takes priority over shipping code or sponsors' organizational goals. Each     contributor participates in the project as an individual.</li> <li>Inclusivity: We innovate through different perspectives and skill sets,     which can only be accomplished in a welcoming and respectful environment.</li> <li>Participation: Responsibilities within the project are earned through     participation, and there is a clear path up the contributor ladder into     leadership positions.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#maintainers","title":"Maintainers","text":"<p>KubeStellar Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found as top-level approvers in OWNERS.  Maintainers collectively  manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the KubeStellar project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which  is the governing body for the project.</p>"},{"location":"Contribution%20guidelines/governance/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>participate in discussions, contributions, code and documentation reviews   for 3 months or more,</li> <li>perform reviews for 5 non-trivial pull requests,</li> <li>contribute 5 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.    </li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority  vote of existing Maintainers approves the application.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights, and invited to the private maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#bootstrapping-maintainers","title":"Bootstrapping Maintainers","text":"<p>To bootstrap the process, 3 maintainers are defined (in the initial PR adding  this to the repository) that do not necessarily follow the above rules. When a  new maintainer is added following the above rules, the existing maintainers  define one not following the rules to step down, until all of them follow the  rules.</p>"},{"location":"Contribution%20guidelines/governance/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to  continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons.  Inactivity is defined as a period of very low or no activity in the project for  a year or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus  status. Emeritus Maintainers will still be consulted on some project matters,  and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"Contribution%20guidelines/governance/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public  community call meeting. Maintainers will also have closed meetings in order to  discuss security reports or Code of Conduct violations. Such meetings should be  scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any  Maintainer who is accused of a CoC violation.</p>"},{"location":"Contribution%20guidelines/governance/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this  responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it. The Maintainers will review who is assigned to this  at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security  holes and breaches according to the security policy.</p>"},{"location":"Contribution%20guidelines/governance/#voting","title":"Voting","text":"<p>While most business in KubeStellar is conducted by \"lazy consensus\", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer mailing list for security or conduct matters.  Votes may also be taken at the community call  meeting. Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed. Maintainers can be removed by a 2/3 majority vote of all Maintainers, and changes to this Governance require a 2/3 vote of all Maintainers.</p>"},{"location":"Contribution%20guidelines/governance/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a  2/3 vote of the Maintainers.</p>"},{"location":"Contribution%20guidelines/onboarding/","title":"Onboarding","text":"<p>KubeStellar GitHub Organization On-boarding and Off-boarding Policy</p> <p>Effective Date: June 1st, 2023</p> <p>At KubeStellar we love our contributors.  Our contributors can make various valuable contributions to our project. They can actively engage in code development by submitting pull requests, implementing new features, or fixing bugs. Additionally, contributors can assist with testing, CICD, documentation, providing clear and comprehensive guides, tutorials, and examples. Moreover, they can contribute to the project by participating in discussions, offering feedback, and helping to improve overall community engagement and collaboration.</p> <ol> <li> <p>Introduction: The purpose of this policy is to ensure a smooth on-boarding and off-boarding process for members of the KubeStellar GitHub organization. This policy applies to all individuals joining or leaving the organization, including community contributors.</p> </li> <li> <p>On-boarding Process: 2.1. Access Request:</p> </li> <li>New members shall submit an access request, via a blank GitHub issue from the KubeStellar repository, mentioning all members of the OWNERS file.</li> <li>The access request should include the member's GitHub username and a brief description of their role and contributions to the KubeStellar project.</li> </ol> <p>2.2. Review and Approval: - The organization's maintainers or designated personnel will review the access request issue. - The maintainers will evaluate the request based on the member's role, contributions, and adherence to the organization's code of conduct. - Upon approval, the member will receive an invitation to join the KubeStellar GitHub organization.</p> <p>2.3. Getting Help: - The organization's maintainers are here to help contributors be efficient and confident in their collaboration effort. If you need help you can reach out to the maintainers on slack at the KubeStellar-dev channel. - Be sure to join the KubeStellar-dev Google Group to get access to important artifacts like proposals, diagrams, and meeting invitations.</p> <p>2.4. Orientation: - Newly on-boarded members will be provided with contribution guidelines. - The guide will include instructions on how to access relevant repositories, participate in discussions, and contribute to ongoing projects.</p> <ol> <li>Off-boarding Process: 3.1. Departure Notification:</li> <li>Members leaving the organization shall notify the maintainers or their respective team lead in advance of their departure date.</li> <li>The notification should include the member's departure date and any necessary transition information.</li> </ol> <p>3.2. Access Termination: - Upon receiving the departure notification, the maintainers or designated personnel will initiate the off-boarding process. - The member's access to the KubeStellar GitHub organization will be revoked promptly to ensure data security and prevent unauthorized access.</p> <p>3.3. Knowledge Transfer: - Departing members should facilitate the transfer of their ongoing projects, tasks, and knowledge to their respective replacements or relevant team members. - Documentation or guidelines related to ongoing projects should be updated and made available to the team for seamless continuity.</p> <ol> <li>Code of Conduct:</li> <li>All members of the KubeStellar GitHub organization are expected to adhere to the organization's code of conduct, promoting a respectful and inclusive environment.</li> <li> <p>Violations of the code of conduct will be addressed following the organization's established procedures for handling such incidents.</p> </li> <li> <p>Policy Compliance:</p> </li> <li>It is the responsibility of all members to comply with the on-boarding and off-boarding policy.</li> <li> <p>The organization's maintainers or designated personnel will oversee the implementation and enforcement of this policy.</p> </li> <li> <p>Policy Review:</p> </li> <li>This policy will be reviewed periodically to ensure its effectiveness and relevance.</li> <li>Any updates or revisions to the policy will be communicated to the organization's members in a timely manner.</li> </ol> <p>Please note that this policy is subject to change, and any modifications will be communicated to all members of the KubeStellar GitHub organization.</p> <p>By joining the organization, all members agree to abide by the terms and guidelines outlined in this policy.</p> <p>Andy Anderson (clubanderson) KubeStellar Maintainer June 1, 2023</p>"},{"location":"Contribution%20guidelines/operations/all-macros/","title":"All macros","text":"<p>All variables supported by this documentation implementation:</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#macros-plugin-environment","title":"Macros Plugin Environment","text":""},{"location":"Contribution%20guidelines/operations/all-macros/#general-list","title":"General List","text":"<p>All available variables and filters within the macros plugin:</p> Variable Type Content extra dict version [dict], analytics [dict] config MkDocsConfig {'config_file_path': '/home/runner/work/kubestellar/kubestellar/docs/mike-mkdocs6t1jv_uo.yml', 'site_name': 'KubeStellar', 'nav': [{'Home': [{'Overview': 'index.md'}, {'Readme': 'readme.md'}, {'Quickstart using Kind': 'Getting-Started/user-quickstart-kind.md'}, {'Quickstart using OpenShift': 'Getting-Started/user-quickstart-openshift.md'}]}, {'Getting Started': [{'Quickstart using Kind': 'Getting-Started/user-quickstart-kind.md'}, {'Quickstart using OpenShift': 'Getting-Started/user-quickstart-openshift.md'}, {'Quickstart for testing': 'Getting-Started/user-quickstart-test.md'}, {'Detailed QuickStart': 'Getting-Started/quickstart.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'User Guide': 'Getting-Started/user-guide.md'}, {'KubeStellar - The Infomercial': 'Getting-Started/infomercial.md'}]}, {'Contributing': [{'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}, {'Operations': [{'Code Management': 'Contribution guidelines/operations/code-management.md'}, {'Packaging and Versioning': 'Contribution guidelines/operations/packaging-and-versioning.md'}, {'Release Management': 'Contribution guidelines/operations/release-management.md'}, {'Document Management': 'Contribution guidelines/operations/document-management.md'}, {'Testing Documentation PRs': 'Contribution guidelines/operations/testing-doc-prs.md'}]}]}, {'Coding Milestones': [{'PoC2023q1': [{'Details': 'Coding Milestones/PoC2023q1/outline.md'}, {'Invitation to Contribute': 'Coding Milestones/PoC2023q1/coding-milestone-invite-q1.md'}, {'KubeStellar-Syncer': 'Coding Milestones/PoC2023q1/kubestellar-syncer.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'KubeStellar Processes': [{'KubeStellar Where Resolver': 'Coding Milestones/PoC2023q1/where-resolver.md'}, {'KubeStellar Mailbox Controller': 'Coding Milestones/PoC2023q1/mailbox-controller.md'}, {'KubeStellar Placement Translator': 'Coding Milestones/PoC2023q1/placement-translator.md'}]}, {'Use Cases Affecting the Roadmap for PoC2023q1': 'Coding Milestones/PoC2023q1/roadmap-uses.md'}, {'Environments': [{'Overview': 'Coding Milestones/PoC2023q1/environments/_index.md'}, {'Cloud Environment': 'Coding Milestones/PoC2023q1/environments/cloud-env.md'}, {'Developer Environment': 'Coding Milestones/PoC2023q1/environments/dev-env.md'}]}, {'Reference': [{'Commands': 'Coding Milestones/PoC2023q1/commands.md'}]}]}, {'PoC2023q4': [{'Details': 'Coding Milestones/PoC2023q4/outline.md'}]}]}, {'Community': [{'Get Involved': 'Community/_index.md'}, {'Contributor Guidelines': 'https://docs.kubestellar.io/main/Contribution%20guidelines/CONTRIBUTING/'}, {'Contact Us': [{'Mailing List': 'https://kubestellar.io/join_us'}, {'Community Meeting Agenda (join mailing list first)': 'https://kubestellar.io/agenda'}, {'Slack': 'https://kubestellar.io/slack'}, {'Medium Blog': 'https://kubestellar.io/blog'}, {'YouTube Channel': 'https://kubestellar.io/tv'}, {'LinkedIn': 'https://kubestellar.io/linkedin'}, {'Reddit': 'https://www.reddit.com/r/kubestellar/'}, {'Google Drive': 'https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS'}]}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}], 'pages': None, 'site_url': 'https://docs.kubestellar.io/main', 'site_description': None, 'site_author': None, 'theme': Theme(name='material', dirs=['/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/kubestellar/kubestellar/docs/overrides', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png'), 'docs_dir': '/home/runner/work/kubestellar/kubestellar/docs/content', 'site_dir': '/home/runner/work/kubestellar/kubestellar/docs/generated', 'copyright': None, 'google_analytics': None, 'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000), 'use_directory_urls': True, 'repo_url': 'https://github.com/kubestellar/kubestellar', 'repo_name': 'GitHub', 'edit_uri_template': None, 'edit_uri': 'edit/main/docs/content/', 'extra_css': ['stylesheets/kubestellar.css'], 'extra_javascript': ['js/open_in_new_tab.js'], 'extra_templates': [], 'markdown_extensions': ['toc', 'tables', 'fenced_code', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'], 'mdx_configs': {'toc': {'permalink': '#'}, 'pymdownx.highlight': {'anchor_linenums': True, 'pygments_lang_class': True}, 'pymdownx.tabbed': {'alternate_style': True}}, 'strict': False, 'remote_branch': 'gh-pages', 'remote_name': 'origin', 'extra': {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}}, 'plugins': {'mike': , 'awesome-pages': , 'material/search': , 'open-in-new-tab': , 'include-markdown': , 'macros': , 'i18n': }, 'hooks': {}, 'watch': ['/home/runner/work/kubestellar/kubestellar/docs/mkdocs.yml', '/home/runner/work/kubestellar/kubestellar/docs/content', '/home/runner/work/kubestellar/kubestellar/docs/overrides'], 'repo_short_name': 'kubestellar/kubestellar', 'repo_default_file_path': 'kubestellar', 'helm_repo_short_name': 'kubestellar/helm', 'helm_repo_default_file_path': 'helm', 'brew_repo_short_name': 'kubestellar/homebrew-kubestellar', 'brew_repo_default_file_path': 'homebrew-kubestellar', 'docs_url': 'https://docs.kubestellar.io', 'repo_raw_url': 'https://raw.githubusercontent.com/kubestellar/kubestellar', 'ks_branch': 'main', 'ks_tag': 'latest', 'ks_stable_tag': 'v0.9.0', 'ks_current_branch': 'release-0.15', 'ks_current_tag': 'v0.15.0', 'ks_current_helm_version': '8', 'ks_next_branch': 'release-0.16', 'ks_next_tag': 'v0.16.0', 'ks_next_helm_version': '9', 'ks_kind_port_num': '1119'} environment dict system = 'Linux', system_version = '6.2.0-1019-azure', python_version = '3.10.13', mkdocs_version = '1.4.2', macros_plugin_version = '0.7.0', jinja2_version = '3.1.2' plugin LegacyConfig {'module_name': 'main', 'modules': [], 'include_dir': 'overrides', 'include_yaml': [], 'j2_block_start_string': '', 'j2_block_end_string': '', 'j2_variable_start_string': '', 'j2_variable_end_string': '', 'on_undefined': 'keep', 'on_error_fail': False, 'verbose': False} git dict status = True, date [datetime], short_commit = 'f0d43c38f', commit = 'f0d43c38ffd13447927a9117ba518806c6b1461c', tag = '', author = 'Paolo Dettori', author_email = 'dettori@us.ibm.com', committer = 'GitHub', committer_email = 'noreply@github.com', date_ISO = 'Mon Feb 5 15:30:28 2024 -0500', message = 'Update examples.md', raw = 'commit f0d43c38ffd13447927a9117ba518806c6b1461c\\nAuthor: Paolo Dettori \\nDate:   Mon Feb 5 15:30:28 2024 -0500\\n\\n    Update examples.md', root_dir = '/home/runner/work/kubestellar/kubestellar' version dict default = 'stable', provider = 'mike' analytics dict provider = 'google', property = 'G-SR5TD1CXY7', feedback [dict] macros SuperDict context [function], macros_info [function], now [function], fix_url [function], section_items [function] filters dict pretty [function] filters_builtin dict abs [builtin_function_or_method], attr [function], batch [function], capitalize [function], center [function], count [builtin_function_or_method], d [function], default [function], dictsort [function], e [builtin_function_or_method], escape [builtin_function_or_method], filesizeformat [function], first [function], float [function], forceescape [function], format [function], groupby [function], indent [function], int [function], join [function], last [function], length [builtin_function_or_method], list [function], lower [function], items [function], map [function], min [function], max [function], pprint [function], random [function], reject [function], rejectattr [function], replace [function], reverse [function], round [function], safe [function], select [function], selectattr [function], slice [function], sort [function], string [builtin_function_or_method], striptags [function], sum [function], title [function], trim [function], truncate [function], unique [function], upper [function], urlencode [function], urlize [function], wordcount [function], wordwrap [function], xmlattr [function], tojson [function] navigation Navigation Section(title='Home')    Page(title='Overview', url='/main/')    Page(title='Readme', url='/main/readme/')    Page(title='Quickstart using Kind', url='/main/Getting-Started/user-quickstart-kind/')    Page(title='Quickstart using OpenShift', url='/main/Getting-Started/user-quickstart-openshift/')Section(title='Getting Started')    Page(title='Quickstart using Kind', url='/main/Getting-Started/user-quickstart-kind/')    Page(title='Quickstart using OpenShift', url='/main/Getting-Started/user-quickstart-openshift/')    Page(title='Quickstart for testing', url='/main/Getting-Started/user-quickstart-test/')    Page(title='Detailed QuickStart', url='/main/Getting-Started/quickstart/')    Page(title='Extended Example', url='/main/Coding%20Milestones/PoC2023q1/example1/')    Page(title='User Guide', url='/main/Getting-Started/user-guide/')    Page(title='KubeStellar - The Infomercial', url='/main/Getting-Started/infomercial/')Section(title='Contributing')    Page(title='Guidelines', url='/main/Contribution%20guidelines/CONTRIBUTING/')    Page(title='Code of Conduct', url='/main/Contribution%20guidelines/coc/')    Page(title='License', url='/main/Contribution%20guidelines/LICENSE/')    Page(title='Governance', url='/main/Contribution%20guidelines/governance/')    Page(title='Onboarding', url='/main/Contribution%20guidelines/onboarding/')    Section(title='Security')        Page(title='Policy', url='/main/Contribution%20guidelines/security/security/')        Page(title='Contacts', url='/main/Contribution%20guidelines/security/security_contacts/')    Section(title='Operations')        Page(title='Code Management', url='/main/Contribution%20guidelines/operations/code-management/')        Page(title='Packaging and Versioning', url='/main/Contribution%20guidelines/operations/packaging-and-versioning/')        Page(title='Release Management', url='/main/Contribution%20guidelines/operations/release-management/')        Page(title='Document Management', url='/main/Contribution%20guidelines/operations/document-management/')        Page(title='Testing Documentation PRs', url='/main/Contribution%20guidelines/operations/testing-doc-prs/')Section(title='Coding Milestones')    Section(title='PoC2023q1')        Page(title='Details', url='/main/Coding%20Milestones/PoC2023q1/outline/')        Page(title='Invitation to Contribute', url='/main/Coding%20Milestones/PoC2023q1/coding-milestone-invite-q1/')        Page(title='KubeStellar-Syncer', url='/main/Coding%20Milestones/PoC2023q1/kubestellar-syncer/')        Page(title='Extended Example', url='/main/Coding%20Milestones/PoC2023q1/example1/')        Section(title='KubeStellar Processes')            Page(title='KubeStellar Where Resolver', url='/main/Coding%20Milestones/PoC2023q1/where-resolver/')            Page(title='KubeStellar Mailbox Controller', url='/main/Coding%20Milestones/PoC2023q1/mailbox-controller/')            Page(title='KubeStellar Placement Translator', url='/main/Coding%20Milestones/PoC2023q1/placement-translator/')        Page(title='Use Cases Affecting the Roadmap for PoC2023q1', url='/main/Coding%20Milestones/PoC2023q1/roadmap-uses/')        Section(title='Environments')            Page(title='Overview', url='/main/Coding%20Milestones/PoC2023q1/environments/_index/')            Page(title='Cloud Environment', url='/main/Coding%20Milestones/PoC2023q1/environments/cloud-env/')            Page(title='Developer Environment', url='/main/Coding%20Milestones/PoC2023q1/environments/dev-env/')        Section(title='Reference')            Page(title='Commands', url='/main/Coding%20Milestones/PoC2023q1/commands/')    Section(title='PoC2023q4')        Page(title='Details', url='/main/Coding%20Milestones/PoC2023q4/outline/')Section(title='Community')    Page(title='Get Involved', url='/main/Community/_index/')    Link(title='Contributor Guidelines', url='https://docs.kubestellar.io/main/Contribution%20guidelines/CONTRIBUTING/')    Section(title='Contact Us')        Link(title='Mailing List', url='https://kubestellar.io/join_us')        Link(title='Community Meeting Agenda (join mailing list first)', url='https://kubestellar.io/agenda')        Link(title='Slack', url='https://kubestellar.io/slack')        Link(title='Medium Blog', url='https://kubestellar.io/blog')        Link(title='YouTube Channel', url='https://kubestellar.io/tv')        Link(title='LinkedIn', url='https://kubestellar.io/linkedin')        Link(title='Reddit', url='https://www.reddit.com/r/kubestellar/')        Link(title='Google Drive', url='https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS')    Section(title='Partners')        Page(title='ArgoCD', url='/main/Community/partners/argocd/')        Page(title='Turbonomic', url='/main/Community/partners/turbonomic/')        Page(title='MVI', url='/main/Community/partners/mvi/')        Page(title='FluxCD', url='/main/Community/partners/fluxcd/')        Page(title='OpenZiti', url='/main/Community/partners/openziti/')        Page(title='Kyverno', url='/main/Community/partners/kyverno/')Link(title='Blog', url='https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank') files I18nFiles page Page Page(title='All macros', url='/main/Contribution%20guidelines/operations/all-macros/')"},{"location":"Contribution%20guidelines/operations/all-macros/#config-information","title":"Config Information","text":"<p>Standard MkDocs configuration information. Do not try to modify.</p> <p>e.g. <code>{{ config.docs_dir }}</code></p> <p>See also the MkDocs documentation on the config object.</p> Variable Type Content config_file_path str '/home/runner/work/kubestellar/kubestellar/docs/mike-mkdocs6t1jv_uo.yml' site_name str 'KubeStellar' nav list [{'Home': [{'Overview': 'index.md'}, {'Readme': 'readme.md'}, {'Quickstart using Kind': 'Getting-Started/user-quickstart-kind.md'}, {'Quickstart using OpenShift': 'Getting-Started/user-quickstart-openshift.md'}]}, {'Getting Started': [{'Quickstart using Kind': 'Getting-Started/user-quickstart-kind.md'}, {'Quickstart using OpenShift': 'Getting-Started/user-quickstart-openshift.md'}, {'Quickstart for testing': 'Getting-Started/user-quickstart-test.md'}, {'Detailed QuickStart': 'Getting-Started/quickstart.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'User Guide': 'Getting-Started/user-guide.md'}, {'KubeStellar - The Infomercial': 'Getting-Started/infomercial.md'}]}, {'Contributing': [{'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}, {'Operations': [{'Code Management': 'Contribution guidelines/operations/code-management.md'}, {'Packaging and Versioning': 'Contribution guidelines/operations/packaging-and-versioning.md'}, {'Release Management': 'Contribution guidelines/operations/release-management.md'}, {'Document Management': 'Contribution guidelines/operations/document-management.md'}, {'Testing Documentation PRs': 'Contribution guidelines/operations/testing-doc-prs.md'}]}]}, {'Coding Milestones': [{'PoC2023q1': [{'Details': 'Coding Milestones/PoC2023q1/outline.md'}, {'Invitation to Contribute': 'Coding Milestones/PoC2023q1/coding-milestone-invite-q1.md'}, {'KubeStellar-Syncer': 'Coding Milestones/PoC2023q1/kubestellar-syncer.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'KubeStellar Processes': [{'KubeStellar Where Resolver': 'Coding Milestones/PoC2023q1/where-resolver.md'}, {'KubeStellar Mailbox Controller': 'Coding Milestones/PoC2023q1/mailbox-controller.md'}, {'KubeStellar Placement Translator': 'Coding Milestones/PoC2023q1/placement-translator.md'}]}, {'Use Cases Affecting the Roadmap for PoC2023q1': 'Coding Milestones/PoC2023q1/roadmap-uses.md'}, {'Environments': [{'Overview': 'Coding Milestones/PoC2023q1/environments/_index.md'}, {'Cloud Environment': 'Coding Milestones/PoC2023q1/environments/cloud-env.md'}, {'Developer Environment': 'Coding Milestones/PoC2023q1/environments/dev-env.md'}]}, {'Reference': [{'Commands': 'Coding Milestones/PoC2023q1/commands.md'}]}]}, {'PoC2023q4': [{'Details': 'Coding Milestones/PoC2023q4/outline.md'}]}]}, {'Community': [{'Get Involved': 'Community/_index.md'}, {'Contributor Guidelines': 'https://docs.kubestellar.io/main/Contribution%20guidelines/CONTRIBUTING/'}, {'Contact Us': [{'Mailing List': 'https://kubestellar.io/join_us'}, {'Community Meeting Agenda (join mailing list first)': 'https://kubestellar.io/agenda'}, {'Slack': 'https://kubestellar.io/slack'}, {'Medium Blog': 'https://kubestellar.io/blog'}, {'YouTube Channel': 'https://kubestellar.io/tv'}, {'LinkedIn': 'https://kubestellar.io/linkedin'}, {'Reddit': 'https://www.reddit.com/r/kubestellar/'}, {'Google Drive': 'https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS'}]}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}] pages NoneType None site_url str 'https://docs.kubestellar.io/main' site_description NoneType None site_author NoneType None theme Theme Theme(name='material', dirs=['/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/kubestellar/kubestellar/docs/overrides', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png') docs_dir str '/home/runner/work/kubestellar/kubestellar/docs/content' site_dir str '/home/runner/work/kubestellar/kubestellar/docs/generated' copyright NoneType None google_analytics NoneType None dev_addr _IpAddressValue _IpAddressValue(host='127.0.0.1', port=8000) use_directory_urls bool True repo_url str 'https://github.com/kubestellar/kubestellar' repo_name str 'GitHub' edit_uri_template NoneType None edit_uri str 'edit/main/docs/content/' extra_css list ['stylesheets/kubestellar.css'] extra_javascript list ['js/open_in_new_tab.js'] extra_templates list [] markdown_extensions list ['toc', 'tables', 'fenced_code', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'] mdx_configs dict toc [dict], pymdownx.highlight [dict], pymdownx.tabbed [dict] strict bool False remote_branch str 'gh-pages' remote_name str 'origin' extra LegacyConfig {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}} plugins PluginCollection mike [MikePlugin], awesome-pages [AwesomePagesPlugin], material/search [SearchPlugin], open-in-new-tab [OpenInNewTabPlugin], include-markdown [IncludeMarkdownPlugin], macros [MacrosPlugin], i18n [I18n] hooks dict watch list ['/home/runner/work/kubestellar/kubestellar/docs/mkdocs.yml', '/home/runner/work/kubestellar/kubestellar/docs/content', '/home/runner/work/kubestellar/kubestellar/docs/overrides'] repo_short_name str 'kubestellar/kubestellar' repo_default_file_path str 'kubestellar' helm_repo_short_name str 'kubestellar/helm' helm_repo_default_file_path str 'helm' brew_repo_short_name str 'kubestellar/homebrew-kubestellar' brew_repo_default_file_path str 'homebrew-kubestellar' docs_url str 'https://docs.kubestellar.io' repo_raw_url str 'https://raw.githubusercontent.com/kubestellar/kubestellar' ks_branch str 'main' ks_tag str 'latest' ks_stable_tag str 'v0.9.0' ks_current_branch str 'release-0.15' ks_current_tag str 'v0.15.0' ks_current_helm_version str '8' ks_next_branch str 'release-0.16' ks_next_tag str 'v0.16.0' ks_next_helm_version str '9' ks_kind_port_num str '1119'"},{"location":"Contribution%20guidelines/operations/all-macros/#macros","title":"Macros","text":"<p>These macros have been defined programmatically for this environment (module or pluglets). </p> Variable Type Content context function (obj, e) <p>Default mkdocs_macro List the defined variables</p> macros_info function () <p>Test/debug function:         list useful documentation on the mkdocs_macro environment.</p> now function () <p>Get the current time (returns a datetime object).          Used alone, it provides a timestamp.         To get the year use <code>now().year</code>, for the month number          <code>now().month</code>, etc.</p> fix_url function (url, r) <p>If url is relative, fix it so that it points to the docs diretory.     This is necessary because relative links in markdown must be adapted     in html ('img/foo.png' =&gt; '../img/img.png').</p> section_items function (page, nav, config, children, siblings, child) <p>Returns a list of all pages that are siblings to page.</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#git-information","title":"Git Information","text":"<p>Information available on the last commit and the git repository containing the documentation project:</p> <p>e.g. <code>{{ git.message }}</code></p> Variable Type Content status bool True date datetime datetime.datetime(2024, 2, 5, 15, 30, 28, tzinfo=tzoffset(None, -18000)) short_commit str 'f0d43c38f' commit str 'f0d43c38ffd13447927a9117ba518806c6b1461c' tag str '' author str 'Paolo Dettori' author_email str 'dettori@us.ibm.com' committer str 'GitHub' committer_email str 'noreply@github.com' date_ISO str 'Mon Feb 5 15:30:28 2024 -0500' message str 'Update examples.md' raw str 'commit f0d43c38ffd13447927a9117ba518806c6b1461c\\nAuthor: Paolo Dettori \\nDate:   Mon Feb 5 15:30:28 2024 -0500\\n\\n    Update examples.md' root_dir str '/home/runner/work/kubestellar/kubestellar'"},{"location":"Contribution%20guidelines/operations/all-macros/#page-attributes","title":"Page Attributes","text":"<p>Provided by MkDocs. These attributes change for every page (the attributes shown are for this page).</p> <p>e.g. <code>{{ page.title }}</code></p> <p>See also the MkDocs documentation on the page object.</p> Variable Type Content file I18nFile I18nFile(src_path='Contribution guidelines/operations/all-macros.md', abs_src_path='/home/runner/work/kubestellar/kubestellar/docs/content/Contribution guidelines/operations/all-macros.md', dest_path='Contribution guidelines/operations/all-macros/index.html', abs_dest_path='/home/runner/work/kubestellar/kubestellar/docs/generated/Contribution guidelines/operations/all-macros/index.html', name='all-macros', locale_suffix='None', dest_language='', dest_name='all-macros.md', url='Contribution%20guidelines/operations/all-macros/') title str 'All macros' parent NoneType None children NoneType None previous_page NoneType None next_page NoneType None _Page__active bool False update_date str '2024-02-05' canonical_url str 'https://docs.kubestellar.io/main/Contribution%20guidelines/operations/all-macros/' abs_url str '/main/Contribution%20guidelines/operations/all-macros/' edit_url str 'https://github.com/kubestellar/kubestellar/edit/main/docs/content/Contribution guidelines/operations/all-macros.md' markdown str 'All variables supported by this documentation implementation:\\n\\n{{ macros_info() }}' content NoneType None toc list [] meta dict <p>To have all titles of all pages, use:</p> <pre><code>{% for page in navigation.pages %}\n- {{ page.title }}\n{% endfor %}\n</code></pre>"},{"location":"Contribution%20guidelines/operations/all-macros/#plugin-filters","title":"Plugin Filters","text":"<p>These filters are provided as a standard by the macros plugin.</p> Variable Type Content pretty function (var_list, rows, header, e) <p>Default mkdocs_macro Prettify a dictionary or object          (used for environment documentation, or debugging).</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#builtin-jinja2-filters","title":"Builtin Jinja2 Filters","text":"<p>These filters are provided by Jinja2 as a standard.</p> <p>See also the Jinja2 documentation on builtin filters).</p> Variable Type Content abs builtin_function_or_method <p>Return the absolute value of the argument.</p> attr function (environment, obj, name, value) <p>Get an attribute of an object.  <code>foo|attr(\"bar\")</code> works like     <code>foo.bar</code> just that always an attribute is returned and items are not     looked up.</p> batch function (value, linecount, fill_with, tmp, item) <p>A filter that batches items. It works pretty much like <code>slice</code>     just the other way round. It returns a list of lists with the     given number of items. If you provide a second parameter this     is used to fill up missing items. See this example.</p> capitalize function (s) <p>Capitalize a value. The first character will be uppercase, all others     lowercase.</p> center function (value, width) <p>Centers the value in a field of a given width.</p> count builtin_function_or_method <p>Return the number of items in a container.</p> d function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> default function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> dictsort function (value, case_sensitive, by, reverse, sort_func) <p>Sort a dict and yield (key, value) pairs. Python dicts may not     be in the order you want to display them in, so sort them first.</p> e builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> escape builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> filesizeformat function (value, binary, bytes, base, prefixes, i, prefix, unit) <p>Format the value like a 'human-readable' file size (i.e. 13 kB,     4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,     Giga, etc.), if the second parameter is set to <code>True</code> the binary     prefixes are used (Mebi, Gibi).</p> first function (args, kwargs, b) <p>Return the first item of a sequence.</p> float function (value, default) <p>Convert the value into a floating point number. If the     conversion doesn't work it will return <code>0.0</code>. You can     override this default using the first parameter.</p> forceescape function (value) <p>Enforce HTML escaping.  This will probably double escape variables.</p> format function (value, args, kwargs) <p>Apply the given values to a <code>printf-style</code>_ format string, like     <code>string % values</code>.</p> groupby function (args, kwargs, b) <p>Group a sequence of objects by an attribute using Python's     :func:<code>itertools.groupby</code>. The attribute can use dot notation for     nested access, like <code>\"address.city\"</code>. Unlike Python's <code>groupby</code>,     the values are sorted first so only one group is returned for each     unique value.</p> indent function (s, width, first, blank, newline, rv, lines) <p>Return a copy of the string with each line indented by 4 spaces. The     first line and blank lines are not indented by default.</p> int function (value, default, base) <p>Convert the value into an integer. If the     conversion doesn't work it will return <code>0</code>. You can     override this default using the first parameter. You     can also override the default base (10) in the second     parameter, which handles input with prefixes such as     0b, 0o and 0x for bases 2, 8 and 16 respectively.     The base is ignored for decimal numbers and non-string values.</p> join function (args, kwargs, b) <p>Return a string which is the concatenation of the strings in the     sequence. The separator between elements is an empty string per     default, you can define it with the optional parameter.</p> last function (environment, seq) <p>Return the last item of a sequence.</p> length builtin_function_or_method <p>Return the number of items in a container.</p> list function (args, kwargs, b) <p>Convert the value into a list.  If it was a string the returned list     will be a list of characters.</p> lower function (s) <p>Convert a value to lowercase.</p> items function (value) <p>Return an iterator over the <code>(key, value)</code> items of a mapping.</p> map function (args, kwargs, b) <p>Applies a filter on a sequence of objects or looks up an attribute.     This is useful when dealing with lists of objects but you are really     only interested in a certain value of it.</p> min function (environment, value, case_sensitive, attribute) <p>Return the smallest item from the sequence.</p> max function (environment, value, case_sensitive, attribute) <p>Return the largest item from the sequence.</p> pprint function (value) <p>Pretty print a variable. Useful for debugging.</p> random function (context, seq) <p>Return a random item from the sequence.</p> reject function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and rejecting the objects with the test succeeding.</p> rejectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and rejecting the objects with the test     succeeding.</p> replace function (eval_ctx, s, old, new, count) <p>Return a copy of the value with all occurrences of a substring     replaced with a new one. The first argument is the substring     that should be replaced, the second is the replacement string.     If the optional third argument <code>count</code> is given, only the first     <code>count</code> occurrences are replaced.</p> reverse function (value, rv, e) <p>Reverse the object or return an iterator that iterates over it the other     way round.</p> round function (value, precision, method, func) <p>Round the number to a given precision. The first     parameter specifies the precision (default is <code>0</code>), the     second the rounding method.</p> safe function (value) <p>Mark the value as safe which means that in an environment with automatic     escaping enabled this variable will not be escaped.</p> select function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and only selecting the objects with the test succeeding.</p> selectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and only selecting the objects with the     test succeeding.</p> slice function (args, kwargs, b) <p>Slice an iterator and return a list of lists containing     those items. Useful if you want to create a div containing     three ul tags that represent columns.</p> sort function (environment, value, reverse, case_sensitive, attribute, key_func) <p>Sort an iterable using Python's :func:<code>sorted</code>.</p> string builtin_function_or_method <p>Convert an object to a string if it isn't already. This preserves a :class:<code>Markup</code> string rather than converting it back to a basic string, so it will still be marked as safe and won't be escaped again.</p> striptags function (value) <p>Strip SGML/XML tags and replace adjacent whitespace by one space.</p> sum function (args, kwargs, b) <p>Returns the sum of a sequence of numbers plus the value of parameter     'start' (which defaults to 0).  When the sequence is empty it returns     start.</p> title function (s) <p>Return a titlecased version of the value. I.e. words will start with     uppercase letters, all remaining characters are lowercase.</p> trim function (value, chars) <p>Strip leading and trailing characters, by default whitespace.</p> truncate function (env, s, length, killwords, end, leeway, result) <p>Return a truncated copy of the string. The length is specified     with the first parameter which defaults to <code>255</code>. If the second     parameter is <code>true</code> the filter will cut the text at length. Otherwise     it will discard the last word. If the text was in fact     truncated it will append an ellipsis sign (<code>\"...\"</code>). If you want a     different ellipsis sign than <code>\"...\"</code> you can specify it using the     third parameter. Strings that only exceed the length by the tolerance     margin given in the fourth parameter will not be truncated.</p> unique function (environment, value, case_sensitive, attribute, getter, seen, item, key) <p>Returns a list of unique items from the given iterable.</p> upper function (s) <p>Convert a value to uppercase.</p> urlencode function (value, items) <p>Quote data for use in a URL path or query using UTF-8.</p> urlize function (eval_ctx, value, trim_url_limit, nofollow, target, rel, extra_schemes, policies, rel_parts, scheme, rv) <p>Convert URLs in text into clickable links.</p> wordcount function (s) <p>Count the words in that string.</p> wordwrap function (environment, s, width, break_long_words, wrapstring, break_on_hyphens) <p>Wrap a string to the given width. Existing newlines are treated     as paragraphs to be wrapped separately.</p> xmlattr function (eval_ctx, d, autospace, rv) <p>Create an SGML/XML attribute string based on the items in a dict.     All values that are neither <code>none</code> nor <code>undefined</code> are automatically     escaped.</p> tojson function (eval_ctx, value, indent, policies, dumps, kwargs) <p>Serialize an object to a string of JSON, and mark it safe to     render in HTML. This filter is only for use in HTML documents.</p>"},{"location":"Contribution%20guidelines/operations/code-management/","title":"Code Management","text":""},{"location":"Contribution%20guidelines/operations/code-management/#code-management","title":"Code Management","text":"<p>Fork kubestellar into your own repo, create a local branch, set upstream to kubestellar, add and commit changes to local branch, and squash your commits</p>"},{"location":"Contribution%20guidelines/operations/code-management/#initial-setup","title":"Initial setup","text":""},{"location":"Contribution%20guidelines/operations/code-management/#fork-the-github-kubestellar-repo-into-your-own-github-repo","title":"Fork the Github kubestellar repo into your own Github repo:","text":"<p>You can do this either 1: from the kubestellar Github website using the \"Fork\" button or 2: by using the git fork command from your local git command line interface, such as git bash.</p> <p>copy the forked repo from Github to your local system by using the \"git clone\" command or by downloading the repository's zip file.</p> <p>In your new local forked repo, set upstream to kubestellar main</p> <p>check what your repo's remote settings are <pre><code>git remote -v\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#set-upstream-to-use-kubestellar","title":"Set upstream to use kubestellar:","text":"<pre><code>git remote add upstream git@github.com:kubestellar/kubestellar.git\n</code></pre> <p>For example: <pre><code>owner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git remote -v\norigin  git@github.com:fileppb/edge-mc.git (fetch)\norigin  git@github.com:fileppb/edge-mc.git (push)\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git remote add upstream git@github.com:kubestellar/kubestellar.git\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git remote -v\norigin  git@github.com:fileppb/edge-mc.git (fetch)\norigin  git@github.com:fileppb/edge-mc.git (push)\nupstream        git@github.com:kubestellar/kubestellar.git (fetch)\nupstream        git@github.com:kubestellar/kubestellar.git (push)\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git fetch upstream\nEnter passphrase for key '/c/Users/owner/.ssh/id_rsa':\nremote: Enumerating objects: 60394, done.\nremote: Counting objects: 100% (5568/5568), done.\nremote: Compressing objects: 100% (255/255), done.\nremote: Total 60394 (delta 4768), reused 5457 (delta 4706), pack-reused 54826\nReceiving objects: 100% (60394/60394), 52.38 MiB | 3.25 MiB/s, done.\nResolving deltas: 100% (34496/34496), completed with 415 local objects.\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git status\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#ongoing-contributions","title":"Ongoing contributions","text":""},{"location":"Contribution%20guidelines/operations/code-management/#prior-to-working-on-an-issue","title":"Prior to working on an issue","text":"<p>Ensure that you personal repository if up to date with the kubestellar repository. You can do this by opening your github repository page, check that the selected branch is \"main\", and press the \"sync fork\" button.</p>"},{"location":"Contribution%20guidelines/operations/code-management/#select-an-issue-to-work-on-and-create-a-local-branch","title":"Select an issue to work on and create a local branch,","text":"<p>Create a local branch for your work, preferably including the issue number in the branch name</p> <p>for example if working on issue #11187, then you might name your local branch \"issue-1187\" <pre><code>git checkout -b issue-1187\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#as-you-work-and-change-files-you-should-try-to-commit-relatively-small-pieces-of-work-using-the-following-commands","title":"As you work and change files, you should try to commit relatively small pieces of work, using the following commands","text":"<pre><code>git add (there are several options you can specify for the git add command)\n\ngit commit -m \"your message\"\n\ngit push -u origin branch-name (-u sets upstream to origin which is your remote github repository)\n</code></pre>"},{"location":"Contribution%20guidelines/operations/code-management/#when-you-have-completed-your-work-and-tested-it-locally-then-you-should-perform-a-squash-of-the-git-commits-to-make-the-upcoming-push-request-more-manageable","title":"When you have completed your work and tested it locally, then you should perform a squash of the git commits to make the upcoming push request more manageable.","text":"<p>To perform a squash, checkout the branch you want to squash, 1. use the \"git log\" command to see the history of commits to the branch 2. Count the number of commits you want to squash 3. use the \"git rebase -i HEAD~n\" where n is the number of commits you would like to squash together. (There are other ways to do this) 4. The text editor you have configured to use with git should automagically open your source and you will see a list of commits preceded by \"pick\". Leaving the first \"pick\" as it is, replace the remaining \"pick\"s with \"squash\"es.  5. Save the text file and exit the editor. 6. The text editor will open again to let you edit comments for your new squashed commit. 7. Make your edits if any and save and exit the file. The commits will then be squashed into one commit.</p>"},{"location":"Contribution%20guidelines/operations/code-management/#when-you-are-done-with-the-squash-push-your-changes-to-your-remote-branch-you-can-either","title":"When you are done with the squash, push your changes to your remote branch. You can either:","text":"<p><pre><code>git push -u origin &lt;branch-name&gt;\n\nor \n\ngit push --force-with-lease\n</code></pre> Note: if using the git push -u origin  command, the -u only needs to specified the first time you push. It will set tracking for subsequent pushes to the branch. On the other hand, keeping the -u in the command does no particular harm."},{"location":"Contribution%20guidelines/operations/code-management/#run-actions-automated-workflow-tests-manually-in-your-personal-github-repository","title":"Run Actions (automated workflow tests) manually in your personal Github repository","text":"<ol> <li>Select the \"Actions\" tab toward the upper left of your github personal web page. This will cause a list of Actions to show.</li> <li>Select the action you wish to execute from the list of Actions. For example you might chose \"docs-ecutable - example1\". Note: docs-ecutable should be described in a separate section. But in a nutshell it's a Continuous Integration automation technique of embedding scripts and data within the body of documentation, and then parsing and executing those scripts which in turn interpret and execute source code from a branch that you designate. It's somewhat similar to Travis. So the Action \"docs-ecutable - example1\" executes scripts and data embedded within the documentation for the Example 1 scenario, described in the Kubestellar documents. Those scripts will run using the source code pointed to by the next step, step 3.</li> <li>Select the source code branch you wish to exercise by following the next 3 steps:</li> <li>select the black and white \"Run Workflow\" on the right side of your github web page. This will open a dialog box.</li> <li>within the dialog box, select the branch you wish to exercise by opening the dropdown labeled \"use workflow from\"</li> <li>within the dialog box, select the green \"Run Workflow\" button  Your selected Action workflow will execute and the results will be available when it completes.  </li> </ol>"},{"location":"Contribution%20guidelines/operations/code-management/#create-a-pull-request-pr-from-your-github-repo-branch-in-order-to-request-review-and-approval-from-the-kubestellar-team","title":"Create a Pull Request (PR) from your Github repo branch in order to request review and approval from the Kubestellar team","text":"<p>Take a look at https://github.com/kubestellar/kubestellar/blob/main/CONTRIBUTING.md</p> <p>You can create a Pull Request from your Github web repository by selecting the \"Compare &amp; pull request\" button.</p> <p>You will be presented with a Github web page titled Comparing Changes, which allows you to enter metadata regarding your pull request</p> <p>Reference the issue you are addressing ( add #issue-number) Add one of the listed emojis to the first character of the title of your new PR indicating the type of issue (bug fix, feature, etc) Complete the summary description field Complete the Related issue field by inserting the issue number preceded by the # character, for example \"#1187\" Decide whether this is a draft PR or if it's ready for review, and select the option you want by expanding on the Create Pull Reuest button. Assign a label to the PR from the available list of labels (a drop down list on the right side of the web page)</p> <p>Kubestellar CI pipeline:</p> <p>Prow (https://docs.prow.k8s.io/docs/overview/)</p>"},{"location":"Contribution%20guidelines/operations/document-management/","title":"Document Management","text":""},{"location":"Contribution%20guidelines/operations/document-management/#overview","title":"Overview","text":"<p>Our documentation is powered by mike and MkDocs. MkDocs is powered by Python-Markdown. These are immensely configurable and extensible. You can see our MkDocs configuration in <code>docs/mkdocs.yml</code>. Following are some of the choices we have made.</p> <ul> <li>The MkDocs theme is Material for MkDocs.</li> <li>MkDocs plugin awesome-pages for greater control over how navigation links are shown.</li> <li>MkDocs plugin macros.</li> <li>Our own slightly improved vintage of the <code>include-markdown</code> MkDocs plugin, allowing the source to be factored into re-used files.</li> <li>Python-Markdown extension SuperFences, supporting fenced code blocks that play nice with other markdown features.</li> <li>Python-Markdown extension Highlight, for syntax highlighting of fenced code.</li> <li>Pygments for even fancier code highlighting.</li> <li>MkDocs plugin mkdocs-static-i18n to support multiple languages. We currently only have documentation in English. If you're interested in contributing translations, please let us know!</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#serving-up-documents-locally","title":"Serving up documents locally","text":"<p>You can view and modify our documentation in your local development environment.  Simply checkout one of our branches.</p> <pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar/docs\ngit checkout main\n</code></pre> <p>You can view and modify our documentation in the branch you have checked out by using <code>mkdocs serve</code> from mkdocs.  We have a Python requirements file in <code>requirements.txt</code>, and a Makefile target that builds a Python virtual environment and installs the requirements there.  You can either install those requirements into your global Python environment or use the Makefile target.  To install those requirements into your global Python environment, do the following usual thing.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Alternatively, use the following commands to use the Makefile target to construct an adequate virtual environment and enter it.</p> <pre><code>( cd ..; make venv )\n. venv/bin/activate\n</code></pre> <p>Then, using your chosen environment with the requirements installed, build and serve the documents with the following command.</p> <p><pre><code>mkdocs serve\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p> <p>Another way to view (not modify - this method reflects what has been deployed to the <code>gh-pages</code> branch of our repo) all branches/versions of our documentation locally using 'mike' mike for mkdocs:</p> <p><pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\ncd docs\nmike set-default main\ncd ..\nmake serve-docs\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p>"},{"location":"Contribution%20guidelines/operations/document-management/#jinja-templating","title":"Jinja templating","text":"<p>Our documentation stack includes Jinja. The Jinja constructs --- {\\% ... \\%} for statements, {{ ... }} for expressions, and {# ... #} for comments --- can appear in the markdown sources.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#file-structure","title":"File structure","text":"<p>All documentation-related items live in <code>docs</code> (with the small exception of various <code>make</code> targets and some helper  scripts in <code>hack</code>).</p> <p>The structure of <code>docs</code> is as follows:</p> Path Description config/$language/mkdocs.yml Language-specific <code>mkdocs</code> configuration. content/$language Language-specific website content. generated/branch All generated content for all languages for the current version. generated/branch/$language Generated content for a single language. Never added to git. generated/branch/index.html Minimal index for the current version that redirects to the default language (en) overrides Global (not language-specific) content. Dockerfile Builds the kubestellar-docs image containing mkdocs + associated tooling. mkdocs.yml Minimal <code>mkdocs</code> configuration for <code>mike</code> for multi-version support. requirements.txt List of Python modules used to build the site."},{"location":"Contribution%20guidelines/operations/document-management/#global-variables","title":"Global Variables","text":"<p>There are many global variables defined in the docs/mkdocs.yml.  The following are some very common variables you are encouraged to use in our documentation.  Use of these variables/macros allows our documentation to have github branch context and take advantage of our evolution without breaking</p> <pre><code>- site_name: KubeStellar\n- repo_url: https://github.com/kubestellar/kubestellar\n- site_url: https://docs.kubestellar.io/main\n- repo_default_file_path: kubestellar\n- repo_short_name: kubestellar/kubestellar\n- docs_url: https://docs.kubestellar.io\n- repo_raw_url: https://raw.githubusercontent.com/kubestellar/kubestellar\n- edit_uri: edit/main/docs/content/\n- ks_branch: main\n- ks_tag: latest\n</code></pre> <p>to use a variables/macro in your documentation reference like this:</p> <p>{{ config.&lt;var_name&gt; }}</p> <p>and in context that can look something like this:</p> <p>bash &lt;(curl -s {{ config.repo_raw_url }}/{{ config.ks_branch }}/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version {{ config.ks_tag }}</p> <p>note:  \u00a0\u00a0\u00a0\u00a0- A more extensive and detailed list is located at mkdocs information  \u00a0\u00a0\u00a0\u00a0- We also check for broken links as part of our PR pipeline.  For more information check out our Broken Links Crawler</p>"},{"location":"Contribution%20guidelines/operations/document-management/#page-variables","title":"Page variables","text":"<p>A markdown source file can contribute additional variables by defining them in <code>name: value</code> lines at the start of the file, set off by lines of triple dashes. For example, suppose a markdown file begins with the following.</p> <pre><code>---\nshort_name: example1\nmanifest_name: 'docs/content/Coding Milestones/PoC2023q1/example1.md'\n---\n</code></pre> <p>These variables can be referenced as {{ page.meta.short_name }} and {{ page.meta.manifest_name }}.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#including-external-markdown","title":"Including external markdown","text":"<p>We make extensive use of 'include-markdown' to help us keep our documentation modular and up-to-date.  To use 'include-markdown' you must add a block in your document that refers to a block in your external document content:</p> <p>In your original markdown document, add a block that refers to the external markdown you want to include: </p> <p>In the document you want to include, add the start and end tags you configured in the include-markdown block in your original document: </p> <p>for more information on the 'include-markdown' plugin for mkdocs look here</p>"},{"location":"Contribution%20guidelines/operations/document-management/#supported-aliases-for-our-documentation","title":"Supported aliases for our documentation","text":"<p>We currently support 3 aliases for our documentation:</p> <pre><code>- from the release major.minor branch:\n    - [https://docs.kubestellar.io/stable](../../../https://docs.kubestellar.io/stable)\n- from the main branch:\n    - [https://docs.kubestellar.io/unstable](../../../https://docs.kubestellar.io/unstable)\n    - [https://docs.kubestellar.io/latest](../../../https://docs.kubestellar.io/latest)\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#shortcut-urls","title":"Shortcut URLs","text":"<p>We have a few shortcut urls that come in handy when referring others to our project:</p> <p>note: You need to join our mailing list first to get access to some of the links that follow (https://docs.kubestellar.io/joinus)</p> <ul> <li>https://kubestellar.io/agenda - our community meeting agenda google doc</li> <li>https://kubestellar.io/blog - our medium reading list</li> <li>https://kubestellar.io/code - our current GitHub repo (wherever that is)</li> <li>https://kubestellar.io/community - our stable docs community page</li> <li>https://kubestellar.io/drive - our google drive</li> <li>https://kubestellar.io/joinus - our dev mailing list where you join and get our invites</li> <li>https://kubestellar.io/join_us - also, our dev mailing list</li> <li>https://kubestellar.io/linkedin - our linkedin filter (soon, our page)</li> <li>https://kubestellar.io/tv - our youtube channel</li> <li>https://kubestellar.io/youtube - also, our youtube channel</li> <li>https://kubestellar.io/infomercial - our infomercial that premieres on June 12th at 9am</li> </ul> <p>and.. the very important\u2026 - https://kubestellar.io/quickstart - our 'stable' quickstart</p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblocks","title":"Codeblocks","text":"<p>mkdocs has some very helpful ways to include blocks of code in a style that makes it clear to our readers that console interaction is necessary in the documentation.  There are options to include a plain codeblock (```), shell (shell), console (console - no used in our documentation), language or format-specific (yaml, etc.), and others.  For more detailed information, checkout the mkdocs information on codeblocks.</p> <p>NOTE: the docs-ecutable technology does not apply Jinja, at any stage; Jinja source inside executed code blocks will not be expanded by Jinja but rather seen directly by <code>bash</code>.</p> <p>Here are some examples of how we use codeblocks.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-and-executed","title":"Seen and executed","text":"<p>For a codeblock that can be 'tested' (and seen by the reader) as part of our CI, use the <code>shell</code> block: codeblock: <pre><code>```shell\nmkdocs serve\n```\n</code></pre> as seen by reader: <pre><code>mkdocs serve\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#executed-but-not-seen","title":"Executed but not seen","text":"<p>(Think hard before hiding stuff from your reader.)</p> <p>For a codeblock that should be 'tested', BUT not seen by the reader, use the <code>.bash</code> with the plain codeblock, and the '.hide-me' style (great for hiding a sleep command that user does not need to run, but CI does): codeblock: <pre><code>``` {.bash .hide-me}\nsleep 10\n```\n</code></pre> as seen by reader: <pre><code>\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-but-not-executed","title":"Seen but not executed","text":"<p>(To avoid confusing readers of the HTML, this should be used only for output seen in a shell session.)</p> <p>For a codeblock that should not be 'tested' as part of our CI, use the <code>.bash</code> with the plain codeblock, and without the '.hide-me' style: codeblock: <pre><code>``` {.bash}\nmkdocs server\n```\n</code></pre> as seen by reader: <pre><code>mkdocs server\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-but-not-executed-and-no-copy-button","title":"Seen but not executed and no copy button","text":"<p>For a codeblock that should not be 'tested', be seen by the reader, and not include a 'copy' icon (great for output-only instances), use the <code>.bash</code> codeblock without the '.no-copy' style: codeblock: <pre><code>``` {.bash .no-copy}\nI0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n```\n</code></pre> as seen by reader: <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#other-language-specific-highlighting","title":"Other language-specific highlighting","text":"<p>For other language-specific highlighting (yaml, etc.), use the yaml codeblock codeblock: <pre><code>```yaml\nnav:\n  - Home: index.md\n  - QuickStart: Getting-Started/quickstart.md\n  - Contributing: \n      - Guidelines: Contribution guidelines/CONTRIBUTING.md\n```\n</code></pre> as seen by reader: <pre><code>nav:\n- Home: index.md\n- QuickStart: Getting-Started/quickstart.md\n- Contributing: - Guidelines: Contribution guidelines/CONTRIBUTING.md\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblock-with-a-title","title":"Codeblock with a title","text":"<p>For a codeblock that has a title, and will not be tested, use the 'title' parameter in conjunction with the plain codeblock (greater for showing or prescribing contents of files): codeblock: <pre><code>``` title=\"testing.sh\"\n#!/bin/sh\necho hello KubeStellar\n```\n</code></pre> as seen by reader: testing.sh<pre><code>#!/bin/sh\necho hello KubeStellar\n</code></pre> </p> <p>(other variations are possible, PR an update to the kubestellar.css file and, once approved, use the style on the plain codeblock in your documentation.)</p>"},{"location":"Contribution%20guidelines/operations/document-management/#testingrunning-docs","title":"Testing/Running Docs","text":"<p>How do we ensure that our documented examples work?  Simple, we 'execute' our documentation in our CI.  We built automation called 'docs-ecutable' which can be invoked to test any markdown (.md) file in our repository. You could use it in your project as well - afterall it is opensource.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-way-it-works","title":"The way it works:","text":"<ul> <li>create your .md file as you normally would</li> <li>add codeblocks that can be tested, tested but hidden, or not tested at all:<ul> <li>use 'shell' to indicate code you want to be tested</li> <li>use '.bash' with the plain codeblock, and the '.hide-md' style for code you want to be tested, but hidden from the reader (some like this, but its not cool if you want others to run your instructions without hiccups)</li> <li>use plain codeblock (```) if you want to show sample output that is not to be tested</li> </ul> </li> <li>you can use 'include-markdown' blocks, and they will also be executed (or not), depending on the codeblock style you use in the included markdown files.</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-github-workflow","title":"The GitHub Workflow:","text":"<ul> <li>One example of the GitHub Workflow is located in our kubestellar/kubestellar at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-where-resolver.yml</li> <li>An example workflow using the newer technology is located in our kubestellar/kubestellar repo at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-example1.yml</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-original-secret-sauce","title":"The original secret sauce:","text":"<ul> <li>The original code that made all this possible is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/docs-ecutable.sh<ul> <li>This code parses the .md file you give it to pull out all the 'shell' and '.bash .hide-me' blocks</li> <li>The code is smart enough to traverse the include-markdown blocks and include the 'shell' and '.bash .hide-me' blocks in them</li> <li>The Jinja constructs are not expanded by this code.</li> <li>It then creates a file called 'generate_script.sh' which is then run at the end of the docs-ecutable execution.</li> </ul> </li> </ul> <p>All of this is invoke in a target in our Makefile <pre><code>.PHONY: docs-ecutable\ndocs-ecutable: MANIFEST=$(MANIFEST) docs/scripts/docs-ecutable.sh\n</code></pre></p> <p>You give the path from that follows the 'https://github.com/kubestellar/kubestellar/docs' path, and name of the .md file you want to 'execute'/'test' as the value for the MANIFEST variable:</p> How to 'make' our docs-ecutable target<pre><code>make MANIFEST=\"'docs/content/Getting-Started/quickstart.md'\" docs-ecutable\n</code></pre> <p>note: there are single and double-quotes used here to avoid issues with 'spaces' used in files names or directories.  Use the single and double-quotes as specified in the quickstart example here.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-new-and-improved-secret-sauce","title":"The new and improved secret sauce:","text":"<ul> <li>The newer code for executing bash snippets in documentation is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/execute-html.sh<ul> <li>This code parses the HTML generated by MkDocs to extract all the fenced code blocks tagged for the \"shell\" language.</li> <li>This HTML scraping is relatively easy because it does not have to work on general HTML but only the HTML generated by our stack from our sources. The use of the option setting <code>pygments_lang_class: true</code> for the Python-Markdown extension <code>pymdownx.highlight</code> plays a critical role, getting the source language into the generated HTML.</li> <li>Because it reads the generated HTML, invisible code blocks are not extracted.</li> <li>Because it reads the generated HTML, the Jinja constructs have their usual effects.</li> <li>This script is given the name of the HTML file to read and the current working directory to establish at the start of the extracted bash.</li> <li>It then creates a file called 'generated_script.sh' which is then run.</li> </ul> </li> </ul> <p>All of this is invoked in a target in our Makefile <pre><code>.PHONY: execute-html\nexecute-html: venv\n    . $(VENV)/activate; \\\ncd docs; \\\nmkdocs build; \\\nscripts/execute-html.sh \"$$PWD/..\" \"generated/$(MANIFEST)/index.html\"\n</code></pre></p> <p>The <code>make</code> target requires the variable <code>MANIFEST</code> to be set to the directory that contains the generated <code>index.html</code> file, relative to 'https://github.com/kubestellar/kubestellar/docs/generated'. This is the name of the markdown source file, relative to 'https://github.com/kubestellar/kubestellar/docs/content' and with the <code>.md</code> extension dropped.</p> How to 'make' a docs-ecutable target<pre><code>make MANIFEST=\"Coding Milestones/PoC2023q1/example1\" execute-html\n</code></pre> <p>note: this target has no special needs for quoting --- which is not to deny the quoting that your shell needs.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#important-files-in-our-gh-pages-branch","title":"Important files in our gh-pages branch","text":""},{"location":"Contribution%20guidelines/operations/document-management/#indexhtml-and-homehtml","title":"index.html and home.html","text":"<p>In the 'gh-pages' branch there are two(2) important files that redirect the github docs url to our KubeStellar doc site hosted with GoDaddy.com.</p> <p>https://github.com/kubestellar/kubestellar/blob/gh-pages/home.html https://github.com/kubestellar/kubestellar/blob/gh-pages/index.html</p> <p>both files have content similar to: index.html and home.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;KubeStellar&lt;/title&gt;\n&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" &gt;\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://docs.kubestellar.io/stable\" /&gt;\n&lt;/head&gt;\n</code></pre></p> <p>Do not remove these files!</p>"},{"location":"Contribution%20guidelines/operations/document-management/#cname","title":"CNAME","text":"<p>The CNAME file has to be in the gh-pages root to allow github to recognize the url tls cert served by our hosting provider.  Do not remove this file!</p> <p>the CNAME file must have the following content in it: CNAME<pre><code>docs.kubestellar.io\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/document-management/#versionsjson","title":"versions.json","text":"<p>The versions.json file contains the version and alias information required by 'mike' to properly serve our doc site.  This file is maintained by the 'mike' environment and should not be edited by hand.</p> <pre><code>[{\"version\": \"release-0.2\", \"title\": \"release-0.2\", \"aliases\": [\"stable\"]}, {\"version\": \"main\", \"title\": \"main\", \"aliases\": [\"latest\", \"unstable\"]}]\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#in-case-of-emergency","title":"In case of emergency","text":"<p>If you find yourself in a jam and the pages are not showing up at kubestellar.io or docs.kubestellar.io, check the following 1) Is the index.html, home.html, CNAME, and versions.json file in the gh-pages branch alongside the folders for the compiled documents?  If not, then recreate those files as indicated above (except for versions.json which is programmatically created by 'mike'). 2) Is GitHub settings for 'Pages' for the domain pointing at the https://docs.kubestellar.io url?  If not, paste it in and check off 'enforce https'.  This can happen if the CNAME file goes missing from the gh-pages branch.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#how-to-recreate-the-gh-pages-branch","title":"How to recreate the gh-pages branch","text":"<p>To recreate the gh-pages branch, do the following: - checkout the gh-pages branch to your local system <pre><code>git clone -b gh-pages https://github.com/kubestellar/kubestellar KubeStellar\ncd KubeStellar\n</code></pre> - delete all files in the branch and push it to GitHub <pre><code>rm -rf *\ngit add; git commit -m \"removing all gh-pages files\"; git push -u origin gh-pages\n</code></pre> -- switch to the 'main' branch <pre><code>git checkout main\ngit pull\n</code></pre> - switch to /docs and run 'mike deploy' for the main branch for alias 'unstable' and 'latest' <pre><code>cd docs\nmike deploy --push --rebase --update-aliases main unstable\nmike deploy --push --rebase --update-aliases main latest\n</code></pre> - switch to the 'release' branch and 'mike deploy' for the release branch for alias 'stable' (your release name will vary) <pre><code>git checkout release-0.2\ngit pull\nmike deploy --push --rebase --update-aliases release-0.2 stable\n</code></pre> - switch back to the gh-pages branch and recreate the home.html, index.html, and CNAME files as needed (make sure you back out of the docs path first before switching to gh-pages because that path does not exist in that branch) <pre><code>cd ..\ngit checkout gh-pages\ngit pull\nvi index.html\nvi home.html\nvi CNAME\n</code></pre> - push the new files into gh-pages <pre><code>git add .;git commit -m \"add index, home, and CNAME files\";git push -u origin gh-pages\n</code></pre> - go into the GitHub UI and go to the settings for the project and click on 'Pages' to add https://docs.kubestellar.io as the domain and check the box to enforce https.</p> <ul> <li>if the above did not work, then you might have an issue with the GoDaddy domain (expired, files missing, etc.)</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#publishing-workflow","title":"Publishing Workflow","text":"<p>All documentation building and publishing is done using GitHub Actions in docs-gen-and-push.yaml. The overall sequence is:</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/","title":"Packaging and Versioning","text":"<p>This document surveys the various sorts of piles of things that exist in KubeStellar and how they are versioned.</p> <p></p> <p>The picture above outlines the situation. The thing at the tail of an arrow refers to or in some way consumes from the thing at the head of the arrow. Some details omitted for brevity: the full release archives, the Kubernetes API group, and the space framework component.</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubernetes-api-group","title":"Kubernetes API group","text":"<p>KubeStellar defines a Kubernetes API group, named <code>control.kubestellar.io</code>. API groups are versioned; see the Kubernetes documentation on API group versioning. The Kubernetes API of KubeStellar is defined in <code>api/control</code>.</p> <p>The API objects form the interface between users and the controllers that implement the semantics of those objects.</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#syncer-container-image","title":"Syncer container image","text":"<p>The syncer is KubeStellar's agent in WECs, and is based on a Kubernetes Pod that uses a container image at <code>quay.io/kubestellar/syncer</code>. This container image is built from contents of the KubeStellar GitHub repo focused by ko on the syncer binary. This image is normally built using the <code>make</code> target named \"build-kubestellar-syncer-image. The syncer image is referenced in the output generated by the <code>kubectl kubestellar syncer-gen</code> command, which is invoked by <code>kubectl kubestellar prep-for-syncer</code>, which is invoked by <code>kubectl kubestellar prep-for-cluster</code>. The <code>kubectl kubestellar prep-for-syncer</code> script holds the default value for the tag to use in the reference to the syncer image.</p> <p>Following are some recent tags and what they correspond to.</p> <ul> <li> <p><code>v0.9.0</code>, for the git commit with that tag (i.e., that release).</p> </li> <li> <p><code>v0.8.0</code>, for the git commit with that tag (i.e., that release).</p> </li> <li> <p><code>pr-1103</code> = <code>git-ae64d5722-clean</code>, incorporating PR   1103, the   latest development in the release-0.7 line.</p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#core-container-image","title":"Core container image","text":"<p>The core container image is used in containers that run the KubeStellar core controllers , initialize the KubeStellar core, and run one of the supported space providers. It is built from some contents of the KubeStellar GitHub repo, specifically excluding some that are only needed by users outside this container. This image is normally built using the <code>make</code> target \"kubestellar-image\". This image is referenced from the core Helm chart, which can be used directly from the Helm repo and also is used by the <code>kubectl kubestellar deploy</code> command (which has a fixed image tag in its source, which is edited when it is time to advance that tag).</p> <p>Following are some recent tags and what they correspond to.</p> <ul> <li> <p><code>release-0.9</code>, for the release 0.9 line of development; currently   this points to the same image as <code>v0.9.0</code>   but may move in the future.</p> </li> <li> <p><code>v0.9.0</code> = <code>v0.9.0-alpha.1.git1f0e2cc-clean</code>, for the v0.9.0 release.</p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellar-core-helm-chart","title":"KubeStellar core Helm chart","text":"<p>This Helm chart describes a deployment of the KubeStellar core components (a space provider, KubeStellar controllers, initialization) as workload for a Kubernetes cluster.</p> <p>The source of this Helm chart is in the <code>core-helm-chart</code> subdirectory of the kubestellar GitHub repo.</p> <p>The <code>kubectl kubestellar deploy</code> command uses the adjacent copy of this Helm chart. That is: (1) in the case of working with a local copy of the kubestellar/kubestellar GitHub repo, the copy in the <code>core-helm-chart</code> directory, and (2) in the case of using an unpacked release archive --- which contains <code>core-helm-chart</code> as well as <code>bin</code> --- that unpacked copy of <code>core-helm-chart</code>.</p> <p>Alternatively, users can use the Helm repository at <code>https://helm.kubestellar.io/</code>, which is served via GitHub pages from the <code>main</code> branch of the kubestellar/helm GitHub repo. This repository holds tar archives of released versions of the chart, created manually from the sources in the <code>core-helm-chart</code> subdirectory of the kubestellar GitHub repo.</p> <p>A Helm chart has both a version number for the chart itself and a version number for the \"application\" it installs.</p> <p>Currently there is only one version of the chart that is interesting.</p> Chart Version App Version What it is 1/2 (some confusion here) v0.9.0 KubeStellar release-0.9 branch"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#github-repos","title":"GitHub repos","text":""},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarkubestellar","title":"kubestellar/kubestellar","text":"<p>The kubestellar repo is the main repo for KubeStellar. It holds the code and documentation and the source of the Helm chart. The following branches and tags are most interesting.</p> <ul> <li>branch <code>main</code> is the main line of development.</li> <li>branch <code>release-0.9</code> is for development along the line of the 0.9 release.</li> <li>tag <code>v0.9.0</code> is release 0.9.0.</li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarhelm","title":"kubestellar/helm","text":"<p>The helm GitHub repo hosts distributions of archived versions of the chart, as discussed above.</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarhomebrew-kubestellar","title":"kubestellar/homebrew-kubestellar","text":"<p>The homebrew-kubestellar GitHub repo serves directly as the homebrew repository for KubeStellar. According to homebrew convention, users refer to it in homebrew as <code>kubestellar/kubestellar</code> (e.g., <code>brew tap kubestellar/kubestellar</code> connects directly to this GitHub repository).</p> <p>This HomeBrew repository has two formulae.  One to control the space provider and another formula called <code>kubestellar_cli</code> containing tools needed by a user who deploys the KubeStellar core as workload in a Kubernetes cluster or another user of that deployment. This formula fetches and installs a <code>kubestellaruser</code> release archive from GitHub. Users invoke the <code>brew</code> command directly to install, upgrade, and remove these formulae.</p> <p>Following are the versions available.</p> <ul> <li><code>kubestellar_cli</code> <code>v0.9.0</code>, for that release of KubeStellar.</li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarkubeflex","title":"kubestellar/kubeflex","text":"<p>The kubeflex repo holds kubeflex, an independent component that we anticipate using as a provider for the Space framework in the future.</p>"},{"location":"Contribution%20guidelines/operations/release-management/","title":"Release Management","text":""},{"location":"Contribution%20guidelines/operations/release-management/#publishing-a-new-kubestellar-release","title":"Publishing a new KubeStellar release","text":""},{"location":"Contribution%20guidelines/operations/release-management/#prerequisite-make-sure-you-have-a-gpg-signing-key","title":"Prerequisite - make sure you have a GPG signing key","text":"<ol> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key</li> </ol>"},{"location":"Contribution%20guidelines/operations/release-management/#create-the-tags","title":"Create the tags","text":""},{"location":"Contribution%20guidelines/operations/release-management/#note","title":"Note:","text":"<p>You currently need write access to the https://github.com/kubestellar/kubestellar repository to perform these tasks.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#checkout-the-main-branch","title":"Checkout the main branch","text":"<pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-kubectl-kubestellar-prep_for_syncer-file-with-a-reference-to-the-new-version-of-the-kubestellar-syncer-version-if-you-made-a-new-version-and-see-the-syncer-doc-for-how-to-do-that-being-careful-to-not-exclude-the-git-commit-and-cleanliness-information-from-all-the-tags-on-the-image","title":"Update the 'kubectl-kubestellar-prep_for_syncer' file with a reference to the new version of the kubestellar syncer version IF you made a new version (and see the syncer doc for how to do that, being careful to not exclude the git commit and cleanliness information from all the tags on the image).","text":"<pre><code>vi scripts/outer/kubectl-kubestellar-prep_for_syncer\n</code></pre> <p>change the version in the following line: <pre><code>syncer_image=\"quay.io/kubestellar/syncer:v0.16.0\"\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-core-helm-chart-chartyaml-and-valuesyaml-files-with-a-reference-to-the-new-version-of-the-kubestellar-helm-chart-version","title":"Update the core-helm-chart 'Chart.yaml' and 'values.yaml' files with a reference to the new version of the KubeStellar Helm chart version","text":"<pre><code>vi core-helm-chart/Chart.yaml\n</code></pre> <p>change the versions in the 'Chart.yaml' file in the following lines: <pre><code>...\nversion: 9\n...\nappVersion: v0.16.0\n...\n</code></pre></p> <p>then in 'values.yaml' <pre><code>vi core-helm-chart/values.yaml\n</code></pre></p> <p>change the version in the 'values.yaml' file in the following line: <pre><code># KubeStellar image parameters\nimage:\n  repository: quay.io/kubestellar/kubestellar\n  pullPolicy: IfNotPresent\n  tag: release-0.16\n...\n# Space abstraction layer image parameters\nspaceimage:\n  repository: quay.io/kubestellar/space-framework\n  pullPolicy: IfNotPresent\n  tag: release-0.16\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-version-file","title":"Update the VERSION file","text":"<p>The VERSION file points to the 'latest' and 'stable' release tags associated with the most recent release (latest) and the most stable release (stable).  Update the 'stable' and 'latest tags accordingly</p> <pre><code>vi VERSION\n</code></pre> <p>before: VERSION<pre><code>...\nstable=v0.9.0\nlatest=v0.15.0\n...\n</code></pre></p> <p>after: VERSION<pre><code>...\nstable=v0.9.0\nlatest=v0.16.0\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-mkdocsyml-file-pre-branch","title":"Update the mkdocs.yml file (pre branch)","text":"<p>The mkdocs.yml file points to the branch and tag associated with the branch you have checked out.  Update the ks_branch and ks_tag key/value pairs at the top of the file</p> <pre><code>vi docs/mkdocs.yml\n</code></pre> <p>before: mkdocs.yml<pre><code>...\nks_current_branch: 'release-0.15'\nks_current_tag: 'v0.15.0'\nks_current_helm_version: 8\nks_next_branch: 'release-0.16'\nks_next_tag: 'v0.16.0'\nks_next_helm_version: 9\n...\n</code></pre></p> <p>after: mkdocs.yml<pre><code>...\nks_current_branch: 'release-0.16'\nks_current_tag: 'v0.16.0'\nks_current_helm_version: 9\nks_next_branch:    # put the branch name of the next numerical branch that will come in the future\nks_next_tag:       # put the tag name of the next numerical tag that will come in the future\nks_next_helm_version: # put the number of the next logical helm version\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#push-the-main-branch","title":"Push the main branch","text":"<pre><code>git add .\ngit commit -m \"updates to main to support new release v0.16.0\"\ngit push -u origin main\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-majorminor-branch","title":"Create a release-major.minor branch","text":"<p>To create a release branch, identify the current 'release' branches' name (e.g. release-0.16).  Increment the  or  segment as part of the 'release' branches' name.  For instance, the 'release' branch is 'main', you might name the new release branch 'release-0.16'. <pre><code>git checkout -b release-0.16\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-mkdocsyml-file-post-branch","title":"Update the mkdocs.yml file (post branch)","text":"<p>The mkdocs.yml file points to the branch and tag associated with the branch you have checked out.  Update the ks_branch and ks_tag key/value pairs at the top of the file</p> <pre><code>vi docs/mkdocs.yml\n</code></pre> <p>before: mkdocs.yml<pre><code>...\nedit_uri: edit/main/docs/content\nks_branch: 'main'\nks_tag: 'latest'\n...\n</code></pre></p> <p>after: mkdocs.yml<pre><code>...\nedit_uri: edit/release-0.16/docs/content\nks_branch: 'release-0.16'\nks_tag: 'v0.16.0'\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-branch-name-in-readmemd","title":"Update the branch name in /README.MD","text":"<p>There are quite a few references to the main branch /README.MD.  They connect the GitHub Actions for the specific branch to the README.MD page.  Since we are on the new release branch, its time to update these to point to the release itself.</p> <pre><code>vi README.MD\n</code></pre> <p>before: <pre><code>https://github.com/kubestellar/kubestellar/actions/workflows/docs-gen-and-push.yml/badge.svg?branch=main\n</code></pre></p> <p>after: <pre><code>https://github.com/kubestellar/kubestellar/actions/workflows/docs-gen-and-push.yml/badge.svg?branch=release-0.16\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#push-the-new-release-branch","title":"Push the new release branch","text":"<pre><code>git add .\ngit commit -m \"new release version release-0.16\"\ngit push -u origin release-0.16 # replace &lt;major&gt;.&lt;minor&gt; with your incremented &lt;major&gt;.&lt;minor&gt; pair\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#remove-the-current-stable-alias-using-mike-danger","title":"Remove the current 'stable' alias using 'mike' (DANGER!)","text":"<p>Be careful, this will cause links to the 'stable' docs, which is the default for our community, to become unavailable.  For now, point 'stable' at 'main' <pre><code>cd docs\nmike delete stable # remove the 'stable' alias from the current 'main' branches' doc set\nmike deploy --push --rebase --update-aliases main stable # this generates the 'main' branches' docs set and points 'stable' at it temporarily\ncd ..\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-stable-alias-using-mike","title":"Update the 'stable' alias using 'mike'","text":"<pre><code>cd docs\nmike delete stable # remove the 'stable' alias from the 'main' branches' doc set\ngit pull\nmike deploy --push --rebase --update-aliases release-0.16 stable  # this generates the new 'release-0.16' branches' doc set and points 'stable' at it\ncd ..\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#test-your-doc-site","title":"Test your doc site","text":"<p>Open a Chrome Incognito browser to https://docs.kubestellar.io and look for the version drop down to be updated to the new release you just pushed with 'git' and deployed with 'mike'</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-tagged-release","title":"Create a tagged release","text":"<p>View the existing tags you have for the repo</p> <pre><code>git fetch --tags\ngit tag\n</code></pre> <p>create a tag that follows ...  For this example we will increment tag 'v0.15.0' to 'v0.16.0' <pre><code>TAG=v0.16.0\nREF=release-0.16\ngit tag --sign --message \"$TAG\" \"$TAG\" \"$REF\"\ngit push origin --tags\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#clean-out-previous-release-tar-images-and-the-checksums256txt-file-from-your-local-build-environment","title":"Clean out previous release tar images and the checksums256.txt file from your local build environment","text":"<p>When you create a build, output goes to your local /build/release.  Make sure this path is empty before you start so there is no mixup with your current build.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-kubestellar-full-build","title":"Create a KubeStellar full build","text":"<pre><code>./hack/make-release-full.sh v0.16.0\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-in-gh-ui","title":"Create a release in GH UI","text":"<ul> <li>Navigate to the KubeStellar GitHub Source Repository Releases section at https://github.com/kubestellar/kubestellar/releases</li> <li> <p>Click 'Draft a new release' and select the tag ('v0.16.0')</p> <ul> <li>Select the release branch you created above (release-0.16)</li> <li>Add a release title (v0.16.0)</li> <li>Add some release notes ('generate release notes' if you like)</li> <li>select 'pre-release' as a the first step.  Once validated the release is working properly, come back and mark as 'release'</li> <li>Attach the binaries that were created in the 'make-release-full' process above<ul> <li>You add the KubeStellar-specific '*.tar.gz' and the 'checksums256.txt' files</li> <li>GitHub will automatically add the 'Source Code (zip)' and 'Source Code (tar.gz)'</li> </ul> </li> </ul> <p></p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/release-management/#create-the-kubestellar-core-container-image","title":"Create the KubeStellar Core container image","text":"<p>First, login to quay.io with a user that has credentials to 'write' to the kubestellar quay repo <pre><code>docker login quay.io\n</code></pre></p> <p>then, remove any running container from moby/buildkit <pre><code>CONTAINER ID   IMAGE                           COMMAND              \nc943925fd137   moby/buildkit:buildx-stable-1   \"buildkitd\" \n\ndocker rm c943925fd137 -f\n</code></pre></p> <p>and remove the 'buildx' container image from your local docker images <pre><code>REPOSITORY      TAG               IMAGE ID       CREATED        SIZE\nmoby/buildkit   buildx-stable-1   16fc6c95ddff   10 days ago    168MB\n\ndocker rmi 16fc6c95ddff\n</code></pre></p> <p>finally, make the KubeStellar image from within the local copy of the release branch 'release-0.16' <pre><code>make kubestellar-image\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-space-core-build","title":"Create a Space Core build","text":"<pre><code>pushd space-framework\n./make spacecore-image v0.16.0\npopd\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-kubestellar-and-space-core-container-images-just-build-and-uploaded-to-quayio","title":"Update the KubeStellar and Space Core container images just build and uploaded to quay.io","text":"<p>Head up to quay.io and look for the image of KubeStellar Core container just uploaded. Tag the image with: 'latest', 'release-0.16', and 'v0.16.0' so that helm and other install methods pickup this image.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-kubestellar-core-helm-repository","title":"Update KubeStellar Core Helm repository","text":"<p>First, make sure you have a version of 'tar' that supports the '--transform' command line option <pre><code>brew install gnu-tar\n</code></pre></p> <p>then, from root of local copy of https://github.com/kubestellar/kubestellar repo: <pre><code>gtar -zcf kubestellar-core-9.tar.gz core-helm-chart/ --transform s/core-helm-chart/kubestellar-core/\nmv kubestellar-core-9.tar.gz ~\nshasum -a 256 ~/kubestellar-core-9.tar.gz\n</code></pre> Clone the homebrew-kubestellar repo <pre><code>git clone git@github.com:kubestellar/helm.git\ncd helm\ngit checkout main\n</code></pre></p> <p>then, from root of local copy of https://github.com/kubestellar/helm repo <pre><code>mv ~/kubestellar-core-9.tar.gz charts\n</code></pre></p> <p>next, update 'index.yaml' in root of local copy of helm repo (only update the data, not time, on lines 6 and 15): index.yaml<pre><code>apiVersion: v1\nentries:\n  kubestellar-core:\n  - apiVersion: v2\n    appVersion: v0.16.0\n    created: \"2023-10-30T12:00:00.727185806-04:00\"\ndescription: A Helm chart for KubeStellar Core deployment as a service\n    digest: 6f42d9e850308f8852842cd23d1b03ae5be068440c60b488597e4122809dec1e\n    icon: https://raw.githubusercontent.com/kubestellar/kubestellar/main/docs/favicons/favicon.ico\n    name: kubestellar\n    type: application\n    urls:\n    - https://helm.kubestellar.io/charts/kubestellar-core-{{ no such element: mkdocs.config.defaults.MkDocsConfig object['ks_new_helm_version'] }}.tar.gz\n    version: \"9\"\ngenerated: \"2023-10-30T12:00:00.727185806-04:00\"\n</code></pre></p> <p>finally, finally, push to the main branch <pre><code>git add .\ngit commit -m \"updates to main to support release v0.16.0 of KubeStellar Helm component\"\ngit push -u origin main\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-kubestellar-cli-brew-repository","title":"Update KubeStellar CLI Brew repository","text":"<p>Clone the homebrew-kubestellar repo <pre><code>git clone git@github.com:kubestellar/homebrew-kubestellar.git\ncd homebrew-kubestellar\ngit checkout main\n</code></pre></p> <p>edit the kubestellar_cli.rb file <pre><code>vi Formula/kubestellar_cli.rb\n</code></pre></p> <p>update all instances of 'url' from v0.15.0 to v0.16.0 (should be 6 of these) <pre><code>...\n    when :arm64\n      url \"https://github.com/kubestellar/kubestellar/releases/download/v0.16.0/kubestellaruser_v0.16.0_darwin_arm64.tar.gz\"\nsha256 \"5be4c0b676e8a4f5985d09f2cfe6c473bd2f56ebd3ef4803ca345e6f04d83d6b\" ...\n</code></pre></p> <p>then, update all instances of 'sha256' with the corresponding sha256 hash values in the build/release/checksums256.txt you create during the make-full-release.sh section above. (should be 6 of these)</p> <pre><code>...\n    when :arm64\n      url \"https://github.com/kubestellar/kubestellar/releases/download/v0.16.0/kubestellaruser_v0.16.0_darwin_arm64.tar.gz\"\nsha256 \"&lt;corresponding sha256 hash from checksums256.txt&gt;\" ...\n</code></pre> <p>finally, push to the main branch <pre><code>git add .\ngit commit -m \"updates to main to support release v0.16.0 of KubeStellar Brew component\"\ngit push -u origin main\n</code></pre></p> <p>and, to test <pre><code>brew update\nbrew install kubestellar-cli\n</code></pre></p> <p>you should see output that indicates an update for the kubestellar brew tap and then an update to version v0.16.0 of the kubestellar_cli brew formula.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#check-that-gh-workflows-for-docs-are-working","title":"Check that GH Workflows for docs are working","text":"<p>Check to make sure the GitHub workflows for doc generation, doc push, and broken links is working and passing https://github.com/kubestellar/kubestellar/actions/workflows/docs-gen-and-push.yml https://github.com/kubestellar/kubestellar/actions/workflows/broken-links-crawler.yml</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-an-email-addressed-to-kubestellar-devgooglegroupscom-and-kubestellar-usersgooglegroupscom","title":"Create an email addressed to kubestellar-dev@googlegroups.com and kubestellar-users@googlegroups.com","text":"<pre><code>Subject: KubeStellar release v0.16.0\n\nBody:\n\nDear KubeStellar Community,\n    Release v0.16.0 is now available at https://github.com/kubestellar/kubestellar/releases/tag/v0.16.0\n\nWhat's Changed\n\n\ud83d\udc1b Fix display of initial spaces after deploy in kube by @MikeSpreitzer in #1143\n\u2728 Generalize bootstrap wrt namespace in hosting cluster by @MikeSpreitzer in #1144\n\u2728 Generalize bootstrap wrt namespace in hosting cluster by @MikeSpreitzer in #1145\n\u2728 Switch to use k8s code generators by @ezrasilvera in #1139\n\u2728 Bump actions/checkout from 4.1.0 to 4.1.1 by @dependabot in #1151\n\ud83c\udf31 Align default core image ref in chart with coming release by @MikeSpreitzer in #1146\n\ud83d\udcd6Update dev-env.md by @francostellari in #1157\n\ud83d\udcd6Update Chart.yaml appVersion by @francostellari in #1158\n\ud83d\udc1b Use realpath to see through symlinks by @MikeSpreitzer in #1156\n\u2728 Increase kind version to v0.20 for ubuntu by @fab7 in #1155\n\ud83d\udcd6 Document syncer removal by @MikeSpreitzer in #1164\n\ud83c\udf31 Rename urmeta to ksmeta by @MikeSpreitzer in #1166\n\u2728 Make get-internal-kubeconfig fetch mid-level kubeconfig by @MikeSpreitzer in #1161\n\u2728 Make ensure/remove wmw insensitive to current workspace by @MikeSpreitzer in #1160\nNew Contributors\n\n@fab7 made their first contribution in #1155\nFull Changelog: v0.8.0\u2026v0.9.0\n\nThank you for your continued support,\n\nAndy\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#post-the-same-message-in-the-kubestellar-slack-channel","title":"Post the same message in the #kubestellar Slack channel","text":""},{"location":"Contribution%20guidelines/operations/testing-doc-prs/","title":"Testing Documentation PRs","text":""},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#testing-a-kubestellar-documentation-pr","title":"Testing a KubeStellar documentation PR","text":"<p>Here are the steps to checkout a git pull request for local testing.</p> <p>STEP-1: Checkout the Pull Request</p> <p>Helpers: GitHub, DevOpsCube</p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#11-use-the-pull-request-number-to-fetch-origin-note-be-sure-to-check-out-the-right-branch","title":"1.1 Use the pull request number to fetch origin (note: be sure to check out the right branch!)","text":"<p>\u00a0\u00a0\u00a0\u00a0Fetch the reference to the pull request based on its ID number, creating a new branch locally. Replace ID with your PR # and BRANCH_NAME with the desired branch name.</p> <p> <code>git fetch origin pull/ID/head:BRANCH_NAME</code> </p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#12-switch-to-the-new-branch","title":"1.2 Switch to the new branch","text":"<p>\u00a0\u00a0\u00a0Checkout the BRANCH_NAME where you have all the changes from the pull request.</p> <p> <code>git switch BRANCH_NAME</code> </p> <p>\u00a0\u00a0\u00a0\u00a0At this point, you can do anything you want with this branch. You can run some local tests, or merge other branches into the branch.</p> <p>STEP-2: Test and Build the Documentation (optional)</p> <p>\u00a0\u00a0\u00a0\u00a0Use this procedure if you want to view and modify the documentation in the branch you have checked out.</p> <p>Helpers: KubeStellar/docs, MkDocs</p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#21-install-mkdocs-and-its-requirements","title":"2.1 Install MkDocs and its requirements","text":"<pre><code>  cd docs\n  pip install mkdocs\n  pip install -r requirements.txt  \n</code></pre>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#22-build-and-view-the-documentation","title":"2.2 Build and view the documentation","text":"<p> <code>mkdocs serve</code></p> <p>\u00a0\u00a0\u00a0\u00a0Next, open a browser to http://127.0.0.1:8000 and review the changes.</p>"},{"location":"Contribution%20guidelines/security/security/","title":"Policy","text":""},{"location":"Contribution%20guidelines/security/security/#security-announcements","title":"Security Announcements","text":"<p>Join the kubestellar-security-announce group for emails about security and major API announcements.</p>"},{"location":"Contribution%20guidelines/security/security/#report-a-vulnerability","title":"Report a Vulnerability","text":"<p>We're extremely grateful for security researchers and users that report vulnerabilities to the KubeStellar Open Source Community. All reports are thoroughly investigated by a set of community volunteers.</p> <p>You can also email the private kubestellar-security-announce@googlegroups.com list with the security details and the details expected for all KubeStellar bug reports.</p>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-report-a-vulnerability","title":"When Should I Report a Vulnerability?","text":"<ul> <li>You think you discovered a potential security vulnerability in KubeStellar</li> <li>You are unsure how a vulnerability affects KubeStellar</li> <li>You think you discovered a vulnerability in another project that KubeStellar depends on</li> <li>For projects with their own vulnerability reporting and disclosure process, please report it directly there</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-not-report-a-vulnerability","title":"When Should I NOT Report a Vulnerability?","text":"<ul> <li>You need help tuning KubeStellar components for security</li> <li>You need help applying security related updates</li> <li>Your issue is not security related</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#security-vulnerability-response","title":"Security Vulnerability Response","text":"<p>Each report is acknowledged and analyzed by the maintainers of KubeStellar within 3 working days.</p> <p>Any vulnerability information shared with Security Response Committee stays within KubeStellar project and will not be disseminated to other projects unless it is necessary to get the issue fixed.</p> <p>As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated.</p>"},{"location":"Contribution%20guidelines/security/security/#public-disclosure-timing","title":"Public Disclosure Timing","text":"<p>A public disclosure date is negotiated by the KubeStellar Security Response Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible once a user mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure is from immediate (especially if it's already publicly known) to a few weeks. For a vulnerability with a straightforward mitigation, we expect report date to disclosure date to be on the order of 7 days. The KubeStellar maintainers hold the final say when setting a disclosure date.</p>"},{"location":"Contribution%20guidelines/security/security_contacts/","title":"Contacts","text":"<p>Defined below are the security contacts for this repo.</p> <p>They are the contact point for the Product Security Committee to reach out to for triaging and handling of incoming issues.</p> <p>The below names agree to address security concerns if and when they arise.</p> <p>DO NOT REPORT SECURITY VULNERABILITIES DIRECTLY TO THESE NAMES, SEND INFORMATION TO kubestellar-security-announce@googlegroups.com</p> <p>clubanderson MikeSpreitzer ezrasilvera pdettori</p>"},{"location":"Getting-Started/infomercial/","title":"KubeStellar - The Infomercial","text":""},{"location":"Getting-Started/quickstart/","title":"Detailed QuickStart","text":""},{"location":"Getting-Started/quickstart/#demo-video","title":"Demo Video","text":"<p>Watch this video to see a step-by-step demo of KubeStellar running and then follow the instructions below to get your own KubeStellar started quickly.</p> <p> </p> <p>Estimated time to complete this example:</p> <p>~4 minutes (after installing prerequisites)</p>"},{"location":"Getting-Started/quickstart/#setup-instructions","title":"Setup Instructions","text":"<p>Table of contents:</p> <ol> <li>Check Required Packages</li> <li>Install and run kcp and KubeStellar</li> <li>Example deployment of Apache HTTP Server workload into two local kind clusters<ol> <li>Stand up two kind clusters: florin and guilder</li> <li>Onboarding the clusters</li> <li>Create and deploy the Apache Server workload into florin and guilder clusters</li> </ol> </li> <li>Teardown the environment</li> <li>Next Steps</li> </ol> <p>This guide is intended to show how to (1) quickly bring up a KubeStellar environment with its dependencies from a binary release and then (2) run through a simple example usage.</p>"},{"location":"Getting-Started/quickstart/#1-check-required-packages","title":"1. Check Required Packages","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p>"},{"location":"Getting-Started/quickstart/#2-install-and-run-kcp-and-kubestellar","title":"2. Install and run kcp and KUBESTELLAR","text":"<p>We support two ways to deploy kcp and KubeStellar. The older way is to run them as bare processes. The newer way is to deploy them as workload in a Kubernetes (possibly OpenShift) cluster.</p>"},{"location":"Getting-Started/quickstart/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":"<p>The following commands will download the kcp and KubeStellar executables into subdirectories of your current working directory, deploy (i.e., start and configure) kcp and KubeStellar as bare processes, and configure your shell to use kcp and KubeStellar.  If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation about the commands for bare process deployment.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.15.0\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport KUBECONFIG=\"$(pwd)/.kcp/admin.kubeconfig\"\nexport SM_CONFIG=~/.kube/config\nmkdir -p ${PWD}/temp-space-config\n</code></pre> <p>Check that <code>KubeStellar</code> is running.</p> <p>First, check that controllers are running with the following command:</p> <pre><code>ps aux | grep -e mailbox-controller -e placement-translator -e kubestellar-where-resolver\n</code></pre> <p>which should yield something like:</p> <pre><code>user     1892  0.0  0.3 747644 29628 pts/1    Sl   10:51   0:00 mailbox-controller -v=2\nuser     1902  0.3  0.3 743652 27504 pts/1    Sl   10:51   0:02 kubestellar-where-resolver -v 2\nuser     1912  0.3  0.5 760428 41660 pts/1    Sl   10:51   0:02 placement-translator -v=2\n</code></pre> <p>Second, check that TMC compute service provider space and the KubeStellar Edge Service Provider space (<code>espw</code>) have been created with the following command:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <p>which should yield:</p> <pre><code>NAMESPACE               NAME   AGE\nspaceprovider-default   espw   1h\nspaceprovider-default   imw1   1h\nspaceprovider-default   wmw1   1h\n</code></pre>"},{"location":"Getting-Started/quickstart/#deploy-kcp-and-kubestellar-as-kubernetes-workload","title":"Deploy kcp and KubeStellar as Kubernetes workload","text":"<p>This requires a KubeStellar release GREATER THAN v0.5.0.</p> <p>This example uses a total of three <code>kind</code> clusters, which tends to run into a known issue with a known work-around, so take care of that.</p> <p>Before you can deploy kcp and KubeStellar as workload in a Kubernetes cluster, you need a Kubernetes cluster and it needs to have an Ingress controller installed.  We use the term \"hosting cluster\" for the cluster that plays this role.  In this quickstart, we make such a cluster with kind.  Follow the developer directions for making a hosting cluster with kind; you need not worry about loading a locally built container image into that cluster.</p> <p>This example uses the domain name \"hostname.favorite.my\" for the machine where you invoked <code>kind create cluster</code>. If you have not already done so then issue the following command, replacing <code>a_good_IP_address_for_this_machine</code> with an IPv4 address for your machine that can be reached from inside a container or VM (i.e., not 127.0.0.1).</p> <pre><code>sudo sh -c \"echo a_good_IP_address_for_this_machine hostname.favorite.my &gt;&gt; /etc/hosts\"\n</code></pre> <p>The next command relies on <code>kubectl</code> already being configured to manipulate the hosting cluster, which is indeed the state that <code>kind create cluster</code> leaves it in.</p> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory and (b) deploy (i.e., start and configure) kcp and KubeStellar as workload in the hosting cluster. If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation for the commands about deployment into a Kubernetes cluster.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.15.0 --external-endpoint hostname.favorite.my:1119\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport SM_CONFIG=~/.kube/config\nmkdir -p ${PWD}/temp-space-config\n</code></pre> <p>Using your original <code>kubectl</code> configuration that manipulates the hosting cluster, check that the KubeStellar Deployment has its intended one running Pod.</p> <pre><code>kubectl get deployments -n kubestellar\n</code></pre> <p>which should yield something like:</p> <pre><code>NAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-server   1/1     1            1           2m42s\n</code></pre> <p>It may take some time for that Pod to reach Running state.</p> <p>The bootstrap command above will print out instructions to set your KUBECONFIG environment variable to the pathname of a kubeconfig file that you can use as a user of kcp and KubeStellar.  Do that now, for the benefit of the remaining commands in this example.  It will look something like the following command.</p> <pre><code>export KUBECONFIG=\"$(pwd)/kubestellar.kubeconfig\"\n</code></pre> <p>Check that the TMC compute service provider spaces and the KubeStellar Edge Service Provider space (<code>espw</code>) have been created with the following command:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <p>which should yield:</p> <pre><code>NAMESPACE               NAME   AGE\nspaceprovider-default   espw   1h\nspaceprovider-default   imw1   1h\nspaceprovider-default   wmw1   1h\n</code></pre>"},{"location":"Getting-Started/quickstart/#3-example-deployment-of-apache-http-server-workload-into-two-local-kind-clusters","title":"3. Example deployment of Apache HTTP Server workload into two local kind clusters","text":"<p>In this example you will create two edge clusters and define one workload that will be distributed from the center to those edge clusters.  This example is similar to the one described more expansively on the website, but with the some steps reorganized and combined and the special workload and summarization aspirations removed.</p>"},{"location":"Getting-Started/quickstart/#a-stand-up-two-kind-clusters-florin-and-guilder","title":"a. Stand up two kind clusters: florin and guilder","text":"<p>Create the first edge cluster:</p> <pre><code>kind create cluster --name florin --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'florin' from a previous exercise of KubeStellar, please delete the florin cluster ('kind delete cluster --name florin') and create it using the instruction above.</p> <p>Create the second edge cluster:</p> <pre><code>kind create cluster --name guilder --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'guilder' from a previous exercise of KubeStellar, please delete the guilder cluster ('kind delete cluster --name guilder') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart/#b-onboarding-the-clusters","title":"b. Onboarding the clusters","text":"<p>KubeStellar will have created an Inventory Management space (IMW) for the user to put inventory objects in, describing the user's clusters. The IMW that is automatically created for the user is at <code>imw1</code>.</p> <p>Let's begin by onboarding the <code>florin</code> cluster:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl kubestellar prep-for-cluster --imw root:imw1 florin env=prod\n</code></pre> <p>which should yield something like:</p> <pre><code>synctarget.edge.kubestellar.io/florin created\nlocation.edge.kubestellar.io/florin created\nsynctarget.edge.kubestellar.io/florin labeled\nlocation.edge.kubestellar.io/florin labeled\nCreating service account \"kubestellar-syncer-florin-1yi5q9c4\"\nCreating cluster role \"kubestellar-syncer-florin-1yi5q9c4\" to give service account \"kubestellar-syncer-florin-1yi5q9c4\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-1yi5q9c4\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-1yi5q9c4\" to bind service account \"kubestellar-syncer-florin-1yi5q9c4\" to cluster role \"kubestellar-syncer-florin-1yi5q9c4\".\n\nWrote workload execution cluster (WEC) manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-1yi5q9c4\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-1yi5q9c4\" kubestellar-syncer-florin-1yi5q9c4\n\nto verify the syncer pod is running.\n</code></pre> <p>An edge syncer manifest yaml file was created in your current directory: <code>florin-syncer.yaml</code>. The default for the output file is the name of the SyncTarget object with \u201c-syncer.yaml\u201d appended.</p> <p>Now let's deploy the edge syncer to the <code>florin</code> edge cluster:</p> <pre><code>kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <p>which should yield something like:</p> <pre><code>namespace/kubestellar-syncer-florin-1yi5q9c4 created\nserviceaccount/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4 created\ndeployment.apps/kubestellar-syncer-florin-1yi5q9c4 created\n</code></pre> <p>Optionally, check that the edge syncer pod is running:</p> <pre><code>kubectl --context kind-florin get pods -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                                  READY   STATUS    RESTARTS   AGE\nkubestellar-syncer-florin-1yi5q9c4   kubestellar-syncer-florin-1yi5q9c4-77cb588c89-xc5qr   1/1     Running   0          12m\nkube-system                          coredns-565d847f94-92f4k                              1/1     Running   0          58m\nkube-system                          coredns-565d847f94-kgddm                              1/1     Running   0          58m\nkube-system                          etcd-florin-control-plane                             1/1     Running   0          58m\nkube-system                          kindnet-p8vkv                                         1/1     Running   0          58m\nkube-system                          kube-apiserver-florin-control-plane                   1/1     Running   0          58m\nkube-system                          kube-controller-manager-florin-control-plane          1/1     Running   0          58m\nkube-system                          kube-proxy-jmxsg                                      1/1     Running   0          58m\nkube-system                          kube-scheduler-florin-control-plane                   1/1     Running   0          58m\nlocal-path-storage                   local-path-provisioner-684f458cdd-kw2xz               1/1     Running   0          58m\n</code></pre> <p>Now, let's onboard the <code>guilder</code> cluster:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl kubestellar prep-for-cluster --imw root:imw1 guilder env=prod extended=yes\n</code></pre> <p>Apply the created edge syncer manifest:</p> <pre><code>kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre>"},{"location":"Getting-Started/quickstart/#c-create-and-deploy-the-apache-server-workload-into-florin-and-guilder-clusters","title":"c. Create and deploy the Apache Server workload into florin and guilder clusters","text":"<p>KubeStellar will have automatically created a Workload Management space (WMW) for the user to store workload descriptions and KubeStellar Core control objects in. The automatically created WMW is at <code>wmw1</code>.</p> <p>Create the <code>EdgePlacement</code> object for your workload. Its \u201cwhere predicate\u201d (the locationSelectors array) has one label selector that matches the Location objects (<code>florin</code> and <code>guilder</code>) created earlier, thus directing the workload to both edge clusters. The <code>upsync</code> field is only a demonstration of the syntax, it plays no functional role in this scenario.</p> <p>In the <code>wmw1</code> space create the following <code>EdgePlacement</code> object: </p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {common: \"yes\"}\n    objectNames: [ commond ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <p>Put the prescription of the HTTP server workload into the WMW. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>edge-placement-c</code>) object created above. </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"yes\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Now, let's check that the deployment was created in the <code>florin</code> edge cluster - it may take a few 10s of seconds to appear:</p> <pre><code>kubectl --context kind-florin get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                          commond                              1/1     1            1           6m48s\nkubestellar-syncer-florin-2upj1awn   kubestellar-syncer-florin-2upj1awn   1/1     1            1           16m\nkube-system                          coredns                              2/2     2            2           28m\nlocal-path-storage                   local-path-provisioner               1/1     1            1           28m\n</code></pre> <p>Also, let's check that the deployment was created in the <code>guilder</code> edge cluster:</p> <pre><code>kubectl --context kind-guilder get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                           commond                               1/1     1            1           7m54s\nkubestellar-syncer-guilder-6tuay5d6   kubestellar-syncer-guilder-6tuay5d6   1/1     1            1           12m\nkube-system                           coredns                               2/2     2            2           27m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           27m\n</code></pre> <p>Lastly, let's check that the workload is working in both clusters:</p> <p>For <code>florin</code>:</p> <pre><code>while [[ $(kubectl --context kind-florin get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8094\n</code></pre> <p>which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>For <code>guilder</code>:</p> <p><pre><code>while [[ $(kubectl --context kind-guilder get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8096\n</code></pre> which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Congratulations, you\u2019ve just deployed a workload to two edge clusters using kubestellar! To learn more about kubestellar please visit our User Guide</p>"},{"location":"Getting-Started/quickstart/#4-teardown-the-environment","title":"4. Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Getting-Started/quickstart/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and the space provider with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Getting-Started/quickstart/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is the space provider and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Getting-Started/quickstart/#5-next-steps","title":"5. Next Steps","text":"<p>What you just did is a shortened version of the  more detailed example on the website, but with the some steps reorganized and combined and the special workload and summarization aspiration removed.  You can continue from here, learning more details about what you did in the QuickStart, and adding on some more steps for the special workload.</p>"},{"location":"Getting-Started/user-guide/","title":"User Guide","text":""},{"location":"Getting-Started/user-guide/#kubestellar-primer","title":"KubeStellar Primer","text":"<p>This is a description of the concepts behind KubeStellar.</p>"},{"location":"Getting-Started/user-guide/#glossary","title":"Glossary","text":"<p>Downsynced Object - One of two categories of workload object, complementary to \"upsynced object\".  In KubeStellar, a downsynced object first appears in a Workload Description Space and the object's desired state propagates from there through Mailbox Spaces to Workload Execution Clusters and that object's reported state originates in the Workload Execution Clusters and propagates back to the Mailbox Spaces and in the future will be summarized into the Workload Description Space.</p> <p>EdgePlacement - A kind of Kubernetes API object, in a Workload Description Space. One of objects these binds some workload (\"what\") with a set of workload execution clusters (\"where\") it should run. The workload is identified by a predicate over namespaced objects and a predicate over cluster-scoped objects. The where is identified by a predicate over workload execution clusters as represented by <code>Location</code> objects.</p> <p>Inventory Space (IS) - Holds the <code>SyncTarget</code> and <code>Location</code> objects describing the Workload Execution Clusters. </p> <p>+kube-bind - KubeStellar is introducing an abstraction layer for \"spaces\" and developing the ability to use kube-bind for API export and binding.</p> <p>KubeStellar Core Space (KCS) - Exports the Kubestellar API.</p> <p>KubeStellar Syncer - The KubeStellar agent in a Workload Execution Cluster; syncs workload objects between the Workload Execution Cluster and the corresponding Mailbox Space.</p> <p>Location - A kind of Kubernetes API object, in an Inventory Space. Paired one-to-one with a <code>SyncTarget</code> object in the same space. Together these describe a workload execution cluster. The Location's labels are tested by the \"where predicate\" in an <code>EdgePlacement</code> object, and this object's labels and annotations provide values used in customization of workload objects going to the workload execution cluster.</p> <p>Mailbox Controller - One of the central KubeStellar controllers; maintains a Mailbox Space for each <code>SyncTarget</code> object. This includes putting an APIBinding to the KubeStellar API into those mailbox spaces.</p> <p>Mailbox Space - There is one Mailbox Space for each workload execution cluster. It stores the <code>SyncerConfig</code> object and copies of the workload(s).</p> <p>PlacementTranslator - One of the central KubeStellar controllers; maintains the <code>SyncerConfig</code> objects in the Mailbox Spaces and syncs workload objects between the Workload Description Spaces and the Mailbox Spaces.</p> <p>SinglePlacementSlice - A kind of Kubernetes API object, in a Workload Description Space.  Such an object holds a list of references to <code>Location</code> &amp; <code>SyncTarget</code> objects that match the \"where predicate\" of an <code>EdgePlacement</code>.  Currently there is exactly one <code>SinglePlacementSlice</code> for each <code>EdgePlacement</code> but in the future the matches for one <code>EdgePlacement</code> could be spread among several <code>SinglePlacementSlice</code> objects (analogously to <code>EndpointSlice</code> vs <code>Service</code> in Kubernetes).</p> <p>Space Object - A kind of Kubernetes API object that represents a physical space.  Its \"kind\" is <code>Space</code>, so we include the \"object\" part when writing specifically about the API objects to clearly distinguish them from what they represent.</p> <p>Space, physical space (use the latter term to clearly distinguish from \"space object\") - A thing that behaves like a Kubernetes kube-apiserver (including the persistent storage behind it) and the subset of controllers in the kube-controller-manager that are concerned with API machinery generalities (not management of containerized workloads). A KubeFlex logical cluster is an example. A regular Kubernetes cluster is another example.</p> <p>Status Summarizer - A planned central KubeStellar controller that will maintain the status summary objects in the Workload Description Spaces as a function of the <code>EdgePlacement</code> objects and the workload objects in the Mailbox Spaces.</p> <p>SyncerConfig - A kind of Kubernetes API object, in a Mailbox Space. Such an object holds the dynamic configuration for the syncer in the corresponding workload execution cluster.</p> <p>SyncTarget - A kind of Kubernetes API object, in an Inventory Space. Paired one-to-one with a Location in the same space, jointly representing a Workload Execution Cluster.</p> <p>Upsynced Object - One of two categories of workload object, complementary to \"downsynced object\".  Upsynced objects originate in Workload Execution Clusters and propagate inward to Mailbox Spaces and in the future will be summarized into Workload Description Spaces.</p> <p>Where Resolver - One of the central KubeStellar controllers; tests the <code>Location</code> objects against the \"where predicates\" in the <code>EdgePlacement</code> objects to maintain the corresponding <code>SinglePlacementSlice</code> objects.</p> <p>Workload Description Space (WDS) - Holds workload objects and the adjacent KubeStellar control objects, which are the <code>EdgePlacement</code>, <code>SinglePlacementSlice</code>, and <code>Customizer</code> objects and, eventually, the ones developed to prescribe summarization.</p> <p>Workload Execution Cluster - A Kubernetes cluster which can execute a workload. In the examples on this website, we use Kind clusters.</p>"},{"location":"Getting-Started/user-guide/#older-terminology","title":"Older Terminology","text":"<p>There have been some terminology shifts since the start of the project.  The project started with a focus on edge computing scenarios; later we realized that the technical problems addressed are not limited to those scenarios.  We are working on generalizing KubeStellar so that it can run in the context of any space provider.</p> <ul> <li>The term \"space\" is intended to be a generalization covering both a \"logical cluster\" or \"workspace\" and other things that have the same essential behavior</li> <li>The term \"workload execution cluster\" was formerly \"edge cluster\"</li> <li>The term \"workload description space\" was formerly \"workload management workspace\"</li> <li>The term \"inventory space\" was formerly \"inventory management workspace\"</li> <li>The term \"kubestellar core space\" was formerly \"edge service provider workspace\"</li> <li>The term \"mailbox space\" was formerly \"mailbox workspace\"</li> </ul>"},{"location":"Getting-Started/user-quickstart-kind/","title":"Quickstart using Kind","text":""},{"location":"Getting-Started/user-quickstart-kind/#how-to-deploy-and-use-kubestellar-on-kind-kubernetes-clusters","title":"How to deploy and use KUBESTELLAR on Kind Kubernetes Clusters","text":"Goals <p>This guide will show how to:</p> <ol> <li>quickly deploy the KubeStellar Core component on a Kind cluster using helm (ks-core), </li> <li>install the KubeStellar user commands and kubectl plugins on your computer with brew,</li> <li>retrieve the KubeStellar Core component kubeconfig, </li> <li>install the KubeStellar Syncer component on two edge Kind clusters (ks-edge-cluster1 and ks-edge-cluster2), </li> <li>deploy an example kubernetes workload to both edge Kind clusters from KubeStellar Core (ks-core),</li> <li>view the example kubernetes workload running on two edge Kind clusters (ks-edge-cluster1 and ks-edge-cluster2)</li> <li>view the status of your deployment across both edge Kind clusters from KubeStellar Core (ks-core)</li> </ol> <p>important: For this quickstart you will need to know how to use kubernetes' kubeconfig context to access multiple clusters.  You can learn more about kubeconfig context here</p> Pre-reqsuh oh, error?Special notes for Debian users <ul> <li> <p>kubectl (version range expected: 1.24-1.26)</p> </li> <li> <p>helm - to deploy the KubeStellar-core helm chart</p> </li> <li> <p>brew - to install the KubeStellar user commands and kubectl plugins</p> </li> <li> <p>Kind - to create a few small kubernetes clusters</p> </li> <li> <p>3 Kind clusters configured as follows</p> </li> </ul> <p>create the ks-core kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-core --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 1119\n    protocol: TCP\nEOF\n</code></pre></p> <p>Be sure to create an ingress control with SSL passthrough to ks-core. This is a special requirement for Kind that allows access to the KubeStellar core running on ks-core. <pre><code>KUBECONFIG=~/.kube/config kubectl \\\ncreate -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\n</code></pre> Wait about 20 seconds and then check if the ingress control is ready on ks-core: <pre><code>sleep 20\nKUBECONFIG=~/.kube/config kubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=90s\n</code></pre></p> <p>create the ks-edge-cluster1 kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-edge-cluster1 --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre></p> <p>create the ks-edge-cluster2 kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-edge-cluster2 --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre></p> <p>important: delete any existing kubernetes contexts of the clusters you may have created previously <pre><code>KUBECONFIG=~/.kube/config kubectl config delete-context ks-core || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster1 || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster2 || true\n</code></pre></p> <p> </p> <p>important: rename the kubernetes contexts of the Kind clusters to match their use in this guide <pre><code>KUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-core ks-core\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-edge-cluster1 ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-edge-cluster2 ks-edge-cluster2\n</code></pre></p> <p>if you apply the ingress and then receive an error while waiting:   <code>error: no matching resources found</code></p> <p>this might mean that you did not wait long enough before issuing the check command. Simply try the check command again.</p> <p>on Debian, the syncers on ks-edge-cluster1 and ks-edge-cluster2 will not resolve the kubestellar.core hostname </p> <p>You have 2 choices:  </p> <ol> <li> <p>Use the value of <code>hostname -f</code> instead of kubestellar.core as your \"EXTERNAL_HOSTNAME\" in \"Step 1:  Deploy the KubeStellar Core Component\", or</p> </li> <li> <p>Just before step 6 in the KubeStellar User Quickstart for Kind do the following</p> </li> </ol> <p>Add IP/domain to /etc/hosts of cluster1/cluster2 containers (replace with appropriate IP address):</p> <pre><code>docker exec -it $(docker ps | grep ks-edge-cluster1 | cut -d \" \" -f 1) \\\n    sh -c \"echo '192.168.122.144 kubestellar.core' &gt;&gt; /etc/hosts\"\ndocker exec -it $(docker ps | grep ks-edge-cluster2 | cut -d \" \" -f 1) \\\n    sh -c \"echo '192.168.122.144 kubestellar.core' &gt;&gt; /etc/hosts\"\n</code></pre> <p>Edit coredns ConfigMap for cluster1 and cluster1 (see added lines in example):</p> <pre><code>KUBECONFIG=~/.kube/config kubectl edit cm coredns -n kube-system --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl edit cm coredns -n kube-system --context=ks-edge-cluster2\n</code></pre> <p>add the highlighted information <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n           lameduck 5s\n        }\n        hosts /etc/coredns/customdomains.db core {  \n          fallthrough                               \n        }                                          \n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n  customdomains.db: |                   \n    192.168.122.144 kubestellar.core  \nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2023-10-24T19:18:05Z\"\n  name: coredns\n  namespace: kube-system\n  resourceVersion: \"10602\"\n  uid: 3930c18f-23e8-4d0b-9ddf-658fdf3cb20f\n</code></pre></p> <p>Edit Deployment for coredns on cluster1 and cluster2, adding the key/path at the given location: <pre><code>KUBECONFIG=~/.kube/config kubectl edit -n kube-system \\\n    deployment coredns --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl edit -n kube-system \\\n    deployment coredns --context=ks-edge-cluster2\n</code></pre></p> <pre><code>spec:\n  template:\n    spec:\n      volumes:\n      - configMap:\n          items:\n          - key: customdomains.db\n            path: customdomains.db\n</code></pre> <p>Restart coredns pods: <pre><code>KUBECONFIG=~/.kube/config kubectl rollout restart \\\n    -n kube-system deployment/coredns --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl rollout restart \\\n    -n kube-system deployment/coredns --context=ks-edge-cluster2\n</code></pre></p> <p>(adapted from \"The Cluster-wise solution\" at https://stackoverflow.com/questions/37166822/is-there-a-way-to-add-arbitrary-records-to-kube-dns)</p>"},{"location":"Getting-Started/user-quickstart-kind/#1-deploy-the-kubestellar-core-component","title":"1. Deploy the KUBESTELLAR Core component","text":"deployuh oh, error?open a bug report <p>deploy the KubeStellar Core components on the ks-core Kind cluster you created in the pre-req section above  </p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install kubestellar/kubestellar-core \\\n--set EXTERNAL_HOSTNAME=\"kubestellar.core\" \\\n--set EXTERNAL_PORT=1119 \\\n--namespace kubestellar \\\n--generate-name\n</code></pre> <p>important: You must add 'kubestellar.core' to your /etc/hosts file with the local network IP address (e.g., 192.168.x.y) where your ks-core Kind cluster is running. DO NOT use <code>127.0.0.1</code> because the ks-edge-cluster1 and ks-edge-cluster2 kind clusters map <code>127.0.0.1</code> to their local kubernetes cluster, not the ks-core kind cluster.</p> <p>run the following to wait for KubeStellar to be ready to take requests: </p> <pre><code>echo -n 'Waiting for KubeStellar to be ready'\nwhile ! KUBECONFIG=~/.kube/config kubectl exec $(KUBECONFIG=~/.kube/config kubectl get pod \\\n--selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n-n kubestellar -c init -- ls /home/kubestellar/ready &amp;&gt; /dev/null; do\nsleep 10\necho -n \".\"\ndone\necho; echo; echo \"KubeStellar is now ready to take requests\"\n</code></pre> <p>Checking the initialization log to see if there are any obvious errors:  <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  \nkubectl logs \\\n  $(kubectl get pod --selector=app=kubestellar \\\n  -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n  -n kubestellar -c init\n</code></pre>  if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-kind/#2-install-kubestellars-user-commands-and-kubectl-plugins","title":"2. Install KUBESTELLAR's user commands and kubectl plugins","text":"installremoveuh oh, no brew? <pre><code>if ! command -v brew; then\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/runner/.bashrc\n    eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nmore /etc/hosts\n    # sudo echo $(curl https://api.ipify.org) kubestellar.core | sudo tee -a /etc/host\nfi\nbrew tap kubestellar/kubestellar\nbrew update\nbrew install kubestellar-cli\n</code></pre> <pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) \\\n    --kubestellar-version latest --deploy false\n\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre>"},{"location":"Getting-Started/user-quickstart-kind/#3-view-your-kubestellar-core-space-environment","title":"3. View your KUBESTELLAR Core Space environment","text":"show all available KubeStellar Core Spacesuh oh, error?open a bug report <p>Let's store the KubeStellar kubeconfig to a file we can reference later and then check out the Spaces KubeStellar created during installation</p> <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-core get spaces -A </code></pre> <p>Did you received the following error:  <code>`Error: Get \"https://some_hostname.some_domain_name:1119/clusters/root/apis/tenancy.kcp.io/v1alpha1/workspaces\": dial tcp: lookup some_hostname.some_domain_name on x.x.x.x: no such host</code></p> <p>A common error occurs if you set your port number to a pre-occupied port number and/or you set your EXTERNAL_HOSTNAME to something other than \"localhost\" so that you can reach your KubeStellar Core from another host, check the following:</p> <p>Check if the port specified in the ks-core Kind cluster configuration and the EXTERNAL_PORT helm value are occupied by another application:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. is the `hostPort`` specified in the ks-core Kind cluster configuration is occupied by another process?  If so, delete the ks-core Kind cluster and create it again using an available port for your 'hostPort' value</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. if you change the port for your ks-core 'hostPort', remember to also use that port as the helm 'EXTERNAL_PORT' value</p> <p>Check that your EXTERNAL_HOSTNAME helm value is reachable via DNS:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. use 'nslookup ' to make sure there is a valid IP address associated with the hostname you have specified <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. make sure your EXTERNAL_HOSTNAME and associated IP address are listed in your /etc/hosts file.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03. make sure the IP address is associated with the system where you have deployed the ks-core Kind cluster</p> <p>if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-kind/#4-install-kubestellar-syncers-on-your-edge-clusters","title":"4. Install KUBESTELLAR Syncers on your Edge Clusters","text":"Prep and apply <p>prepare KubeStellar Syncers, with <code>kubestellar prep-for-cluster</code>, for ks-edge-cluster1 and ks-edge-cluster2 and then apply the files that <code>kubestellar prep-for-cluster</code> prepared for you</p> <p>important: make sure you created Kind clusters for ks-edge-cluster1 and ks-edge-cluster2 from the pre-req step above before proceeding how-to-deploy-and-use-kubestellar</p> <pre><code>KUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster1 \\\nenv=ks-edge-cluster1 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\nKUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster2 \\\nenv=ks-edge-cluster2 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\n</code></pre> <pre><code>#apply ks-edge-cluster1 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 apply -f ks-edge-cluster1-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster1 correctly\n#apply ks-edge-cluster2 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 apply -f ks-edge-cluster2-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster2 correctly\n</code></pre> <p>This example (and others) involve user-managed kubeconfig files. These examples organize those files into one subdirectory. Start by making sure that directory exists without any contents left over from previous runs.</p> <pre><code>MY_KUBECONFIGS=${PWD}/my-kubeconfigs\nrm -rf \"$MY_KUBECONFIGS\"\nmkdir -p \"$MY_KUBECONFIGS\"\n</code></pre> <p>Wait for the mailbox controller to create the corresponding mailbox spaces and remember them.</p> <pre><code>while [ $(KUBECONFIG=~/.kube/config kubectl get spaces -A | grep -c -e -mb-) -lt 2 ]; do sleep 10; done\nMB1=$(KUBECONFIG=~/.kube/config kubectl get spaces -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"ks-edge-cluster1\") | .name')\necho The mailbox for ks-edge-cluster1 is $MB1\nMB2=$(KUBECONFIG=~/.kube/config kubectl get spaces -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"ks-edge-cluster2\") | .name')\necho The mailbox for ks-edge-cluster2 is $MB2\nMB1_KUBECONFIG=\"${MY_KUBECONFIGS}/${MB1}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $MB1 --kubeconfig ~/.kube/config $MB1_KUBECONFIG\nMB2_KUBECONFIG=\"${MY_KUBECONFIGS}/${MB2}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $MB2 --kubeconfig ~/.kube/config $MB2_KUBECONFIG\nWMW1_KUBECONFIG=\"${MY_KUBECONFIGS}/wmw1.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig wmw1 --kubeconfig ~/.kube/config $WMW1_KUBECONFIG\n</code></pre>"},{"location":"Getting-Started/user-quickstart-kind/#5-deploy-an-apache-web-server-to-ks-edge-cluster1-and-ks-edge-cluster2","title":"5. Deploy an Apache Web Server to ks-edge-cluster1 and ks-edge-cluster2","text":"deploy <p>KubeStellar's helm chart automatically creates a Workload Management  Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>In the <code>wmw1</code> space create the following <code>EdgePlacement</code> object:   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly  <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre></p> <p>Check if your configmap and deployment were created correctly.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments,cm -n my-namespace\n</code></pre>"},{"location":"Getting-Started/user-quickstart-kind/#6-view-the-apache-web-server-running-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"6. View the Apache Web Server running on ks-edge-cluster1 and ks-edge-cluster2","text":"viewuh oh, error? <p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear):  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same:  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters:  For ks-edge-cluster1:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8094\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8096\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>If you are unable to see the namespace 'my-namespace' or the deployment 'my-first-kubestellar-deployment' you can view the logs for the KubeStellar syncer on the ks-edge-cluster1 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster1\nks_ns_edge_cluster1=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster1 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster1\n</code></pre> <p>and on the ks-edge-cluster2 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster2\nks_ns_edge_cluster2=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster2 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster2\n</code></pre> <p>If you see a <code>connection refused</code> error in either KubeStellar Syncer log(s):</p> <p><code>E1021 21:22:58.000110       1 reflector.go:138] k8s.io/client-go@v0.0.0-20230210192259-aaa28aa88b2d/tools/cache/reflector.go:215: Failed to watch *v2alpha1.EdgeSyncConfig: failed to list *v2alpha1.EdgeSyncConfig: Get \"https://kubestellar.core:1119/apis/edge.kubestellar.io/v2alpha1/edgesyncconfigs?limit=500&amp;resourceVersion=0\": dial tcp 127.0.0.1:1119: connect: connection refused</code></p> <p>it means that your <code>/etc/hosts</code> does not have a proper IP address (NOT <code>127.0.0.1</code>) listed for the <code>kubestellar.core</code> hostname. Once there is a valid address in <code>/etc/hosts</code> for <code>kubestellar.core</code>, the syncer will begin to work properly and pull the namespace, deployment, and configmap from this instruction set. </p> <p>Mac OS users may also experience issues when <code>stealth mode</code> (system settings/firewall).  If you decide to disable this mode temporarily, please be sure to re-enable it once you are finished with this guide.</p>"},{"location":"Getting-Started/user-quickstart-kind/#7-check-the-status-of-your-apache-server-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"7. Check the status of your Apache Server on ks-edge-cluster1 and ks-edge-cluster2","text":"<pre><code>TODO\n</code></pre> <p>what's next... how to upsync a resource how to create, but not overwrite/update a synchronized resource  </p>"},{"location":"Getting-Started/user-quickstart-kind/#_1","title":"Quickstart using Kind","text":""},{"location":"Getting-Started/user-quickstart-kind/#how-to-use-an-existing-kubestellar-environment","title":"How to use an existing KUBESTELLAR environment","text":""},{"location":"Getting-Started/user-quickstart-kind/#1-install-kubestellars-user-commands-and-kubectl-plugins","title":"1. Install KUBESTELLAR's user commands and kubectl plugins","text":"installremoveuh oh, no brew? <pre><code>if ! command -v brew; then\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/runner/.bashrc\n    eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nmore /etc/hosts\n    # sudo echo $(curl https://api.ipify.org) kubestellar.core | sudo tee -a /etc/host\nfi\nbrew tap kubestellar/kubestellar\nbrew update\nbrew install kubestellar-cli\n</code></pre> <pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) \\\n    --kubestellar-version latest --deploy false\n\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre>"},{"location":"Getting-Started/user-quickstart-kind/#2-view-your-kubestellar-core-space-environment","title":"2. View your KUBESTELLAR Core Space environment","text":"show all available KubeStellar Core Spacesuh oh, error?open a bug report <p>Let's store the KubeStellar kubeconfig to a file we can reference later and then check out the Spaces KubeStellar created during installation</p> <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-core get spaces -A \n</code></pre> <p>Did you received the following error:  <code>`Error: Get \"https://some_hostname.some_domain_name:1119/clusters/root/apis/tenancy.kcp.io/v1alpha1/workspaces\": dial tcp: lookup some_hostname.some_domain_name on x.x.x.x: no such host</code></p> <p>A common error occurs if you set your port number to a pre-occupied port number and/or you set your EXTERNAL_HOSTNAME to something other than \"localhost\" so that you can reach your KubeStellar Core from another host, check the following:</p> <p>Check if the port specified in the ks-core Kind cluster configuration and the EXTERNAL_PORT helm value are occupied by another application:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. is the `hostPort`` specified in the ks-core Kind cluster configuration is occupied by another process?  If so, delete the ks-core Kind cluster and create it again using an available port for your 'hostPort' value</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. if you change the port for your ks-core 'hostPort', remember to also use that port as the helm 'EXTERNAL_PORT' value</p> <p>Check that your EXTERNAL_HOSTNAME helm value is reachable via DNS:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. use 'nslookup ' to make sure there is a valid IP address associated with the hostname you have specified <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. make sure your EXTERNAL_HOSTNAME and associated IP address are listed in your /etc/hosts file.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03. make sure the IP address is associated with the system where you have deployed the ks-core Kind cluster</p> <p>if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-kind/#3-deploy-an-apache-web-server-to-ks-edge-cluster1-and-ks-edge-cluster2","title":"3. Deploy an Apache Web Server to ks-edge-cluster1 and ks-edge-cluster2","text":"deploy <p>KubeStellar's helm chart automatically creates a Workload Management  Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>In the <code>wmw1</code> space create the following <code>EdgePlacement</code> object:   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly  <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre></p> <p>Check if your configmap and deployment were created correctly.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments,cm -n my-namespace\n</code></pre>"},{"location":"Getting-Started/user-quickstart-kind/#4-view-the-apache-web-server-running-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"4. View the Apache Web Server running on ks-edge-cluster1 and ks-edge-cluster2","text":"viewuh oh, error? <p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear):  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same:  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters:  For ks-edge-cluster1:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8094\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8096\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>If you are unable to see the namespace 'my-namespace' or the deployment 'my-first-kubestellar-deployment' you can view the logs for the KubeStellar syncer on the ks-edge-cluster1 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster1\nks_ns_edge_cluster1=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster1 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster1\n</code></pre> <p>and on the ks-edge-cluster2 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster2\nks_ns_edge_cluster2=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster2 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster2\n</code></pre> <p>If you see a <code>connection refused</code> error in either KubeStellar Syncer log(s):</p> <p><code>E1021 21:22:58.000110       1 reflector.go:138] k8s.io/client-go@v0.0.0-20230210192259-aaa28aa88b2d/tools/cache/reflector.go:215: Failed to watch *v2alpha1.EdgeSyncConfig: failed to list *v2alpha1.EdgeSyncConfig: Get \"https://kubestellar.core:1119/apis/edge.kubestellar.io/v2alpha1/edgesyncconfigs?limit=500&amp;resourceVersion=0\": dial tcp 127.0.0.1:1119: connect: connection refused</code></p> <p>it means that your <code>/etc/hosts</code> does not have a proper IP address (NOT <code>127.0.0.1</code>) listed for the <code>kubestellar.core</code> hostname. Once there is a valid address in <code>/etc/hosts</code> for <code>kubestellar.core</code>, the syncer will begin to work properly and pull the namespace, deployment, and configmap from this instruction set. </p> <p>Mac OS users may also experience issues when <code>stealth mode</code> (system settings/firewall).  If you decide to disable this mode temporarily, please be sure to re-enable it once you are finished with this guide.</p>"},{"location":"Getting-Started/user-quickstart-kind/#5-check-the-status-of-your-apache-server-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"5. Check the status of your Apache Server on ks-edge-cluster1 and ks-edge-cluster2","text":"<p>Every object subject to downsync or upsync has a full per-WEC copy in the core. These include reported state from the WECs. If you are using release 0.10 or later of KubeStellar then you can list these copies of your httpd <code>Deployment</code> objects with the following command.</p> <pre><code>kubestellar-list-syncing-objects --api-group apps --api-kind Deployment\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  availableReplicas: 1\nconditions:\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\ntype: Available\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: ReplicaSet \"my-first-kubestellar-deployment-76f6fc4cfc\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\ntype: Progressing\n  observedGeneration: 618\nreadyReplicas: 1\nreplicas: 1\nupdatedReplicas: 1\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  ... (another happy status) ...\n</code></pre>"},{"location":"Getting-Started/user-quickstart-kind/#_2","title":"Quickstart using Kind","text":""},{"location":"Getting-Started/user-quickstart-kind/#tear-it-all-down","title":"Tear it all down","text":"uninstall brew, delete kind clusters, delete kubernetes contexts <p>to remove what you just installed:</p> <pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre> <p> <pre><code>kind delete cluster --name ks-core\nkind delete cluster --name ks-edge-cluster1\nkind delete cluster --name ks-edge-cluster2\n</code></pre> </p> <p>important: delete any existing kubernetes contexts of the clusters you may have created previously <pre><code>KUBECONFIG=~/.kube/config kubectl config delete-context ks-core || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster1 || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster2 || true\n</code></pre></p>"},{"location":"Getting-Started/user-quickstart-openshift/","title":"Quickstart using OpenShift","text":""},{"location":"Getting-Started/user-quickstart-openshift/#how-to-deploy-and-use-kubestellar-on-red-hat-openshift-kubernetes-clusters","title":"How to deploy and use KUBESTELLAR on Red Hat OpenShift Kubernetes Clusters","text":"Goals <p>This guide will show how to:</p> <ol> <li>quickly deploy the KubeStellar Core component on an OpenShift cluster using helm (ks-core), </li> <li>install the KubeStellar user commands and kubectl plugins on your computer with brew,</li> <li>retrieve the KubeStellar Core component kubeconfig, </li> <li>install the KubeStellar Syncer component on two edge OpenShift clusters (ks-edge-cluster1 and ks-edge-cluster2), </li> <li>deploy an example kubernetes workload to both edge OpenShift clusters from KubeStellar Core (ks-core),</li> <li>view the example kubernetes workload running on two edge OpenShift clusters (ks-edge-cluster1 and ks-edge-cluster2)</li> <li>view the status of your deployment across both edge OpenShift clusters from KubeStellar Core (ks-core)</li> </ol> <p>important: For this quickstart you will need to know how to use kubernetes' kubeconfig context to access multiple clusters.  You can learn more about kubeconfig context here</p> Pre-reqs <ul> <li> <p>kubectl (version range expected: 1.24-1.26)</p> </li> <li> <p>helm - to deploy the KubeStellar-core helm chart</p> </li> <li> <p>brew - to install the KubeStellar user commands and kubectl plugins</p> </li> <li> <p>3 Red Hat OpenShift clusters - we will refer to them as ks-core, ks-edge-cluster1, and ks-edge-cluster2 in this document</p> </li> </ul> <p>important: delete any existing kubernetes contexts of the clusters you may have created previously <pre><code>KUBECONFIG=~/.kube/config kubectl config delete-context ks-core || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster1 || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster2 || true\n</code></pre></p> <p> </p> <p>important: alias the kubernetes contexts of the OpenShift clusters you provided to match their use in this guide <pre><code>oc login &lt;ks-core OpenShift cluster&gt;\nCURRENT_CONTEXT=$(KUBECONFIG=~/.kube/config kubectl config current-context) \\\n    &amp;&amp; KUBECONFIG=~/.kube/config kubectl config set-context ks-core \\\n    --namespace=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $1}') \\\n    --cluster=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $2}') \\\n    --user=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $3\"/\"$2}')\n\noc login &lt;ks-edge-cluster1 OpenShift cluster&gt;\nCURRENT_CONTEXT=$(KUBECONFIG=~/.kube/config kubectl config current-context) \\\n    &amp;&amp; KUBECONFIG=~/.kube/config kubectl config set-context ks-edge-cluster1 \\\n    --namespace=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $1}') \\\n    --cluster=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $2}') \\\n    --user=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $3\"/\"$2}')\n\noc login &lt;ks-edge-cluster2 OpenShift cluster&gt;\nCURRENT_CONTEXT=$(KUBECONFIG=~/.kube/config kubectl config current-context) \\\n    &amp;&amp; KUBECONFIG=~/.kube/config kubectl config set-context ks-edge-cluster2 \\\n    --namespace=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $1}') \\\n    --cluster=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $2}') \\\n    --user=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $3\"/\"$2}')\n</code></pre></p>"},{"location":"Getting-Started/user-quickstart-openshift/#1-deploy-the-kubestellar-core-component","title":"1. Deploy the KUBESTELLAR Core component","text":"deployuh oh, error?open a bug report <p>deploy the KubeStellar Core components on the ks-core OpenShift cluster  </p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install kubestellar/kubestellar-core \\\n--set clusterType=OpenShift \\\n--namespace kubestellar \\\n--generate-name\n</code></pre> <p>run the following to wait for KubeStellar to be ready to take requests: </p> <pre><code>echo -n 'Waiting for KubeStellar to be ready'\nwhile ! KUBECONFIG=~/.kube/config kubectl exec $(KUBECONFIG=~/.kube/config kubectl get pod \\\n--selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n-n kubestellar -c init -- ls /home/kubestellar/ready &amp;&gt; /dev/null; do\nsleep 10\necho -n \".\"\ndone\necho; echo; echo \"KubeStellar is now ready to take requests\"\n</code></pre> <p>Checking the initialization log to see if there are any obvious errors:  <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  \nkubectl logs \\\n  $(kubectl get pod --selector=app=kubestellar \\\n  -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n  -n kubestellar -c init\n</code></pre>  if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-openshift/#2-install-kubestellars-user-commands-and-kubectl-plugins","title":"2. Install KubeStellar's user commands and kubectl plugins","text":"installremoveuh oh, no brew? <pre><code>if ! command -v brew; then\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/runner/.bashrc\n    eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nmore /etc/hosts\n    # sudo echo $(curl https://api.ipify.org) kubestellar.core | sudo tee -a /etc/host\nfi\nbrew tap kubestellar/kubestellar\nbrew update\nbrew install kubestellar-cli\n</code></pre> <pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) \\\n    --kubestellar-version latest --deploy false\n\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre>"},{"location":"Getting-Started/user-quickstart-openshift/#3-view-your-kubestellar-core-space-environment","title":"3. View your KUBESTELLAR Core Space environment","text":"show all available KubeStellar Core Spacesuh oh, error?open a bug report <p>Let's store the KubeStellar kubeconfig to a file we can reference later and then check out the Spaces KubeStellar created during installation  </p> <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-core get spaces -A </code></pre> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-openshift/#4-install-kubestellar-syncers-on-your-edge-clusters","title":"4. Install KUBESTELLAR Syncers on your Edge Clusters","text":"Prep and apply <p>prepare KubeStellar Syncers, with <code>kubestellar prep-for-cluster</code>, for ks-edge-cluster1 and ks-edge-cluster2 and then apply the files that <code>kubestellar prep-for-cluster</code> prepared for you</p> <pre><code>KUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster1 \\\nenv=ks-edge-cluster1 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\nKUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster2 \\\nenv=ks-edge-cluster2 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\n</code></pre> <pre><code>#apply ks-edge-cluster1 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 apply -f ks-edge-cluster1-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster1 correctly\n#apply ks-edge-cluster2 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 apply -f ks-edge-cluster2-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster2 correctly\n</code></pre>"},{"location":"Getting-Started/user-quickstart-openshift/#5-deploy-an-apache-web-server-to-ks-edge-cluster1-and-ks-edge-cluster2","title":"5. Deploy an Apache Web Server to ks-edge-cluster1 and ks-edge-cluster2","text":"deploy <p>KubeStellar's helm chart automatically creates a Workload Management  Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>This EdgePlacement includes downsync of a <code>RoleBinding</code> that grants  privileges that let the httpd pod run in an OpenShift cluster as well  as applying the OpenShift route CRD so support exposure of services.</p> <p>In the <code>root:wmw1</code> workspace create the following <code>EdgePlacement</code> object:   <pre><code>KUBECONFIG=ks-core.kubeconfig \\\nkubectl apply -f https://raw.githubusercontent.com/openshift/router/master/deploy/route_crd.yaml\n\nKUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\n\nKUBECONFIG=ks-core.kubeconfig kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps, services ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: route.openshift.io\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n    resources: [ routes ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  - apiGroup: rbac.authorization.k8s.io\n    resources: [ rolebindings ]\n    namespaces: [ my-namespace ]\n    objectNames: [ let-it-be ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly  <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\nKUBECONFIG=ks-core.kubeconfig kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.   <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: registry.redhat.io/rhel8/httpd-24\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /var/www/html/\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: let-it-be\n  namespace: my-namespace\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:openshift:scc:privileged\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: my-namespace\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: my-namespace\nspec:\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: common\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: my-route\n  namespace: my-namespace\nspec:\n  port:\n    targetPort: 8080\n  to:\n    kind: Service\n    name: my-service\nEOF\n</code></pre></p> <p>check if your configmap, deployment, service, and route was applied to the ks-core <code>my-namespace</code> namespace correctly  <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\nKUBECONFIG=ks-core.kubeconfig kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=ks-core.kubeconfig kubectl get deployments,cm,service,route -n my-namespace\n</code></pre></p>"},{"location":"Getting-Started/user-quickstart-openshift/#6-view-the-apache-web-server-running-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"6. View the Apache Web Server running on ks-edge-cluster1 and ks-edge-cluster2","text":"viewuh oh, error? <p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear):  <pre><code>KUBECONFIG=~/.kube/config kubectl --context \\\nks-edge-cluster1 get deployments -A | grep my-namespace\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same:  <pre><code>KUBECONFIG=~/.kube/config kubectl --context \\\nks-edge-cluster2 get deployments -A | grep my-namespace\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters:  For ks-edge-cluster1:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 \\\nget route/my-route -n my-namespace -o jsonpath='{.spec.host}')\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 \\\nget route/my-route -n my-namespace -o jsonpath='{.spec.host}')\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>If you are unable to see the namespace 'my-namespace' or the deployment 'my-first-kubestellar-deployment' you can view the logs for the KubeStellar syncer on the ks-edge-cluster1 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster1\nks_ns_edge_cluster1=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster1 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster1\n</code></pre> <p>and on the ks-edge-cluster2 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster2\nks_ns_edge_cluster2=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster2 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster2\n</code></pre>"},{"location":"Getting-Started/user-quickstart-openshift/#7-check-the-status-of-your-apache-server-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"7. Check the status of your Apache Server on ks-edge-cluster1 and ks-edge-cluster2","text":"<pre><code>TODO\n</code></pre> <p>what's next... how to upsync a resource how to create, but not overwrite/update a synchronized resource  </p>"},{"location":"Getting-Started/user-quickstart-openshift/#_1","title":"Quickstart using OpenShift","text":""},{"location":"Getting-Started/user-quickstart-openshift/#how-to-use-an-existing-kubestellar-environment","title":"How to use an existing KUBESTELLAR environment","text":""},{"location":"Getting-Started/user-quickstart-openshift/#1-install-kubestellars-user-commands-and-kubectl-plugins","title":"1. Install KubeStellar's user commands and kubectl plugins","text":"installremoveuh oh, no brew? <pre><code>if ! command -v brew; then\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/runner/.bashrc\n    eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nmore /etc/hosts\n    # sudo echo $(curl https://api.ipify.org) kubestellar.core | sudo tee -a /etc/host\nfi\nbrew tap kubestellar/kubestellar\nbrew update\nbrew install kubestellar-cli\n</code></pre> <pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) \\\n    --kubestellar-version latest --deploy false\n\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre>"},{"location":"Getting-Started/user-quickstart-openshift/#2-view-your-kubestellar-core-space-environment","title":"2. View your KUBESTELLAR Core Space environment","text":"show all available KubeStellar Core Spacesopen a bug report <p>Let's store the KubeStellar kubeconfig to a file we can reference later and then check out the Spaces KubeStellar created during installation</p> <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-core get spaces -A \n</code></pre> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-openshift/#3-deploy-an-apache-web-server-to-ks-edge-cluster1-and-ks-edge-cluster2","title":"3. Deploy an Apache Web Server to ks-edge-cluster1 and ks-edge-cluster2","text":"deploy <p>KubeStellar's helm chart automatically creates a Workload Management  Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>This EdgePlacement includes downsync of a <code>RoleBinding</code> that grants  privileges that let the httpd pod run in an OpenShift cluster as well  as applying the OpenShift route CRD so support exposure of services.</p> <p>In the <code>root:wmw1</code> workspace create the following <code>EdgePlacement</code> object:   <pre><code>KUBECONFIG=ks-core.kubeconfig \\\nkubectl apply -f https://raw.githubusercontent.com/openshift/router/master/deploy/route_crd.yaml\n\nKUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\n\nKUBECONFIG=ks-core.kubeconfig kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps, services ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: route.openshift.io\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n    resources: [ routes ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  - apiGroup: rbac.authorization.k8s.io\n    resources: [ rolebindings ]\n    namespaces: [ my-namespace ]\n    objectNames: [ let-it-be ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly  <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\nKUBECONFIG=ks-core.kubeconfig kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.   <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: registry.redhat.io/rhel8/httpd-24\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /var/www/html/\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: let-it-be\n  namespace: my-namespace\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:openshift:scc:privileged\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: my-namespace\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: my-namespace\nspec:\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: common\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: my-route\n  namespace: my-namespace\nspec:\n  port:\n    targetPort: 8080\n  to:\n    kind: Service\n    name: my-service\nEOF\n</code></pre></p> <p>check if your configmap, deployment, service, and route was applied to the ks-core <code>my-namespace</code> namespace correctly  <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\nKUBECONFIG=ks-core.kubeconfig kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=ks-core.kubeconfig kubectl get deployments,cm,service,route -n my-namespace\n</code></pre></p>"},{"location":"Getting-Started/user-quickstart-openshift/#4-view-the-apache-web-server-running-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"4. View the Apache Web Server running on ks-edge-cluster1 and ks-edge-cluster2","text":"viewuh oh, error? <p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear):  <pre><code>KUBECONFIG=~/.kube/config kubectl --context \\\nks-edge-cluster1 get deployments -A | grep my-namespace\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same:  <pre><code>KUBECONFIG=~/.kube/config kubectl --context \\\nks-edge-cluster2 get deployments -A | grep my-namespace\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters:  For ks-edge-cluster1:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 \\\nget route/my-route -n my-namespace -o jsonpath='{.spec.host}')\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 \\\nget route/my-route -n my-namespace -o jsonpath='{.spec.host}')\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>If you are unable to see the namespace 'my-namespace' or the deployment 'my-first-kubestellar-deployment' you can view the logs for the KubeStellar syncer on the ks-edge-cluster1 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster1\nks_ns_edge_cluster1=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster1 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster1\n</code></pre> <p>and on the ks-edge-cluster2 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster2\nks_ns_edge_cluster2=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster2 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster2\n</code></pre>"},{"location":"Getting-Started/user-quickstart-openshift/#5-check-the-status-of-your-apache-server-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"5. Check the status of your Apache Server on ks-edge-cluster1 and ks-edge-cluster2","text":"<p>Every object subject to downsync or upsync has a full per-WEC copy in the core. These include reported state from the WECs. If you are using release 0.10 or later of KubeStellar then you can list these copies of your httpd <code>Deployment</code> objects with the following command.</p> <pre><code>kubestellar-list-syncing-objects --api-group apps --api-kind Deployment\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  availableReplicas: 1\nconditions:\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\ntype: Available\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: ReplicaSet \"my-first-kubestellar-deployment-76f6fc4cfc\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\ntype: Progressing\n  observedGeneration: 618\nreadyReplicas: 1\nreplicas: 1\nupdatedReplicas: 1\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  ... (another happy status) ...\n</code></pre>"},{"location":"Getting-Started/user-quickstart-test/","title":"Quickstart for testing","text":""},{"location":"Getting-Started/user-quickstart-test/#how-to-deploy-and-use-kubestellar-on-kind-kubernetes-clusters","title":"How to deploy and use KUBESTELLAR on Kind Kubernetes Clusters","text":"Goals <p>This guide will show how to:</p> <ol> <li>quickly deploy the KubeStellar Core component on a Kind cluster using helm (ks-core), </li> <li>install the KubeStellar user commands and kubectl plugins on your computer with brew,</li> <li>retrieve the KubeStellar Core component kubeconfig, </li> <li>install the KubeStellar Syncer component on two edge Kind clusters (ks-edge-cluster1 and ks-edge-cluster2), </li> <li>deploy an example kubernetes workload to both edge Kind clusters from KubeStellar Core (ks-core),</li> <li>view the example kubernetes workload running on two edge Kind clusters (ks-edge-cluster1 and ks-edge-cluster2)</li> <li>view the status of your deployment across both edge Kind clusters from KubeStellar Core (ks-core)</li> </ol> <p>important: For this quickstart you will need to know how to use kubernetes' kubeconfig context to access multiple clusters.  You can learn more about kubeconfig context here</p> Pre-reqsuh oh, error?Special notes for Debian users <ul> <li> <p>kubectl (version range expected: 1.24-1.26)</p> </li> <li> <p>helm - to deploy the KubeStellar-core helm chart</p> </li> <li> <p>brew - to install the KubeStellar user commands and kubectl plugins</p> </li> <li> <p>Kind - to create a few small kubernetes clusters</p> </li> <li> <p>3 Kind clusters configured as follows</p> </li> </ul> <p>Build the core images - both kubestellar and space-framework.</p> <pre><code>pwd\nexport EXTRA_CORE_TAG=$(date +test%m%d-%H%M%S)\nmake kubestellar-image-local\ncd space-framework\nmake spacecore-image-local\ncd ..\n</code></pre> <p>Build the syncer image.</p> <pre><code>export SYNCER_IMG_REF=$(\nif (docker info | grep podman) &amp;&gt; /dev/null\n  then export DOCKER_HOST=unix://${HOME}/.local/share/containers/podman/machine/qemu/podman.sock\n  fi\nKO_DOCKER_REPO=\"\" make build-kubestellar-syncer-image-local | grep -v \"make.*directory\" )\necho \"Locally built syncer image reference is &lt;$SYNCER_IMG_REF&gt;; adding alias ko.local/syncer:test\"\ndocker tag \"$SYNCER_IMG_REF\" ko.local/syncer:test\n</code></pre> <p>create the ks-core kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-core --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 1119\n    protocol: TCP\nEOF\n</code></pre></p> <p>Be sure to create an ingress control with SSL passthrough to ks-core. This is a special requirement for Kind that allows access to the KubeStellar core running on ks-core. <pre><code>KUBECONFIG=~/.kube/config kubectl \\\ncreate -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\n</code></pre> Wait about 20 seconds and then check if the ingress control is ready on ks-core: <pre><code>sleep 20\nKUBECONFIG=~/.kube/config kubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=90s\n</code></pre></p> <p>Install the core container image in the ks-core <code>kind</code> cluster.</p> <pre><code>kind load docker-image quay.io/kubestellar/kubestellar:$EXTRA_CORE_TAG --name ks-core\nkind load docker-image quay.io/kubestellar/space-framework:$EXTRA_CORE_TAG --name ks-core\n</code></pre> <p>create the ks-edge-cluster1 kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-edge-cluster1 --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre></p> <p>create the ks-edge-cluster2 kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-edge-cluster2 --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre></p> <p>Install the syncer container image in the two WECs.</p> <pre><code>kind load docker-image ko.local/syncer:test --name ks-edge-cluster1\nkind load docker-image ko.local/syncer:test --name ks-edge-cluster2\n</code></pre> <p>important: delete any existing kubernetes contexts of the clusters you may have created previously <pre><code>KUBECONFIG=~/.kube/config kubectl config delete-context ks-core || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster1 || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster2 || true\n</code></pre></p> <p> </p> <p>important: rename the kubernetes contexts of the Kind clusters to match their use in this guide <pre><code>KUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-core ks-core\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-edge-cluster1 ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-edge-cluster2 ks-edge-cluster2\n</code></pre></p> <p>if you apply the ingress and then receive an error while waiting:   <code>error: no matching resources found</code></p> <p>this might mean that you did not wait long enough before issuing the check command. Simply try the check command again.</p> <p>on Debian, the syncers on ks-edge-cluster1 and ks-edge-cluster2 will not resolve the kubestellar.core hostname </p> <p>You have 2 choices:  </p> <ol> <li> <p>Use the value of <code>hostname -f</code> instead of kubestellar.core as your \"EXTERNAL_HOSTNAME\" in \"Step 1:  Deploy the KubeStellar Core Component\", or</p> </li> <li> <p>Just before step 6 in the KubeStellar User Quickstart for Kind do the following</p> </li> </ol> <p>Add IP/domain to /etc/hosts of cluster1/cluster2 containers (replace with appropriate IP address):</p> <pre><code>docker exec -it $(docker ps | grep ks-edge-cluster1 | cut -d \" \" -f 1) \\\n    sh -c \"echo '192.168.122.144 kubestellar.core' &gt;&gt; /etc/hosts\"\ndocker exec -it $(docker ps | grep ks-edge-cluster2 | cut -d \" \" -f 1) \\\n    sh -c \"echo '192.168.122.144 kubestellar.core' &gt;&gt; /etc/hosts\"\n</code></pre> <p>Edit coredns ConfigMap for cluster1 and cluster1 (see added lines in example):</p> <pre><code>KUBECONFIG=~/.kube/config kubectl edit cm coredns -n kube-system --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl edit cm coredns -n kube-system --context=ks-edge-cluster2\n</code></pre> <p>add the highlighted information <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n           lameduck 5s\n        }\n        hosts /etc/coredns/customdomains.db core {  \n          fallthrough                               \n        }                                          \n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n  customdomains.db: |                   \n    192.168.122.144 kubestellar.core  \nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2023-10-24T19:18:05Z\"\n  name: coredns\n  namespace: kube-system\n  resourceVersion: \"10602\"\n  uid: 3930c18f-23e8-4d0b-9ddf-658fdf3cb20f\n</code></pre></p> <p>Edit Deployment for coredns on cluster1 and cluster2, adding the key/path at the given location: <pre><code>KUBECONFIG=~/.kube/config kubectl edit -n kube-system \\\n    deployment coredns --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl edit -n kube-system \\\n    deployment coredns --context=ks-edge-cluster2\n</code></pre></p> <pre><code>spec:\n  template:\n    spec:\n      volumes:\n      - configMap:\n          items:\n          - key: customdomains.db\n            path: customdomains.db\n</code></pre> <p>Restart coredns pods: <pre><code>KUBECONFIG=~/.kube/config kubectl rollout restart \\\n    -n kube-system deployment/coredns --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl rollout restart \\\n    -n kube-system deployment/coredns --context=ks-edge-cluster2\n</code></pre></p> <p>(adapted from \"The Cluster-wise solution\" at https://stackoverflow.com/questions/37166822/is-there-a-way-to-add-arbitrary-records-to-kube-dns)</p>"},{"location":"Getting-Started/user-quickstart-test/#1-deploy-the-kubestellar-core-component","title":"1. Deploy the KUBESTELLAR Core component","text":"deployuh oh, error?open a bug report <p>deploy the KubeStellar Core components on the ks-core Kind cluster you created in the pre-req section above  </p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install ./core-helm-chart \\\n--set EXTERNAL_HOSTNAME=\"kubestellar.core\" \\\n--set EXTERNAL_PORT=1119 \\\n--set CONTROLLER_VERBOSITY=4 \\\n--set image.tag=$EXTRA_CORE_TAG \\\n--set spaceimage.tag=$EXTRA_CORE_TAG \\\n--namespace kubestellar \\\n--generate-name\n</code></pre> <p>important: You must add 'kubestellar.core' to your /etc/hosts file with the local network IP address (e.g., 192.168.x.y) where your ks-core Kind cluster is running. DO NOT use <code>127.0.0.1</code> because the ks-edge-cluster1 and ks-edge-cluster2 kind clusters map <code>127.0.0.1</code> to their local kubernetes cluster, not the ks-core kind cluster.</p> <p>run the following to wait for KubeStellar to be ready to take requests: </p> <pre><code>echo -n 'Waiting for KubeStellar to be ready'\nwhile ! KUBECONFIG=~/.kube/config kubectl exec $(KUBECONFIG=~/.kube/config kubectl get pod \\\n--selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n-n kubestellar -c init -- ls /home/kubestellar/ready &amp;&gt; /dev/null; do\nsleep 10\necho -n \".\"\ndone\necho; echo; echo \"KubeStellar is now ready to take requests\"\n</code></pre> <p>Checking the initialization log to see if there are any obvious errors:  <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  \nkubectl logs \\\n  $(kubectl get pod --selector=app=kubestellar \\\n  -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n  -n kubestellar -c init\n</code></pre>  if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-test/#2-install-kubestellars-user-commands-and-kubectl-plugins","title":"2. Install KUBESTELLAR's user commands and kubectl plugins","text":"<pre><code>pwd\nrm -f bin/*\nmake userbuild\nexport PATH=$PWD/bin:$PATH\nbash -c \"$(cat bootstrap/install-kcp-with-plugins.sh)\" -V -V --version v0.11.0\nexport PATH=$PWD/kcp/bin:$PATH\n</code></pre>"},{"location":"Getting-Started/user-quickstart-test/#3-view-your-kubestellar-core-space-environment","title":"3. View your KUBESTELLAR Core Space environment","text":"show all available KubeStellar Core Spacesuh oh, error?open a bug report <p>Let's store the KubeStellar kubeconfig to a file we can reference later and then check out the Spaces KubeStellar created during installation</p> <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-core get spaces -A </code></pre> <p>Did you received the following error:  <code>`Error: Get \"https://some_hostname.some_domain_name:1119/clusters/root/apis/tenancy.kcp.io/v1alpha1/workspaces\": dial tcp: lookup some_hostname.some_domain_name on x.x.x.x: no such host</code></p> <p>A common error occurs if you set your port number to a pre-occupied port number and/or you set your EXTERNAL_HOSTNAME to something other than \"localhost\" so that you can reach your KubeStellar Core from another host, check the following:</p> <p>Check if the port specified in the ks-core Kind cluster configuration and the EXTERNAL_PORT helm value are occupied by another application:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. is the `hostPort`` specified in the ks-core Kind cluster configuration is occupied by another process?  If so, delete the ks-core Kind cluster and create it again using an available port for your 'hostPort' value</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. if you change the port for your ks-core 'hostPort', remember to also use that port as the helm 'EXTERNAL_PORT' value</p> <p>Check that your EXTERNAL_HOSTNAME helm value is reachable via DNS:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. use 'nslookup ' to make sure there is a valid IP address associated with the hostname you have specified <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. make sure your EXTERNAL_HOSTNAME and associated IP address are listed in your /etc/hosts file.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03. make sure the IP address is associated with the system where you have deployed the ks-core Kind cluster</p> <p>if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-test/#4-install-kubestellar-syncers-on-your-edge-clusters","title":"4. Install KUBESTELLAR Syncers on your Edge Clusters","text":"Prep and apply <p>prepare KubeStellar Syncers, with <code>kubestellar prep-for-cluster</code>, for ks-edge-cluster1 and ks-edge-cluster2 and then apply the files that <code>kubestellar prep-for-cluster</code> prepared for you</p> <p>important: make sure you created Kind clusters for ks-edge-cluster1 and ks-edge-cluster2 from the pre-req step above before proceeding how-to-deploy-and-use-kubestellar</p> <pre><code>KUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster1 \\\n--syncer-image ko.local/syncer:test \\\nenv=ks-edge-cluster1 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\nKUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster2 \\\n--syncer-image ko.local/syncer:test \\\nenv=ks-edge-cluster2 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\n</code></pre> <pre><code>#apply ks-edge-cluster1 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 apply -f ks-edge-cluster1-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster1 correctly\n#apply ks-edge-cluster2 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 apply -f ks-edge-cluster2-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster2 correctly\n</code></pre> <p>This example (and others) involve user-managed kubeconfig files. These examples organize those files into one subdirectory. Start by making sure that directory exists without any contents left over from previous runs.</p> <pre><code>MY_KUBECONFIGS=${PWD}/my-kubeconfigs\nrm -rf \"$MY_KUBECONFIGS\"\nmkdir -p \"$MY_KUBECONFIGS\"\n</code></pre> <p>Wait for the mailbox controller to create the corresponding mailbox spaces and remember them.</p> <pre><code>while [ $(KUBECONFIG=~/.kube/config kubectl get spaces -A | grep -c -e -mb-) -lt 2 ]; do sleep 10; done\nMB1=$(KUBECONFIG=~/.kube/config kubectl get spaces -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"ks-edge-cluster1\") | .name')\necho The mailbox for ks-edge-cluster1 is $MB1\nMB2=$(KUBECONFIG=~/.kube/config kubectl get spaces -n spaceprovider-default -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"ks-edge-cluster2\") | .name')\necho The mailbox for ks-edge-cluster2 is $MB2\nMB1_KUBECONFIG=\"${MY_KUBECONFIGS}/${MB1}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $MB1 --kubeconfig ~/.kube/config $MB1_KUBECONFIG\nMB2_KUBECONFIG=\"${MY_KUBECONFIGS}/${MB2}.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig $MB2 --kubeconfig ~/.kube/config $MB2_KUBECONFIG\nWMW1_KUBECONFIG=\"${MY_KUBECONFIGS}/wmw1.kubeconfig\"\nkubectl-kubestellar-space-get_kubeconfig wmw1 --kubeconfig ~/.kube/config $WMW1_KUBECONFIG\n</code></pre>"},{"location":"Getting-Started/user-quickstart-test/#5-deploy-an-apache-web-server-to-ks-edge-cluster1-and-ks-edge-cluster2","title":"5. Deploy an Apache Web Server to ks-edge-cluster1 and ks-edge-cluster2","text":"deploy <p>KubeStellar's helm chart automatically creates a Workload Management  Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>In the <code>wmw1</code> space create the following <code>EdgePlacement</code> object:   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly  <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre></p> <p>Check if your configmap and deployment were created correctly.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments,cm -n my-namespace\n</code></pre> <p>Add a ServiceAccount that will be downsynced.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  annotations:\n    edge.kubestellar.io/downsync-overwrite: \"false\"\n  namespace: my-namespace\n  name: test-sa\nEOF\n</code></pre> <p>Add an EdgePlacement that calls for that ServiceAccount to be downsynced.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: sa-test\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ serviceaccounts ]\n    namespaces: [ my-namespace ]\n    objectNames: [ test-sa ]\nEOF\n</code></pre> <p>Wait for the ServiceAccount to get to the mailbox spaces.</p> <pre><code>while ! KUBECONFIG=$MB1_KUBECONFIG kubectl get ServiceAccount -n my-namespace test-sa ; do\nsleep 10\ndone\nwhile ! KUBECONFIG=$MB2_KUBECONFIG kubectl get ServiceAccount -n my-namespace test-sa ; do\nsleep 10\ndone\n</code></pre> <p>Thrash the ServiceAccount some in its WDS.</p> <pre><code>for key in k1 k2 k3 k4; do\nsleep 15\nKUBECONFIG=$WMW1_KUBECONFIG kubectl annotate sa -n my-namespace test-sa ${key}=${key}\ndone\n</code></pre> <p>Give the controllers some time to fight over ServiceAccount secrets.</p> <pre><code>sleep 120\n</code></pre> <p>Look for excess secrets in the WDS. Expect 2 token Secrets: one for the default ServiceAccount and one for <code>test-sa</code>.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get secrets -n my-namespace\n[ $(KUBECONFIG=$WMW1_KUBECONFIG kubectl get Secret -n my-namespace -o jsonpath='{.items[?(@.type==\"kubernetes.io/service-account-token\")]}' | jq length | wc -l) -lt 3 ]\n</code></pre> <p>Look for excess secrets in the two mailbox spaces. Allow up to three: one for the <code>default</code> ServiceAccount, one dragged down from the WDS for the <code>test-sa</code> ServiceAccount, and one generated locally for the <code>test-sa</code> ServiceAccount.</p> <pre><code>for mb in $MB1_KUBECONFIG $MB2_KUBECONFIG; do\nKUBECONFIG=$mb kubectl get sa -n my-namespace test-sa --show-managed-fields -o yaml\n    KUBECONFIG=$mb kubectl get secrets -n my-namespace\n    [ $(KUBECONFIG=$mb kubectl get Secret -n my-namespace -o jsonpath='{.items[?(@.type==\"kubernetes.io/service-account-token\")]}' | jq length | wc -l) -lt 4 ]\ndone\n</code></pre>"},{"location":"Getting-Started/user-quickstart-test/#6-view-the-apache-web-server-running-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"6. View the Apache Web Server running on ks-edge-cluster1 and ks-edge-cluster2","text":"viewuh oh, error? <p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear):  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same:  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters:  For ks-edge-cluster1:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8094\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8096\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>If you are unable to see the namespace 'my-namespace' or the deployment 'my-first-kubestellar-deployment' you can view the logs for the KubeStellar syncer on the ks-edge-cluster1 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster1\nks_ns_edge_cluster1=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster1 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster1\n</code></pre> <p>and on the ks-edge-cluster2 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster2\nks_ns_edge_cluster2=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster2 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster2\n</code></pre> <p>If you see a <code>connection refused</code> error in either KubeStellar Syncer log(s):</p> <p><code>E1021 21:22:58.000110       1 reflector.go:138] k8s.io/client-go@v0.0.0-20230210192259-aaa28aa88b2d/tools/cache/reflector.go:215: Failed to watch *v2alpha1.EdgeSyncConfig: failed to list *v2alpha1.EdgeSyncConfig: Get \"https://kubestellar.core:1119/apis/edge.kubestellar.io/v2alpha1/edgesyncconfigs?limit=500&amp;resourceVersion=0\": dial tcp 127.0.0.1:1119: connect: connection refused</code></p> <p>it means that your <code>/etc/hosts</code> does not have a proper IP address (NOT <code>127.0.0.1</code>) listed for the <code>kubestellar.core</code> hostname. Once there is a valid address in <code>/etc/hosts</code> for <code>kubestellar.core</code>, the syncer will begin to work properly and pull the namespace, deployment, and configmap from this instruction set. </p> <p>Mac OS users may also experience issues when <code>stealth mode</code> (system settings/firewall).  If you decide to disable this mode temporarily, please be sure to re-enable it once you are finished with this guide.</p>"},{"location":"Getting-Started/user-quickstart-test/#7-check-the-status-of-your-apache-server-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"7. Check the status of your Apache Server on ks-edge-cluster1 and ks-edge-cluster2","text":"<pre><code>TODO\n</code></pre> <p>what's next... how to upsync a resource how to create, but not overwrite/update a synchronized resource  </p>"},{"location":"Getting-Started/user-quickstart-test/#_1","title":"Quickstart for testing","text":""},{"location":"Getting-Started/user-quickstart-test/#how-to-use-an-existing-kubestellar-environment","title":"How to use an existing KUBESTELLAR environment","text":""},{"location":"Getting-Started/user-quickstart-test/#1-install-kubestellars-user-commands-and-kubectl-plugins","title":"1. Install KUBESTELLAR's user commands and kubectl plugins","text":"installremoveuh oh, no brew? <pre><code>if ! command -v brew; then\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/runner/.bashrc\n    eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nmore /etc/hosts\n    # sudo echo $(curl https://api.ipify.org) kubestellar.core | sudo tee -a /etc/host\nfi\nbrew tap kubestellar/kubestellar\nbrew update\nbrew install kubestellar-cli\n</code></pre> <pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) \\\n    --kubestellar-version latest --deploy false\n\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre>"},{"location":"Getting-Started/user-quickstart-test/#2-view-your-kubestellar-core-space-environment","title":"2. View your KUBESTELLAR Core Space environment","text":"show all available KubeStellar Core Spacesuh oh, error?open a bug report <p>Let's store the KubeStellar kubeconfig to a file we can reference later and then check out the Spaces KubeStellar created during installation</p> <pre><code>KUBECONFIG=~/.kube/config kubectl get spaces -A \n</code></pre> <p>Did you received the following error:  <code>`Error: Get \"https://some_hostname.some_domain_name:1119/clusters/root/apis/tenancy.kcp.io/v1alpha1/workspaces\": dial tcp: lookup some_hostname.some_domain_name on x.x.x.x: no such host</code></p> <p>A common error occurs if you set your port number to a pre-occupied port number and/or you set your EXTERNAL_HOSTNAME to something other than \"localhost\" so that you can reach your KubeStellar Core from another host, check the following:</p> <p>Check if the port specified in the ks-core Kind cluster configuration and the EXTERNAL_PORT helm value are occupied by another application:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. is the `hostPort`` specified in the ks-core Kind cluster configuration is occupied by another process?  If so, delete the ks-core Kind cluster and create it again using an available port for your 'hostPort' value</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. if you change the port for your ks-core 'hostPort', remember to also use that port as the helm 'EXTERNAL_PORT' value</p> <p>Check that your EXTERNAL_HOSTNAME helm value is reachable via DNS:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. use 'nslookup ' to make sure there is a valid IP address associated with the hostname you have specified <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. make sure your EXTERNAL_HOSTNAME and associated IP address are listed in your /etc/hosts file.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03. make sure the IP address is associated with the system where you have deployed the ks-core Kind cluster</p> <p>if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"Getting-Started/user-quickstart-test/#3-deploy-an-apache-web-server-to-ks-edge-cluster1-and-ks-edge-cluster2","title":"3. Deploy an Apache Web Server to ks-edge-cluster1 and ks-edge-cluster2","text":"deploy <p>KubeStellar's helm chart automatically creates a Workload Management  Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>In the <code>wmw1</code> space create the following <code>EdgePlacement</code> object:   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly  <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.   <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre></p> <p>Check if your configmap and deployment were created correctly.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments,cm -n my-namespace\n</code></pre>"},{"location":"Getting-Started/user-quickstart-test/#4-view-the-apache-web-server-running-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"4. View the Apache Web Server running on ks-edge-cluster1 and ks-edge-cluster2","text":"viewuh oh, error? <p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear):  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same:  <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get deployments -A\n</code></pre></p> <p>you should see output including:  <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters:  For ks-edge-cluster1:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8094\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2:  <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8096\n</code></pre></p> <p>you should see the output:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>If you are unable to see the namespace 'my-namespace' or the deployment 'my-first-kubestellar-deployment' you can view the logs for the KubeStellar syncer on the ks-edge-cluster1 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster1\nks_ns_edge_cluster1=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster1 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster1\n</code></pre> <p>and on the ks-edge-cluster2 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster2\nks_ns_edge_cluster2=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster2 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster2\n</code></pre> <p>If you see a <code>connection refused</code> error in either KubeStellar Syncer log(s):</p> <p><code>E1021 21:22:58.000110       1 reflector.go:138] k8s.io/client-go@v0.0.0-20230210192259-aaa28aa88b2d/tools/cache/reflector.go:215: Failed to watch *v2alpha1.EdgeSyncConfig: failed to list *v2alpha1.EdgeSyncConfig: Get \"https://kubestellar.core:1119/apis/edge.kubestellar.io/v2alpha1/edgesyncconfigs?limit=500&amp;resourceVersion=0\": dial tcp 127.0.0.1:1119: connect: connection refused</code></p> <p>it means that your <code>/etc/hosts</code> does not have a proper IP address (NOT <code>127.0.0.1</code>) listed for the <code>kubestellar.core</code> hostname. Once there is a valid address in <code>/etc/hosts</code> for <code>kubestellar.core</code>, the syncer will begin to work properly and pull the namespace, deployment, and configmap from this instruction set. </p> <p>Mac OS users may also experience issues when <code>stealth mode</code> (system settings/firewall).  If you decide to disable this mode temporarily, please be sure to re-enable it once you are finished with this guide.</p>"},{"location":"Getting-Started/user-quickstart-test/#5-check-the-status-of-your-apache-server-on-ks-edge-cluster1-and-ks-edge-cluster2","title":"5. Check the status of your Apache Server on ks-edge-cluster1 and ks-edge-cluster2","text":"<p>Every object subject to downsync or upsync has a full per-WEC copy in the core. These include reported state from the WECs. If you are using release 0.10 or later of KubeStellar then you can list these copies of your httpd <code>Deployment</code> objects with the following command.</p> <pre><code>kubestellar-list-syncing-objects --api-group apps --api-kind Deployment\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  availableReplicas: 1\nconditions:\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\ntype: Available\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: ReplicaSet \"my-first-kubestellar-deployment-76f6fc4cfc\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\ntype: Progressing\n  observedGeneration: 618\nreadyReplicas: 1\nreplicas: 1\nupdatedReplicas: 1\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  ... (another happy status) ...\n</code></pre>"},{"location":"Getting-Started/user-quickstart-test/#_2","title":"Quickstart for testing","text":""},{"location":"Getting-Started/user-quickstart-test/#tear-it-all-down","title":"Tear it all down","text":"<p>The following command deletes the <code>kind</code> clusters created above.</p> <pre><code>kind delete cluster --name ks-core; kind delete cluster --name ks-edge-cluster1; kind delete cluster --name ks-edge-cluster2\n</code></pre> <p>Or, you could get out the big footgun and delete all your <code>kind</code> clusters as follows.</p> <pre><code>for clu in $(kind get clusters | grep -v enabling); do kind delete cluster --name \"$clu\"; done\n</code></pre> <p>The following commands delete the filesystem contents created above.</p> <pre><code>rm ks-edge-cluster1-syncer.yaml ks-edge-cluster2-syncer.yaml\nrm -rf kcp my-kubeconfigs\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-0-demo/","title":"Quickstart 0 demo","text":""},{"location":"Getting-Started/quickstart-subs/quickstart-0-demo/#demo-video","title":"Demo Video","text":"<p>Watch this video to see a step-by-step demo of KubeStellar running and then follow the instructions below to get your own KubeStellar started quickly.</p> <p> </p>"},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/","title":"Quickstart 1 install and run kubestellar","text":"<p>We support two ways to deploy kcp and KubeStellar. The older way is to run them as bare processes. The newer way is to deploy them as workload in a Kubernetes (possibly OpenShift) cluster.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":"<p>The following commands will download the kcp and KubeStellar executables into subdirectories of your current working directory, deploy (i.e., start and configure) kcp and KubeStellar as bare processes, and configure your shell to use kcp and KubeStellar.  If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation about the commands for bare process deployment.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.15.0\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport KUBECONFIG=\"$(pwd)/.kcp/admin.kubeconfig\"\nexport SM_CONFIG=~/.kube/config\nmkdir -p ${PWD}/temp-space-config\n</code></pre> <p>Check that <code>KubeStellar</code> is running.</p> <p>First, check that controllers are running with the following command:</p> <pre><code>ps aux | grep -e mailbox-controller -e placement-translator -e kubestellar-where-resolver\n</code></pre> <p>which should yield something like:</p> <pre><code>user     1892  0.0  0.3 747644 29628 pts/1    Sl   10:51   0:00 mailbox-controller -v=2\nuser     1902  0.3  0.3 743652 27504 pts/1    Sl   10:51   0:02 kubestellar-where-resolver -v 2\nuser     1912  0.3  0.5 760428 41660 pts/1    Sl   10:51   0:02 placement-translator -v=2\n</code></pre> <p>Second, check that TMC compute service provider space and the KubeStellar Edge Service Provider space (<code>espw</code>) have been created with the following command:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <p>which should yield:</p> <pre><code>NAMESPACE               NAME   AGE\nspaceprovider-default   espw   1h\nspaceprovider-default   imw1   1h\nspaceprovider-default   wmw1   1h\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/#deploy-kcp-and-kubestellar-as-kubernetes-workload","title":"Deploy kcp and KubeStellar as Kubernetes workload","text":"<p>This requires a KubeStellar release GREATER THAN v0.5.0.</p> <p>This example uses a total of three <code>kind</code> clusters, which tends to run into a known issue with a known work-around, so take care of that.</p> <p>Before you can deploy kcp and KubeStellar as workload in a Kubernetes cluster, you need a Kubernetes cluster and it needs to have an Ingress controller installed.  We use the term \"hosting cluster\" for the cluster that plays this role.  In this quickstart, we make such a cluster with kind.  Follow the developer directions for making a hosting cluster with kind; you need not worry about loading a locally built container image into that cluster.</p> <p>This example uses the domain name \"hostname.favorite.my\" for the machine where you invoked <code>kind create cluster</code>. If you have not already done so then issue the following command, replacing <code>a_good_IP_address_for_this_machine</code> with an IPv4 address for your machine that can be reached from inside a container or VM (i.e., not 127.0.0.1).</p> <pre><code>sudo sh -c \"echo a_good_IP_address_for_this_machine hostname.favorite.my &gt;&gt; /etc/hosts\"\n</code></pre> <p>The next command relies on <code>kubectl</code> already being configured to manipulate the hosting cluster, which is indeed the state that <code>kind create cluster</code> leaves it in.</p> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory and (b) deploy (i.e., start and configure) kcp and KubeStellar as workload in the hosting cluster. If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation for the commands about deployment into a Kubernetes cluster.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.15.0 --external-endpoint hostname.favorite.my:1119\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport SM_CONFIG=~/.kube/config\nmkdir -p ${PWD}/temp-space-config\n</code></pre> <p>Using your original <code>kubectl</code> configuration that manipulates the hosting cluster, check that the KubeStellar Deployment has its intended one running Pod.</p> <pre><code>kubectl get deployments -n kubestellar\n</code></pre> <p>which should yield something like:</p> <pre><code>NAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-server   1/1     1            1           2m42s\n</code></pre> <p>It may take some time for that Pod to reach Running state.</p> <p>The bootstrap command above will print out instructions to set your KUBECONFIG environment variable to the pathname of a kubeconfig file that you can use as a user of kcp and KubeStellar.  Do that now, for the benefit of the remaining commands in this example.  It will look something like the following command.</p> <pre><code>export KUBECONFIG=\"$(pwd)/kubestellar.kubeconfig\"\n</code></pre> <p>Check that the TMC compute service provider spaces and the KubeStellar Edge Service Provider space (<code>espw</code>) have been created with the following command:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl get spaces -A\n</code></pre> <p>which should yield:</p> <pre><code>NAMESPACE               NAME   AGE\nspaceprovider-default   espw   1h\nspaceprovider-default   imw1   1h\nspaceprovider-default   wmw1   1h\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-a-kind-clusters-florin/","title":"Quickstart 2 apache example deployment a kind clusters florin","text":"<p>Create the first edge cluster:</p> <pre><code>kind create cluster --name florin --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'florin' from a previous exercise of KubeStellar, please delete the florin cluster ('kind delete cluster --name florin') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-a-kind-clusters-guilder/","title":"Quickstart 2 apache example deployment a kind clusters guilder","text":"<p>Create the second edge cluster:</p> <pre><code>kind create cluster --name guilder --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'guilder' from a previous exercise of KubeStellar, please delete the guilder cluster ('kind delete cluster --name guilder') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-c-onboarding-clusters/","title":"Quickstart 2 apache example deployment c onboarding clusters","text":"<p>KubeStellar will have created an Inventory Management space (IMW) for the user to put inventory objects in, describing the user's clusters. The IMW that is automatically created for the user is at <code>imw1</code>.</p> <p>Let's begin by onboarding the <code>florin</code> cluster:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl kubestellar prep-for-cluster --imw root:imw1 florin env=prod\n</code></pre> <p>which should yield something like:</p> <pre><code>synctarget.edge.kubestellar.io/florin created\nlocation.edge.kubestellar.io/florin created\nsynctarget.edge.kubestellar.io/florin labeled\nlocation.edge.kubestellar.io/florin labeled\nCreating service account \"kubestellar-syncer-florin-1yi5q9c4\"\nCreating cluster role \"kubestellar-syncer-florin-1yi5q9c4\" to give service account \"kubestellar-syncer-florin-1yi5q9c4\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-1yi5q9c4\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-1yi5q9c4\" to bind service account \"kubestellar-syncer-florin-1yi5q9c4\" to cluster role \"kubestellar-syncer-florin-1yi5q9c4\".\n\nWrote workload execution cluster (WEC) manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-1yi5q9c4\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-1yi5q9c4\" kubestellar-syncer-florin-1yi5q9c4\n\nto verify the syncer pod is running.\n</code></pre> <p>An edge syncer manifest yaml file was created in your current directory: <code>florin-syncer.yaml</code>. The default for the output file is the name of the SyncTarget object with \u201c-syncer.yaml\u201d appended.</p> <p>Now let's deploy the edge syncer to the <code>florin</code> edge cluster:</p> <pre><code>kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <p>which should yield something like:</p> <pre><code>namespace/kubestellar-syncer-florin-1yi5q9c4 created\nserviceaccount/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4 created\ndeployment.apps/kubestellar-syncer-florin-1yi5q9c4 created\n</code></pre> <p>Optionally, check that the edge syncer pod is running:</p> <pre><code>kubectl --context kind-florin get pods -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                                  READY   STATUS    RESTARTS   AGE\nkubestellar-syncer-florin-1yi5q9c4   kubestellar-syncer-florin-1yi5q9c4-77cb588c89-xc5qr   1/1     Running   0          12m\nkube-system                          coredns-565d847f94-92f4k                              1/1     Running   0          58m\nkube-system                          coredns-565d847f94-kgddm                              1/1     Running   0          58m\nkube-system                          etcd-florin-control-plane                             1/1     Running   0          58m\nkube-system                          kindnet-p8vkv                                         1/1     Running   0          58m\nkube-system                          kube-apiserver-florin-control-plane                   1/1     Running   0          58m\nkube-system                          kube-controller-manager-florin-control-plane          1/1     Running   0          58m\nkube-system                          kube-proxy-jmxsg                                      1/1     Running   0          58m\nkube-system                          kube-scheduler-florin-control-plane                   1/1     Running   0          58m\nlocal-path-storage                   local-path-provisioner-684f458cdd-kw2xz               1/1     Running   0          58m\n</code></pre> <p>Now, let's onboard the <code>guilder</code> cluster:</p> <pre><code>KUBECONFIG=$SM_CONFIG kubectl kubestellar prep-for-cluster --imw root:imw1 guilder env=prod extended=yes\n</code></pre> <p>Apply the created edge syncer manifest:</p> <pre><code>kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-d-create-and-deploy-apache-into-clusters/","title":"Quickstart 2 apache example deployment d create and deploy apache into clusters","text":"<p>KubeStellar will have automatically created a Workload Management space (WMW) for the user to store workload descriptions and KubeStellar Core control objects in. The automatically created WMW is at <code>wmw1</code>.</p> <p>Create the <code>EdgePlacement</code> object for your workload. Its \u201cwhere predicate\u201d (the locationSelectors array) has one label selector that matches the Location objects (<code>florin</code> and <code>guilder</code>) created earlier, thus directing the workload to both edge clusters. The <code>upsync</code> field is only a demonstration of the syntax, it plays no functional role in this scenario.</p> <p>In the <code>wmw1</code> space create the following <code>EdgePlacement</code> object: </p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {common: \"yes\"}\n    objectNames: [ commond ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <p>Put the prescription of the HTTP server workload into the WMW. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>edge-placement-c</code>) object created above. </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"yes\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Now, let's check that the deployment was created in the <code>florin</code> edge cluster - it may take a few 10s of seconds to appear:</p> <pre><code>kubectl --context kind-florin get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                          commond                              1/1     1            1           6m48s\nkubestellar-syncer-florin-2upj1awn   kubestellar-syncer-florin-2upj1awn   1/1     1            1           16m\nkube-system                          coredns                              2/2     2            2           28m\nlocal-path-storage                   local-path-provisioner               1/1     1            1           28m\n</code></pre> <p>Also, let's check that the deployment was created in the <code>guilder</code> edge cluster:</p> <pre><code>kubectl --context kind-guilder get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                           commond                               1/1     1            1           7m54s\nkubestellar-syncer-guilder-6tuay5d6   kubestellar-syncer-guilder-6tuay5d6   1/1     1            1           12m\nkube-system                           coredns                               2/2     2            2           27m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           27m\n</code></pre> <p>Lastly, let's check that the workload is working in both clusters:</p> <p>For <code>florin</code>:</p> <pre><code>while [[ $(kubectl --context kind-florin get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8094\n</code></pre> <p>which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>For <code>guilder</code>:</p> <p><pre><code>while [[ $(kubectl --context kind-guilder get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8096\n</code></pre> which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Congratulations, you\u2019ve just deployed a workload to two edge clusters using kubestellar! To learn more about kubestellar please visit our User Guide</p>"},{"location":"common-subs/brew-install/","title":"Brew install","text":"<pre><code>if ! command -v brew; then\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/runner/.bashrc\n    eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nmore /etc/hosts\n    # sudo echo $(curl https://api.ipify.org) kubestellar.core | sudo tee -a /etc/host\nfi\nbrew tap kubestellar/kubestellar\nbrew update\nbrew install kubestellar-cli\n</code></pre>"},{"location":"common-subs/brew-no/","title":"Brew no","text":"<p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) \\\n    --kubestellar-version latest --deploy false\n\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre>"},{"location":"common-subs/brew-remove/","title":"Brew remove","text":"<pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre>"},{"location":"common-subs/build-core-image/","title":"Build core image","text":"<p>Build the core images - both kubestellar and space-framework.</p> <pre><code>pwd\nexport EXTRA_CORE_TAG=$(date +test%m%d-%H%M%S)\nmake kubestellar-image-local\ncd space-framework\nmake spacecore-image-local\ncd ..\n</code></pre>"},{"location":"common-subs/build-syncer-image/","title":"Build syncer image","text":"<p>Build the syncer image.</p> <pre><code>export SYNCER_IMG_REF=$(\nif (docker info | grep podman) &amp;&gt; /dev/null\n  then export DOCKER_HOST=unix://${HOME}/.local/share/containers/podman/machine/qemu/podman.sock\n  fi\nKO_DOCKER_REPO=\"\" make build-kubestellar-syncer-image-local | grep -v \"make.*directory\" )\necho \"Locally built syncer image reference is &lt;$SYNCER_IMG_REF&gt;; adding alias ko.local/syncer:test\"\ndocker tag \"$SYNCER_IMG_REF\" ko.local/syncer:test\n</code></pre>"},{"location":"common-subs/check-kubestellar-helm-deployment-running/","title":"Check kubestellar helm deployment running","text":"<pre><code>echo -n 'Waiting for KubeStellar to be ready'\nwhile ! KUBECONFIG=~/.kube/config kubectl exec $(KUBECONFIG=~/.kube/config kubectl get pod \\\n--selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n-n kubestellar -c init -- ls /home/kubestellar/ready &amp;&gt; /dev/null; do\nsleep 10\necho -n \".\"\ndone\necho; echo; echo \"KubeStellar is now ready to take requests\"\n</code></pre>"},{"location":"common-subs/coming-soon/","title":"Coming soon","text":""},{"location":"common-subs/create-files-and-contexts-for-kind-clusters/","title":"Create files and contexts for kind clusters","text":"<p>important: rename the kubernetes contexts of the Kind clusters to match their use in this guide <pre><code>KUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-core ks-core\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-edge-cluster1 ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl config rename-context kind-ks-edge-cluster2 ks-edge-cluster2\n</code></pre></p>"},{"location":"common-subs/create-files-and-contexts-for-openshift-clusters/","title":"Create files and contexts for openshift clusters","text":"<p>important: alias the kubernetes contexts of the OpenShift clusters you provided to match their use in this guide <pre><code>oc login &lt;ks-core OpenShift cluster&gt;\nCURRENT_CONTEXT=$(KUBECONFIG=~/.kube/config kubectl config current-context) \\\n    &amp;&amp; KUBECONFIG=~/.kube/config kubectl config set-context ks-core \\\n    --namespace=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $1}') \\\n    --cluster=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $2}') \\\n    --user=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $3\"/\"$2}')\n\noc login &lt;ks-edge-cluster1 OpenShift cluster&gt;\nCURRENT_CONTEXT=$(KUBECONFIG=~/.kube/config kubectl config current-context) \\\n    &amp;&amp; KUBECONFIG=~/.kube/config kubectl config set-context ks-edge-cluster1 \\\n    --namespace=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $1}') \\\n    --cluster=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $2}') \\\n    --user=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $3\"/\"$2}')\n\noc login &lt;ks-edge-cluster2 OpenShift cluster&gt;\nCURRENT_CONTEXT=$(KUBECONFIG=~/.kube/config kubectl config current-context) \\\n    &amp;&amp; KUBECONFIG=~/.kube/config kubectl config set-context ks-edge-cluster2 \\\n    --namespace=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $1}') \\\n    --cluster=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $2}') \\\n    --user=$(echo \"$CURRENT_CONTEXT\" | awk -F '/' '{print $3\"/\"$2}')\n</code></pre></p>"},{"location":"common-subs/create-ks-core-kind-cluster/","title":"Create ks core kind cluster","text":"<p>create the ks-core kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-core --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 1119\n    protocol: TCP\nEOF\n</code></pre></p> <p>Be sure to create an ingress control with SSL passthrough to ks-core. This is a special requirement for Kind that allows access to the KubeStellar core running on ks-core. <pre><code>KUBECONFIG=~/.kube/config kubectl \\\ncreate -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\n</code></pre> Wait about 20 seconds and then check if the ingress control is ready on ks-core: <pre><code>sleep 20\nKUBECONFIG=~/.kube/config kubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=90s\n</code></pre></p>"},{"location":"common-subs/create-ks-edge-cluster1-kind-cluster/","title":"Create ks edge cluster1 kind cluster","text":"<p>create the ks-edge-cluster1 kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-edge-cluster1 --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre></p>"},{"location":"common-subs/create-ks-edge-cluster2-kind-cluster/","title":"Create ks edge cluster2 kind cluster","text":"<p>create the ks-edge-cluster2 kind cluster <pre><code>KUBECONFIG=~/.kube/config kind create cluster --name ks-edge-cluster2 --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre></p>"},{"location":"common-subs/debian-kind-docker/","title":"Debian kind docker","text":"<p>on Debian, the syncers on ks-edge-cluster1 and ks-edge-cluster2 will not resolve the kubestellar.core hostname </p> <p>You have 2 choices:  </p> <ol> <li> <p>Use the value of <code>hostname -f</code> instead of kubestellar.core as your \"EXTERNAL_HOSTNAME\" in \"Step 1:  Deploy the KubeStellar Core Component\", or</p> </li> <li> <p>Just before step 6 in the KubeStellar User Quickstart for Kind do the following</p> </li> </ol> <p>Add IP/domain to /etc/hosts of cluster1/cluster2 containers (replace with appropriate IP address):</p> <pre><code>docker exec -it $(docker ps | grep ks-edge-cluster1 | cut -d \" \" -f 1) \\\n    sh -c \"echo '192.168.122.144 kubestellar.core' &gt;&gt; /etc/hosts\"\ndocker exec -it $(docker ps | grep ks-edge-cluster2 | cut -d \" \" -f 1) \\\n    sh -c \"echo '192.168.122.144 kubestellar.core' &gt;&gt; /etc/hosts\"\n</code></pre> <p>Edit coredns ConfigMap for cluster1 and cluster1 (see added lines in example):</p> <pre><code>KUBECONFIG=~/.kube/config kubectl edit cm coredns -n kube-system --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl edit cm coredns -n kube-system --context=ks-edge-cluster2\n</code></pre> <p>add the highlighted information <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n           lameduck 5s\n        }\n        hosts /etc/coredns/customdomains.db core {  \n          fallthrough                               \n        }                                          \n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n  customdomains.db: |                   \n    192.168.122.144 kubestellar.core  \nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2023-10-24T19:18:05Z\"\n  name: coredns\n  namespace: kube-system\n  resourceVersion: \"10602\"\n  uid: 3930c18f-23e8-4d0b-9ddf-658fdf3cb20f\n</code></pre></p> <p>Edit Deployment for coredns on cluster1 and cluster2, adding the key/path at the given location: <pre><code>KUBECONFIG=~/.kube/config kubectl edit -n kube-system \\\n    deployment coredns --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl edit -n kube-system \\\n    deployment coredns --context=ks-edge-cluster2\n</code></pre></p> <pre><code>spec:\n  template:\n    spec:\n      volumes:\n      - configMap:\n          items:\n          - key: customdomains.db\n            path: customdomains.db\n</code></pre> <p>Restart coredns pods: <pre><code>KUBECONFIG=~/.kube/config kubectl rollout restart \\\n    -n kube-system deployment/coredns --context=ks-edge-cluster1\nKUBECONFIG=~/.kube/config kubectl rollout restart \\\n    -n kube-system deployment/coredns --context=ks-edge-cluster2\n</code></pre></p> <p>(adapted from \"The Cluster-wise solution\" at https://stackoverflow.com/questions/37166822/is-there-a-way-to-add-arbitrary-records-to-kube-dns)</p>"},{"location":"common-subs/delete-contexts-for-kind-and-openshift-clusters/","title":"Delete contexts for kind and openshift clusters","text":"<p>important: delete any existing kubernetes contexts of the clusters you may have created previously <pre><code>KUBECONFIG=~/.kube/config kubectl config delete-context ks-core || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster1 || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster2 || true\n</code></pre></p>"},{"location":"common-subs/deploy-your-kubestellar-core-component-kind/","title":"Deploy your kubestellar core component kind","text":"deployuh oh, error?open a bug report <p>deploy the KubeStellar Core components on the ks-core Kind cluster you created in the pre-req section above  </p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install kubestellar/kubestellar-core \\\n--set EXTERNAL_HOSTNAME=\"kubestellar.core\" \\\n--set EXTERNAL_PORT=1119 \\\n--namespace kubestellar \\\n--generate-name\n</code></pre> <p>important: You must add 'kubestellar.core' to your /etc/hosts file with the local network IP address (e.g., 192.168.x.y) where your ks-core Kind cluster is running. DO NOT use <code>127.0.0.1</code> because the ks-edge-cluster1 and ks-edge-cluster2 kind clusters map <code>127.0.0.1</code> to their local kubernetes cluster, not the ks-core kind cluster.</p> <p>run the following to wait for KubeStellar to be ready to take requests: </p> <pre><code>echo -n 'Waiting for KubeStellar to be ready'\nwhile ! KUBECONFIG=~/.kube/config kubectl exec $(KUBECONFIG=~/.kube/config kubectl get pod \\\n--selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n-n kubestellar -c init -- ls /home/kubestellar/ready &amp;&gt; /dev/null; do\nsleep 10\necho -n \".\"\ndone\necho; echo; echo \"KubeStellar is now ready to take requests\"\n</code></pre> <p>Checking the initialization log to see if there are any obvious errors:  <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  \nkubectl logs \\\n  $(kubectl get pod --selector=app=kubestellar \\\n  -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n  -n kubestellar -c init\n</code></pre>  if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"common-subs/deploy-your-kubestellar-core-component-openshift/","title":"Deploy your kubestellar core component openshift","text":"deployuh oh, error?open a bug report <p>deploy the KubeStellar Core components on the ks-core OpenShift cluster  </p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install kubestellar/kubestellar-core \\\n--set clusterType=OpenShift \\\n--namespace kubestellar \\\n--generate-name\n</code></pre> <p>run the following to wait for KubeStellar to be ready to take requests: </p> <pre><code>echo -n 'Waiting for KubeStellar to be ready'\nwhile ! KUBECONFIG=~/.kube/config kubectl exec $(KUBECONFIG=~/.kube/config kubectl get pod \\\n--selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n-n kubestellar -c init -- ls /home/kubestellar/ready &amp;&gt; /dev/null; do\nsleep 10\necho -n \".\"\ndone\necho; echo; echo \"KubeStellar is now ready to take requests\"\n</code></pre> <p>Checking the initialization log to see if there are any obvious errors:  <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  \nkubectl logs \\\n  $(kubectl get pod --selector=app=kubestellar \\\n  -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n  -n kubestellar -c init\n</code></pre>  if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"common-subs/deploy-your-kubestellar-core-component-test/","title":"Deploy your kubestellar core component test","text":"deployuh oh, error?open a bug report <p>deploy the KubeStellar Core components on the ks-core Kind cluster you created in the pre-req section above  </p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install ./core-helm-chart \\\n--set EXTERNAL_HOSTNAME=\"kubestellar.core\" \\\n--set EXTERNAL_PORT=1119 \\\n--set CONTROLLER_VERBOSITY=4 \\\n--set image.tag=$EXTRA_CORE_TAG \\\n--set spaceimage.tag=$EXTRA_CORE_TAG \\\n--namespace kubestellar \\\n--generate-name\n</code></pre> <p>important: You must add 'kubestellar.core' to your /etc/hosts file with the local network IP address (e.g., 192.168.x.y) where your ks-core Kind cluster is running. DO NOT use <code>127.0.0.1</code> because the ks-edge-cluster1 and ks-edge-cluster2 kind clusters map <code>127.0.0.1</code> to their local kubernetes cluster, not the ks-core kind cluster.</p> <p>run the following to wait for KubeStellar to be ready to take requests: </p> <pre><code>echo -n 'Waiting for KubeStellar to be ready'\nwhile ! KUBECONFIG=~/.kube/config kubectl exec $(KUBECONFIG=~/.kube/config kubectl get pod \\\n--selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n-n kubestellar -c init -- ls /home/kubestellar/ready &amp;&gt; /dev/null; do\nsleep 10\necho -n \".\"\ndone\necho; echo; echo \"KubeStellar is now ready to take requests\"\n</code></pre> <p>Checking the initialization log to see if there are any obvious errors:  <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  \nkubectl logs \\\n  $(kubectl get pod --selector=app=kubestellar \\\n  -o jsonpath='{.items[0].metadata.name}' -n kubestellar) \\\n  -n kubestellar -c init\n</code></pre>  if there is nothing obvious, open a bug report and we can help you out</p> <p>Stuck? open a bug report and we can help you out</p>"},{"location":"common-subs/install-helm-kind/","title":"Install helm kind","text":"<pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install kubestellar/kubestellar-core \\\n--set EXTERNAL_HOSTNAME=\"kubestellar.core\" \\\n--set EXTERNAL_PORT=1119 \\\n--namespace kubestellar \\\n--generate-name\n</code></pre>"},{"location":"common-subs/install-helm-openshift/","title":"Install helm openshift","text":"<pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install kubestellar/kubestellar-core \\\n--set clusterType=OpenShift \\\n--namespace kubestellar \\\n--generate-name\n</code></pre>"},{"location":"common-subs/install-helm-test/","title":"Install helm test","text":"<pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-core  KUBECONFIG=~/.kube/config kubectl create namespace kubestellar  helm repo add kubestellar https://helm.kubestellar.io\nhelm repo update\nKUBECONFIG=~/.kube/config helm install ./core-helm-chart \\\n--set EXTERNAL_HOSTNAME=\"kubestellar.core\" \\\n--set EXTERNAL_PORT=1119 \\\n--set CONTROLLER_VERBOSITY=4 \\\n--set image.tag=$EXTRA_CORE_TAG \\\n--set spaceimage.tag=$EXTRA_CORE_TAG \\\n--namespace kubestellar \\\n--generate-name\n</code></pre>"},{"location":"common-subs/kubestellar-apply-apache-kind/","title":"Kubestellar apply apache kind","text":"<p>KubeStellar's helm chart automatically creates a Workload Management Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>In the <code>wmw1</code> space create the following <code>EdgePlacement</code> object:  <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.  <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre></p> <p>Check if your configmap and deployment were created correctly.</p> <pre><code>KUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=$WMW1_KUBECONFIG kubectl get deployments,cm -n my-namespace\n</code></pre>"},{"location":"common-subs/kubestellar-apply-apache-openshift/","title":"Kubestellar apply apache openshift","text":"<p>KubeStellar's helm chart automatically creates a Workload Management Workspace (WMW) for you to store kubernetes workload descriptions and KubeStellar control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create an EdgePlacement control object to direct where your workload runs using the 'location-group=edge' label selector. This label selector's value ensures your workload is directed to both clusters, as they were labeled with 'location-group=edge' when you issued the 'kubestellar prep-for-cluster' command above.</p> <p>This EdgePlacement includes downsync of a <code>RoleBinding</code> that grants privileges that let the httpd pod run in an OpenShift cluster as well as applying the OpenShift route CRD so support exposure of services.</p> <p>In the <code>root:wmw1</code> workspace create the following <code>EdgePlacement</code> object:  <pre><code>KUBECONFIG=ks-core.kubeconfig \\\nkubectl apply -f https://raw.githubusercontent.com/openshift/router/master/deploy/route_crd.yaml\n\nKUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\n\nKUBECONFIG=ks-core.kubeconfig kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: my-first-edge-placement\nspec:\n  locationSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps, services ]\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n  - apiGroup: route.openshift.io\n    namespaces: [ my-namespace ]\n    objectNames: [ \"*\" ]\n    resources: [ routes ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaces: [ my-namespace ]\n    objectNames: [ my-first-kubestellar-deployment ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  - apiGroup: rbac.authorization.k8s.io\n    resources: [ rolebindings ]\n    namespaces: [ my-namespace ]\n    objectNames: [ let-it-be ]\nEOF\n</code></pre></p> <p>check if your edgeplacement was applied to the ks-core <code>kubestellar</code> namespace correctly <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\nKUBECONFIG=ks-core.kubeconfig kubectl get edgeplacements -n kubestellar -o yaml\n</code></pre></p> <p>Now, apply the HTTP server workload definition into the WMW on ks-core. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>my-first-edge-placement</code>) object created above.  <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: my-namespace\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: my-namespace\n  name: my-first-kubestellar-deployment\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: registry.redhat.io/rhel8/httpd-24\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /var/www/html/\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: let-it-be\n  namespace: my-namespace\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:openshift:scc:privileged\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: my-namespace\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: my-namespace\nspec:\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: common\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: my-route\n  namespace: my-namespace\nspec:\n  port:\n    targetPort: 8080\n  to:\n    kind: Service\n    name: my-service\nEOF\n</code></pre></p> <p>check if your configmap, deployment, service, and route was applied to the ks-core <code>my-namespace</code> namespace correctly <pre><code>KUBECONFIG=ks-core.kubeconfig kubectl ws root:wmw1\nKUBECONFIG=ks-core.kubeconfig kubectl get deployments/my-first-kubestellar-deployment -n my-namespace -o yaml\nKUBECONFIG=ks-core.kubeconfig kubectl get deployments,cm,service,route -n my-namespace\n</code></pre></p>"},{"location":"common-subs/kubestellar-apply-syncer/","title":"Kubestellar apply syncer","text":"<pre><code>#apply ks-edge-cluster1 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 apply -f ks-edge-cluster1-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster1 correctly\n#apply ks-edge-cluster2 syncer\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 apply -f ks-edge-cluster2-syncer.yaml\nsleep 3\nKUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pods -A | grep kubestellar  #check if syncer deployed to ks-edge-cluster2 correctly\n</code></pre>"},{"location":"common-subs/kubestellar-check-syncers/","title":"Kubestellar check syncers","text":"<p>If you are unable to see the namespace 'my-namespace' or the deployment 'my-first-kubestellar-deployment' you can view the logs for the KubeStellar syncer on the ks-edge-cluster1 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster1\nks_ns_edge_cluster1=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster1 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster1\n</code></pre> <p>and on the ks-edge-cluster2 Kind cluster:</p> <pre><code>KUBECONFIG=~/.kube/config kubectl config use-context ks-edge-cluster2\nks_ns_edge_cluster2=$(KUBECONFIG=~/.kube/config kubectl get namespaces \\\n-o custom-columns=:metadata.name | grep 'kubestellar-')\nKUBECONFIG=~/.kube/config kubectl logs pod/$(kubectl get pods -n $ks_ns_edge_cluster2 \\\n-o custom-columns=:metadata.name | grep 'kubestellar-') -n $ks_ns_edge_cluster2\n</code></pre>"},{"location":"common-subs/kubestellar-kind-ip-error/","title":"Kubestellar kind ip error","text":"<p>Did you received the following error: <code>`Error: Get \"https://some_hostname.some_domain_name:1119/clusters/root/apis/tenancy.kcp.io/v1alpha1/workspaces\": dial tcp: lookup some_hostname.some_domain_name on x.x.x.x: no such host</code></p> <p>A common error occurs if you set your port number to a pre-occupied port number and/or you set your EXTERNAL_HOSTNAME to something other than \"localhost\" so that you can reach your KubeStellar Core from another host, check the following:</p> <p>Check if the port specified in the ks-core Kind cluster configuration and the EXTERNAL_PORT helm value are occupied by another application:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. is the `hostPort`` specified in the ks-core Kind cluster configuration is occupied by another process?  If so, delete the ks-core Kind cluster and create it again using an available port for your 'hostPort' value</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. if you change the port for your ks-core 'hostPort', remember to also use that port as the helm 'EXTERNAL_PORT' value</p> <p>Check that your EXTERNAL_HOSTNAME helm value is reachable via DNS:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01. use 'nslookup ' to make sure there is a valid IP address associated with the hostname you have specified <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02. make sure your EXTERNAL_HOSTNAME and associated IP address are listed in your /etc/hosts file.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03. make sure the IP address is associated with the system where you have deployed the ks-core Kind cluster</p> <p>if there is nothing obvious, open a bug report and we can help you out</p>"},{"location":"common-subs/kubestellar-list-syncing/","title":"Kubestellar list syncing","text":"<p>Every object subject to downsync or upsync has a full per-WEC copy in the core. These include reported state from the WECs. If you are using release 0.10 or later of KubeStellar then you can list these copies of your httpd <code>Deployment</code> objects with the following command.</p> <pre><code>kubestellar-list-syncing-objects --api-group apps --api-kind Deployment\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  availableReplicas: 1\nconditions:\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\ntype: Available\n  - lastTransitionTime: \"2023-10-27T07:00:19Z\"\nlastUpdateTime: \"2023-10-27T07:00:19Z\"\nmessage: ReplicaSet \"my-first-kubestellar-deployment-76f6fc4cfc\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\ntype: Progressing\n  observedGeneration: 618\nreadyReplicas: 1\nreplicas: 1\nupdatedReplicas: 1\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ... (lots of other details) ...\n  name: my-first-kubestellar-deployment\n  namespace: my-namespace\nspec:\n  ... (the spec) ...\nstatus:\n  ... (another happy status) ...\n</code></pre>"},{"location":"common-subs/kubestellar-prep-syncer-test/","title":"Kubestellar prep syncer test","text":"<pre><code>KUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster1 \\\n--syncer-image ko.local/syncer:test \\\nenv=ks-edge-cluster1 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\nKUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster2 \\\n--syncer-image ko.local/syncer:test \\\nenv=ks-edge-cluster2 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\n</code></pre>"},{"location":"common-subs/kubestellar-prep-syncer/","title":"Kubestellar prep syncer","text":"<pre><code>KUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster1 \\\nenv=ks-edge-cluster1 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\nKUBECONFIG=~/.kube/config kubectl kubestellar prep-for-cluster --imw imw1 ks-edge-cluster2 \\\nenv=ks-edge-cluster2 \\\nlocation-group=edge     #add ks-edge-cluster1 and ks-edge-cluster2 to the same group\n</code></pre>"},{"location":"common-subs/kubestellar-show-available-spaces/","title":"Kubestellar show available spaces","text":"<pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-core get spaces -A </code></pre>"},{"location":"common-subs/kubestellar-test-apache-kind/","title":"Kubestellar test apache kind","text":"<p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear): <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get deployments -A\n</code></pre></p> <p>you should see output including: <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same: <pre><code>KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get deployments -A\n</code></pre></p> <p>you should see output including: <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters: For ks-edge-cluster1: <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8094\n</code></pre></p> <p>you should see the output: <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2: <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl http://localhost:8096\n</code></pre></p> <p>you should see the output: <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"common-subs/kubestellar-test-apache-openshift/","title":"Kubestellar test apache openshift","text":"<p>Now, let's check that the deployment was created in the kind ks-edge-cluster1 cluster (it may take up to 30 seconds to appear): <pre><code>KUBECONFIG=~/.kube/config kubectl --context \\\nks-edge-cluster1 get deployments -A | grep my-namespace\n</code></pre></p> <p>you should see output including: <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       6m48s\n</code></pre></p> <p>And, check the ks-edge-cluster2 kind cluster for the same: <pre><code>KUBECONFIG=~/.kube/config kubectl --context \\\nks-edge-cluster2 get deployments -A | grep my-namespace\n</code></pre></p> <p>you should see output including: <pre><code>NAMESPACE           NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\nmy-namespace        my-first-kubestellar-deployment    1/1        1            1       7m54s\n</code></pre></p> <p>Finally, let's check that the workload is working in both clusters: For ks-edge-cluster1: <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster1 \\\nget route/my-route -n my-namespace -o jsonpath='{.spec.host}')\n</code></pre></p> <p>you should see the output: <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>For ks-edge-cluster2: <pre><code>while [[ $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 get pod \\\n-l \"app=common\" -n my-namespace -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;\ncurl $(KUBECONFIG=~/.kube/config kubectl --context ks-edge-cluster2 \\\nget route/my-route -n my-namespace -o jsonpath='{.spec.host}')\n</code></pre></p> <p>you should see the output: <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This web site is hosted on ks-edge-cluster1 and ks-edge-cluster2.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"common-subs/pre-position-core-image/","title":"Pre position core image","text":"<p>Install the core container image in the ks-core <code>kind</code> cluster.</p> <pre><code>kind load docker-image quay.io/kubestellar/kubestellar:$EXTRA_CORE_TAG --name ks-core\nkind load docker-image quay.io/kubestellar/space-framework:$EXTRA_CORE_TAG --name ks-core\n</code></pre>"},{"location":"common-subs/pre-position-syncer-image/","title":"Pre position syncer image","text":"<p>Install the syncer container image in the two WECs.</p> <pre><code>kind load docker-image ko.local/syncer:test --name ks-edge-cluster1\nkind load docker-image ko.local/syncer:test --name ks-edge-cluster2\n</code></pre>"},{"location":"common-subs/pre-req/","title":"Pre req","text":"<pre><code>os_type=\"\"\narch_type=\"\"\nfolder=\"\"\nget_os_type() {\ncase \"$OSTYPE\" in\nlinux*)   echo \"linux\" ;;\ndarwin*)  echo \"darwin\" ;;\n*)        echo \"Unsupported operating system type: $OSTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_arch_type() {\ncase \"$HOSTTYPE\" in\nx86_64*)  echo \"amd64\" ;;\naarch64*) echo \"arm64\" ;;\narm64*)   echo \"arm64\" ;;\n*)        echo \"Unsupported architecture type: $HOSTTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_os_type() {\ncase \"$OSTYPE\" in\nlinux*)   echo \"linux\" ;;\ndarwin*)  echo \"darwin\" ;;\n*)        echo \"Unsupported operating system type: $OSTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_arch_type() {\ncase \"$HOSTTYPE\" in\nx86_64*)  echo \"amd64\" ;;\naarch64*) echo \"arm64\" ;;\narm64*)   echo \"arm64\" ;;\n*)        echo \"Unsupported architecture type: $HOSTTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nif [ \"$os_type\" == \"\" ]; then\nos_type=$(get_os_type)\nfi\nif [ \"$arch_type\" == \"\" ]; then\narch_type=$(get_arch_type)\nfi\nif [ \"$folder\" == \"\" ]; then\nfolder=\"$PWD\"\nfi\necho $os_type\necho $arch_type\necho $folder\nif command -v docker &gt;/dev/null 2&gt;&amp;1; then\necho \"Docker is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install docker\n    fi\nfi\nif docker info &gt;/dev/null 2&gt;&amp;1; then\necho \"Docker is started\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nopen --background -a Docker\n      sleep 30\nfi\nfi\nif command -v go &gt;/dev/null 2&gt;&amp;1; then\ngo_version_output=$(go version)\ngo_version=$(echo \"$go_version_output\" | awk '{print $3}')\necho \"GO is installed\"\nif [[ \"$go_version\" == \"go1.19\"* ]]; then\necho \"Go version 1.19 is installed.\"\nelse\necho \"Go version 1.19 is not installed.  Please install Go version 1.19 to continue\"\nfi\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install go@1.19\n    fi\nfi\nif command -v kubectl &gt;/dev/null 2&gt;&amp;1; then\necho \"kubectl is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install kubectl\n    fi\nfi\nif command -v jq &gt;/dev/null 2&gt;&amp;1; then\necho \"jq is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install jq\n    fi\nfi\nif command -v kind &gt;/dev/null 2&gt;&amp;1; then\necho \"kind is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install kind\n    fi\nfi\nps -ef | grep mailbox-controller | grep -v grep | grep -v make | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep where-resolver | grep -v grep | grep -v make | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep placement-translator | grep -v grep | grep -v make | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kcp | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep 'exe/main -v 2' | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nkind delete cluster --name florin 2&gt;&amp;1\nkind delete cluster --name guilder 2&gt;&amp;1\n</code></pre>"},{"location":"common-subs/remove-all/","title":"Remove all","text":"<pre><code>ps -ef | grep mailbox-controller | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kubestellar-where-resolver | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep placement-translator | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep space-manager | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep example-backend | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kcp | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep 'exe/main -v 2' | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nkind delete cluster --name florin 2&gt;&amp;1\nkind delete cluster --name guilder 2&gt;&amp;1\nkind delete cluster --name sm-mgt 2&gt;&amp;1\nkubectl config delete-context sm-mgt 2&gt;&amp;1\nrm florin-syncer.yaml florin-config.yaml guilder-syncer.yaml guilder-config.yaml || true\nrm -rf dex/ kcp/ kube-bind/ my-kubeconfigs/ konnector-imw1* konnector-wmw* || true\n</code></pre>"},{"location":"common-subs/required-packages/","title":"Required packages","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>If you intend to build kubestellar from source  you will also need </p> <ul> <li> <p>go (Go version &gt;=1.19 required; 1.19 recommended) [go releases] (https://go.dev/dl)</p> </li> <li> <p>for simplicity, here's a direct link to go releases Remember you need go v1.19 or greater; 1.19 recommended!</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <ol> <li> <p>Download the package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the package file you downloaded and follow the prompts to install Go.    The package installs the Go distribution to /usr/local/go. The package should put the /usr/local/go/bin directory in your PATH environment variable. You may need to restart any open Terminal sessions for the change to take effect.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:       <code>$ go version</code>    Confirm that the command prints the desired installed version of Go. </p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>\n</code></pre></p> <p>visit https://go.dev/doc/install for latest instructions</p> <ol> <li> <p>Remove any previous Go installation by deleting the /usr/local/go folder (if it exists), then extract the archive you just downloaded into /usr/local, creating a fresh Go tree in /usr/local/go:</p> <p><code>$ rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz</code></p> <p>(You may need to run the command as root or through sudo).</p> <p>Do not untar the archive into an existing /usr/local/go tree. This is known to produce broken Go installations.</p> </li> <li> <p>Add /usr/local/go/bin to the PATH environment variable.       You can do this by adding the following line to your $HOME/.profile or /etc/profile (for a system-wide installation):</p> <p><code>export PATH=$PATH:/usr/local/go/bin</code></p> <p>Note: Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile.</p> </li> <li> <p>Verify that you've installed Go by opening a command prompt and typing the following command:</p> <p><code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre> go (only required if you build kubestellar from source)<pre><code>visit https://go.dev/doc/install for latest instructions\n</code></pre></p> <ol> <li> <p>Download the go 1.19 MSI package from https://go.dev/dl#go1.19  Be sure to get the correct one for your architecture</p> </li> <li> <p>Open the MSI file you downloaded and follow the prompts to install Go.</p> <p>By default, the installer will install Go to Program Files or Program Files (x86). You can change the location as needed. After installing, you will need to close and reopen any open command prompts so that changes to the environment made by the installer are reflected at the command prompt. </p> </li> <li> <p>Verify that you've installed Go:</p> <ol> <li> <p>In Windows, click the Start menu.</p> </li> <li> <p>In the menu's search box, type cmd, then press the Enter key.</p> </li> <li> <p>In the Command Prompt window that appears, type the following command:     <code>$ go version</code></p> </li> <li> <p>Confirm that the command prints the installed version of Go.</p> </li> </ol> </li> </ol> <p>How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution</p> <p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p> <p> 1. If you're using a VPN, turn it off</p> <p> 2. Install Ubuntu into WSL</p> <p> 2.0 If wsl is not yet installed, open a powershell administrator window and run the following <pre><code>wsl --install\n</code></pre>  2.1 reboot your system</p> <p> 2.2 In a Windows command terminal run the following to list all the linux distributions that are available online <pre><code>wsl -l -o\n</code></pre>  2.3 Select a linux distribution and install it into WSL <pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p> <p> 2.4 Enter your new username and password at the prompts, and you will eventually see something like: <pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre></p> <p> 2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box. Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p> <p> 3. Install pre-requisites into your new VM  3.1 update and apply apt-get packages <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p> 3.2 Install golang <pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p> 3.3 Install ko (but don't do ko set action step) <pre><code>go install github.com/google/ko@latest\n</code></pre></p> <p> 3.4 Install gcc Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p> <p> 3.5 Install make (if you installed build-essential this may already be installed) <pre><code>apt install make\n</code></pre></p> <p> 3.6 Install jq <pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre></p> <p> 3.7 install kubectl <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>  3.8 install helm (required when deploying as workload) <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p> How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution</p> <p> 1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.</p> <p> 2.0 Install docker The installation instructions from docker are not sufficient to get docker working with WSL</p> <p> 2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/</p> <p>Here some additional steps you will need to take:</p> <p> 2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting. If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p> <p> 2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via <pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p> <p> 2.4 Add your user to the docker group <pre><code>sudo usermod -aG docker $USER\n</code></pre></p> <p> 2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground): <pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre></p> <p> 2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655  The following commands will fix the issue: <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p> <p> 3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal</p> <p> 3.1 In your new terminal, install kind <pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre></p>"},{"location":"common-subs/save-some-time/","title":"Save some time","text":"<p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'{{ no such element: dict object['pre_req_name'] }}','{{ no such element: dict object['manifest_name'] }}'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre>"},{"location":"common-subs/tear-down-kind/","title":"Tear down kind","text":"<p>to remove what you just installed:</p> <pre><code>brew remove kubestellar-cli\nbrew untap kubestellar/kubestellar\n</code></pre> <pre><code>kind delete cluster --name ks-core\nkind delete cluster --name ks-edge-cluster1\nkind delete cluster --name ks-edge-cluster2\n</code></pre> <p>important: delete any existing kubernetes contexts of the clusters you may have created previously <pre><code>KUBECONFIG=~/.kube/config kubectl config delete-context ks-core || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster1 || true\nKUBECONFIG=~/.kube/config kubectl config delete-context ks-edge-cluster2 || true\n</code></pre></p>"},{"location":"common-subs/teardown-the-environment/","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"common-subs/teardown-the-environment/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and the space provider with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"common-subs/teardown-the-environment/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is the space provider and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"v0.20/","title":"KubeStellar 0.20","text":"<p>KubeStellar 0.20  supports multi-cluster deployment of Kubernetes objects, controlled by a  simple placement policy and deploying Kubernetes objects in their native format.  It uses OCM as  transport, with standard OCM agents (Klusterlet). We show examples of deploying workloads to  multi-cluster with kubectl, helm and ArgoCD using a simple label-selectors-based placement policy.</p>"},{"location":"v0.20/#supported-features","title":"Supported Features:","text":"<ol> <li>Multi-cluster Deployment: Kubernetes objects are deployed across multiple clusters, controlled by a  straightforward placement policy.</li> <li>Pure-Kube User Experience: Deployment of non-wrapped objects is handled in a pure Kubernetes manner.</li> <li>Object Management via WDS: Creation, update, and deletion of objects in managed clusters are performed from WDS.</li> <li>OCM as Transport: The Open Cluster Management (OCM) is used as transport, with standard OCM agents (Klusterlet).</li> <li>Multi-WDS and single OCM Shard: Multiple WDSs and a single OCM shard are supported.</li> <li>Resiliency: All components are running in Kubernetes, ensuring continued operation even after restarts of any component.</li> <li>Re-evaluation of Objects: Existing objects are re-evaluated when a new placement is added or updated.</li> <li>Object Removal: Objects are removed from clusters when the placement that led to their deployment on  those clusters is deleted or updated and the what or where no longer match.</li> <li>Dynamic Handling of APIs: Dynamically start/stop informers when adding/removing CRDs.</li> <li>Simplified setup: Just 3 commands to get a fully functional setup (<code>kflex init</code>, <code>kflex create imbs</code>, <code>kflex create wds</code>)</li> <li>OpenShift Support: Same commands to set it up. All components have been tested in OpenShift,  including OCM Klusterlet for the WECs.</li> <li>Singleton Status Addressed by the status controller in KubeStellar 0.20 and the Status Add-On for OCM</li> </ol>"},{"location":"v0.20/#to-be-supported","title":"To be supported","text":"<ol> <li>Status summarization</li> <li>Customization</li> <li>OCM sharding</li> <li>Upsync</li> <li>\"Pluggable Transport\" </li> </ol>"},{"location":"v0.20/#architecture","title":"Architecture","text":"<p>See Architecture.</p>"},{"location":"v0.20/#packaging-and-delivery","title":"Packaging and Delivery","text":"<p>See Packaging and Delivery</p>"},{"location":"v0.20/#usage-examples-and-testing","title":"Usage examples, and testing","text":"<p>See Examples.</p>"},{"location":"v0.20/architecture/","title":"KubeStellar Architecture","text":"<p>KubeStellar provides multi-cluster deployment of Kubernetes objects, controlled by a simple  placement policy, where Kubernetes objects are expressed in their native format with no wrapping or bundling. The high-level architecture for KubeStellar is illustrated in Figure 1.</p> Figure 1 - High Level Architecture  <p>KubeStellar relies on the concept of spaces. A Space is an abstraction to represent an API service that  behaves like a Kubernetes kube-apiserver (including the persistent storage behind it)  and the subset of controllers in the kube-controller-manager that are concerned with  API machinery generalities (not management of containerized workloads).  A KubeFlex <code>ControlPlane</code> is an example. A regular Kubernetes cluster is another example. Users can use spaces to perform these tasks:</p> <ol> <li>Create Workload Definition Spaces (WDSs) to store the definitions of their workloads. A Kubernetes workload is an application that runs on Kubernetes. A workload can be made by a  single Kubernetes object or several objects that work together.</li> <li>Create Inventory and Transport Spaces (ITSs) to manage the inventory of clusters and  the transport of workloads.</li> <li>Register and label Workload Execution Clusters (WECs) with the Inventory and  Transport Space, to keep track of the available clusters and their characteristics.</li> <li>Define Placement Policies to specify what objects and where should be  deployed on the WECs.</li> <li>Submit objects in the native Kubernetes format to the WDSs,  and let the Placement Policies govern which WECs should receive them.</li> <li>Check the status of submitted objects from the WDS.</li> </ol> <p>In KubeStellar, users can assume a variety of roles and responsibilities.  These roles could range from system administrators and application owners  to CISOs and DevOps Engineers. However, for the purpose of this document,  we will not differentiate between these roles. Instead we will use the term  'user' broadly, without attempting to make distinctions among roles.</p> <p>Examples of users interaction with KubeStellar are illustrated in the KubeStellar Usage Examples section.</p> <p>The KubeStellar architecture has these main modules:</p> <ul> <li> <p>KubeStellar Controller Manager: this module is responsible of delivering workload objects from the WDS to the ITS according to placement policies, and  updating the status of objects in the WDS.</p> </li> <li> <p>Space Manager: This module manages the lifecycle of spaces.</p> </li> <li> <p>OCM Cluster Manager: This module syncs objects from the ITS to the Workload Execution  Clusters (WECs). In the ITS, each mailbox namespace is associated with one WEC. Objects  that are put in a mailbox namespace are delivered to the matching WEC.</p> </li> <li> <p>Status Add-On Controller: This module installs the OCM status add-on agent  on all WECs and sets the RBAC permissions for it using the OCM add-on framework.</p> </li> <li> <p>OCM Agent: This module registers the WEC to the OCM Hub, watches for  ManifestWorks and unwraps and syncs the objects into the WEC.</p> </li> <li> <p>OCM Status Add-On Agent: This module watches AppliedManifestWorks  to find objects that are synced by the OCM agent, gets their status  and updates WorkStatus objects in the ITS namespace associated with the WEC.</p> </li> </ul> Figure 2 - Main Modules"},{"location":"v0.20/architecture/#kubestellar-controller-manager","title":"KubeStellar Controller Manager","text":"<p>This module manages the placement and status controllers. The placement controller watches  placement and workload objects on the Workload Definition Space  (WDS) and wraps workload objects into a manifest for delivery through the  Inventory and Transport Space (ITS). The status controller watches for WorkStatus objects on the  ITS and updates the status of objects in the WDS when singleton status is requested  in the placement for those objects. There is one instance of a KubeStellar Controller  Manager for each WDS. Currently this controller-manager runs in the KubeFlex hosting cluster and  is responsible of installing the required CRDs in the associated WDS. More details on the internals of this module are provided in KubeStellar Controllers Architecture</p>"},{"location":"v0.20/architecture/#space-manager","title":"Space Manager","text":"<p>The Space Manager handles the lifecycle of spaces.  KubeStellar uses the KubeFlex project for space management. In KubeFlex, a space is named a <code>ControlPlane</code>, and we will use  both terms in this document. KubeSteller currently prereqs KubeFlex to  provide one or more spaces. We plan to make this optional in the near future.</p> <p>KubeFlex is a flexible framework that supports various kinds of control planes, such as k8s, a basic Kubernetes API Server with a subset of kube controllers, and  vcluster: a virtual cluster that runs on the hosting cluster based on the vCluster Project. More detailed information on the different types of control planes and architecture are described in the KubeFlex Architecture.</p> <p>There are currently two roles for spaces managed by KubeFlex: Inventory and Transport Space  (ITS) and Workload Description Space (WDS). The former runs the OCM Cluster Manager on a vcluster-type control plane, and the latter runs on a k8s-type control plane.</p> <p>An ITS holds the OCM inventory (<code>ManagedCluster</code>) objects and mailbox namespaces. The mailbox  namespaces and their contents are implementation details that users do not deal with. Each  mailbox workspace corresponds 1:1 with a WEC and holds <code>ManifestWork</code> objects managed by the  central KubeStellar controllers.</p> <p>A WDS holds user workload objects and the user's objects that form the interface to KubeStellar  control. Currently the only control objects are <code>Placement</code> objects. We plan to add <code>Customization</code> objects soon, and later objects to specify summarization.</p> <p>KubeFlex provides the ability to start controllers connected to a Control Plane API Server or to deploy Helm Charts into a Control Plane API server with post-create hooks. This feature is currently adopted for KubeStellar modules startup, as it allows to create a Workload Description Space (WDS) and start the KubeStellar Controller Manager, and create an Inventory and Transport Space (ITS) in a <code>vcluster</code> and install the Open Cluster Management Hub there.</p>"},{"location":"v0.20/architecture/#ocm-cluster-manager","title":"OCM Cluster Manager","text":"<p>This module is based on the Open Cluster Management Project, a community-driven project that focuses on multicluster and multicloud scenarios for Kubernetes apps.  It provides APIs for cluster registration, work distribution and much more.  The project is based on a hub-spoke architecture, where a single hub cluster  handles the distribution of workloads through manifests, and one or more spoke clusters  receive and apply the workload objects from the manifests. In Open Cluster Management, spoke clusters  are called managed clusters, and the component running on the hub cluster is the cluster manager. Manifests provide a summary for the status of each object, however in some use  cases this might not be sufficient as the full status for objects may be required.  OCM provides an add-on framework that allows to automatically install additional  agents on the managed clusters to provide specific features. This framework is used to install the status add-on on all managed clusters. KubeStellar currently exposes users directly to OCM inventory management and WEC registration. We plan to make the transport features provided by the OCM project pluggable in the near future.</p>"},{"location":"v0.20/architecture/#status-add-on-controller","title":"Status Add-On Controller","text":"<p>This module automates the installation of the status add-on agent  on all managed clusters. It is based on the  OCM Add-on Framework,  which is a framework that helps developers to develop extensions  for working with multiple clusters in custom cases. A module based on  the add-on framework has two components: a controller and the  add-on agent. The controller interacts with the add-on manager to register  the add-on, manage the distribution of the add-on to all clusters, and set  up the RBAC permissions required by the add-on agent to interact with the mailbox  namespace associated with the managed cluster. More specifically, the status  add-on controller sets up RBAC permissions to allow the add-on agent to  list and get ManifestWork objects and create and update WorkStatus objects.</p>"},{"location":"v0.20/architecture/#ocm-agent","title":"OCM Agent","text":"<p>The OCM Agent Module (a.k.a klusterlet) has two main controllers: the registration agent and the work agent. </p> <p>The registration agent is responsible for registering  a new cluster into OCM. The agent creates an unaccepted ManagedCluster into  the hub cluster along with a temporary CertificateSigningRequest (CSR) resource.  The cluster will be accepted by the hub control plane if the CSR is approved and  signed by any certificate provider setting filling <code>.status.certificate</code> with legit  X.509 certificates, and the ManagedCluster resource is approved by setting  <code>.spec.hubAcceptsClient</code> to true in the spec. Upon approval, the registration  agent observes the signed certificate and persists them as a local secret  named <code>hub-kubeconfig-secret</code> (by default in the <code>open-cluster-management-agent</code> namespace)  which will be mounted to the other fundamental components of klusterlet such as  the work agent. The registration process in OCM is called double opt-in mechanism,  which means that a successful cluster registration requires both sides of approval  and commitment from the hub cluster and the managed cluster.</p> <p>The work agent monitors the <code>ManifestWork</code> resource in the cluster namespace  on the hub cluster. The work agent tracks all the resources defined in ManifestWork  and updates its status. There are two types of status in ManifestWork: the resourceStatus  tracks the status of each manifest in the ManifestWork, and conditions reflects the overall  status of the ManifestWork. The work agent checks whether a resource is Available,  meaning the resource exists on the managed cluster, and Applied, meaning the resource  defined in ManifestWork has been applied to the managed cluster. To ensure the resources  applied by ManifestWork are reliably recorded, the work agent creates an AppliedManifestWork  on the managed cluster for each ManifestWork as an anchor for resources relating to ManifestWork.  When ManifestWork is deleted, the work agent runs a Foreground deletion, and that ManifestWork  will stay in deleting state until all its related resources have been fully cleaned in the managed  cluster.</p>"},{"location":"v0.20/architecture/#ocm-status-add-on-agent","title":"OCM Status Add-On Agent","text":"<p>The OCM Status Add-On Agent is a controller that runs alongside the OCM Agent  in the managed cluster. Its primary function is to track objects delivered  by the work agent and report the full status of those objects back to the ITS.  The KubeStellar controller can then use different user-defined summarization  policies to report status, such as the <code>wantSingletonReportedState</code> policy that reports  full status for each deployed object when the workload is delivered only to one  cluster. The controller watches AppliedManifestWork objects to determine which  objects have been delivered through the work agent. It then starts dynamic informers  to watch those objects, collect their individual statuses, and report back the status  updating WorkStatus objects in the namespace associated with the WEC in the ITS. Installing the status add-on cause status to be returned to <code>WorkStatus</code>  objects for all downsynced objects.</p>"},{"location":"v0.20/architecture/#kubestellar-controllers-architecture","title":"KubeStellar Controllers Architecture","text":"<p>The KubeStellar controllers architecture is based on common patterns and best  practices for Kubernetes controllers, such as the  Kubernetes Sample Controller.  A Kubernetes controller uses informers to watch for changes in Kubernetes objects, caches to store the objects, event handlers to react to events, work queues for parallel processing of tasks, and a reconciler to ensure the actual state matches the desired state. However, that pattern has been extended to provide the following features:</p> <ul> <li>Using dynamic informers</li> <li>Starting informers on all API Resources (except some that do not need   watching)</li> <li>Informers and Listers references are maintained in a hash map and   indexed by GVK (Group, Version, Kind) of the watched objects.</li> <li>Using a common work queue and set of workers, where the key is defined as follows:</li> <li>Key is a struct instead than a string, and contains the following:<ul> <li>GVK of the informer and lister for the object that generated the   event</li> <li>Structured Name of the object </li> <li>For delete event: Shallow copy of the object being deleted. This   is required for objects that need to be deleted   from the managed clusters (WECs)</li> </ul> </li> <li>Starting &amp; stopping informers dynamically based on creation or   deletion of CRDs (which add/remove APIs on the WDS).</li> <li>One client connected to the WDS space and one (or more in the future)   to connect to one or more OCM shards.</li> <li>The WDS-connected client is used to start the dynamic     informers/listers for most API resources in the WDS</li> <li>The OCM-connected client is used to start informers/listers for OCM     ManagedClusters and to copy/update/remove the wrapped objects     into/from the OCM mailbox namespaces.</li> </ul> <p>Currently there are two controllers in the KubeStellar controller manager: the placement controller and the status controllor.</p>"},{"location":"v0.20/architecture/#placement-controller","title":"Placement Controller","text":"<p>The Placement controller is responsible for watching workload and  placement objects, and wrapping and delivering objects to the ITS based on placement policies.</p> <p>The architecture and the event flow for create/update object events is illustrated in Figure 3 (some details might be omitted to make the flow easier to understand).</p> Figure 3 - Placement Controller <p>At startup, the controller code sets up the dynamic informers, the event handler and the work queue as follows:</p> <ul> <li>lists all API preferred resources (using discovery client's ServerPreferredResources()   to return only one preferred storage version for API group)</li> <li>Filters out some resources</li> <li>For each resource:</li> <li>Creates GVK key</li> <li>Registers Event Handler</li> <li>Starts Informer</li> <li>Indexes informer and lister in a map by GVK key</li> <li>Waits for all caches to sync</li> <li>Starts N workers to process work queue</li> </ul> <p>The reflector is started as part of the informer and watches specific resources on the WDS API Server; on create/update/delete object events it puts a copy of the object into the local cache. The informer invokes the event handler. The handler implements the event handling functions (AddFunc, UpdateFunc, DeleteFunc)</p> <p>A typical event flow for a create/update object event will run as follows:</p> <ol> <li> <p>Informer invokes the event handler AddFunc or UpdateFunc</p> </li> <li> <p>The event handler does some filtering (for example, to ignore update     events where the object content is not modified) and then creates a     key to store a reference to the object in the work queue. The key     contains the GVK key used to retrieve a reference to the informer     and lister for that kind of object, and a namespace + name key to     retrieve the actual object. Storing the key in the work queue is a     common pattern in client-go as the object may have changed in the     cache (which is kept updated by the reflector) by the time a worker     gets a copy from the work queue. Workers should always receive the     key and use it to retrieve the object from the cache.</p> </li> <li> <p>A worker pulls a key from the work queue, and then does the     following processing:</p> <ul> <li>Uses the GVK key to get the reference to the lister for the     object</li> <li>Gets the object from the lister cache using the NamespacedName of     the object.</li> <li>If the object was not found (because it was deleted) worker returns.     A delete event for that object consumed by the event handler enqueues     a key for Object Deleted.</li> <li>Gets the lister for Placement objects and list all placements</li> <li>Iterates on all placements, and for each placement:<ul> <li>evaluates whether the object matches the downsync selection    criteria in the Placement.</li> <li>If there is a match, list ManagedClusters and   find the matching clusters using the label selector expression   for clusters.</li> <li>If there are matching clusters, add the names of the cluster   to a hashmap setting the name of the cluster as a key. Cluster   groups from different placements are merged together.</li> <li>If any of the matched Placements has <code>WantSingletonReportedState</code>   set to true, clusters are sorted in alphanumerical order and   only the first cluster is selected for delivery. Note that setting    <code>WantSingletonReportedState</code> in one of the placements that    matches the object affects the behavior for all matching placements,    those with matching clusters and those w/o matching clusters. </li> </ul> </li> <li>If there are no matching clusters, the worker returns without     actions and is ready to process other events from the queue.</li> <li>If there are matching clusters:<ul> <li>Wraps the object into a ManifestWork</li> <li>Adds a label for each matched Placement to the ManifestWork that    is used to track the placement that caused the object to be    delivered to one or more clusters. The label contains both the    placement name (note that placement is cluster-scoped) and the WDS name.    This way, when a placement is deleted or updated it is possible to    locate the associated ManifestWorks for deletion or label removal    (if more than one label is present, as other placements may \u201cown\u201d the object).</li> <li>For each cluster:</li> <li>Sets the manifest namespace == name of the cluster</li> <li>Uses the client connected to the ITS to do a server-side      apply patch of the manifest into the namespace.</li> <li>At this time there is only one field manager name for the server-side     apply (<code>kubestellar</code>) but the name should include also the WDS     name to allow detecting conflicts when two different field managers     try to patch a manifest with the same name.   </li> <li>Worker returns and is ready to pick other keys from the queue.</li> </ul> </li> </ul> </li> </ol> <p>There are other event flows, based on the object GVK and type of event.  Error conditions may cause the re-enqueing of keys, resulting in retries. The following sections broadly describe these flows.</p>"},{"location":"v0.20/architecture/#object-deleted","title":"Object Deleted","text":"<p>When an object is deleted from the WDS, the handler\u2019s DeleteFunc is invoked. A shallow copy of the object is added to a field in the key  before pushing it to the work queue. Then:</p> <ul> <li> <p>Worker pulls the key from the work queue</p> </li> <li> <p>Flow continues the same way as in the create/update scenario, however   the deletedObject field in the key indicates that the object has been   deleted and that it needs to be removed from all clusters.</p> </li> <li> <p>Running the selectors matching logic as in the create/delete scenario   once again produces a list of clusters.</p> </li> <li> <p>Worker iterates over the list of clusters and deletes the manifest   work for the object from each namespace associated with the cluster.</p> </li> <li> <p>Worker returns and is ready to pick other keys from the queue.</p> </li> </ul>"},{"location":"v0.20/architecture/#placement-created-or-updated","title":"Placement Created or Updated","text":"<p>Worker pulls key from queue; if it is a placement and it has not been deleted (deletion timestamp not set) it follows the following flow:</p> <ul> <li>Re-enqueues all objects to force re-evaluation: this is done by    iterating all GVK-indexed listers, listing objects for each lister   and re-enqueuing the key for each object.</li> <li>Remove ManifestWorks from ITS for objects no longer matching: generate   the ManagedByPlacementLabelKey for the current (Placement, WDS) and use   that to retrieve all the manifestworks associated with the (Placement, WDS).   Then, for each manifestwork, extract the wrapped object, and re-evaluate the   object vs. the current placement. If no longer a match (either for the   \"what\" or the \"where\" part) check if each manifestwork has other (Placement, WDS)   labels. If yes, remove the label, if not, delete the manifestwork.</li> </ul> <p>Re-enqueuing all object keys forces the re-evaluation of all objects vs. all placements. This is a shortcut as it would be more efficient to re-evaluate all objects vs. the changed placement only, but it saves some additional complexity in the code.</p>"},{"location":"v0.20/architecture/#placement-deleted","title":"Placement Deleted","text":"<p>When the placement controller first processes a new Placement, the placement  controller sets a finalizer on it. The Worker pulls a key from queue; if it is  a placement and it has been deleted (deletion timestamp is set) it follows the flow below:</p> <ul> <li>Lists all ManifestWorks on all namespaces in ITS by   placement label.</li> <li>Iterates on all matching ManifestWorks, for each one:</li> <li>Deletes the ManifestWork (if no other placement labels) or the label     (if other placement labels are present).</li> <li>Deletes placement finalizer.</li> </ul>"},{"location":"v0.20/architecture/#new-crd-added","title":"New CRD Added","text":"<p>When a new CRD is added, the placement controller needs to start a new informer to watch instances of the new CRD on the WDS.</p> <p>The worker pulls a key from queue and creates a GVK Key; if it is a CRD and not deleted:</p> <ul> <li>Checks if an informer for that GVK was already started, return if that   is the case.</li> <li>If not, creates a new informer</li> <li>Registers the event handler for the informer (same one used for all   other api resources)</li> <li>Starts the new informer with a stopper channel, so that it can be   stopped later on by closing the channel.</li> <li>adds informer, lister and stopper channel references to the hashmap   indexed by the GVK key.</li> </ul>"},{"location":"v0.20/architecture/#crd-deleted","title":"CRD Deleted","text":"<p>When a CRD is deleted, the controller needs to stop the informer that was used to watch instances of that CRD on the WDS. This is because informers on CRs will keep on throwing exceptions for missing CRDs.</p> <p>The worker pulls a key from queue and creates a GVK Key; if it is a CRD and it has been deleted:</p> <ul> <li>Uses the GVK key to retrieve the stopper channel for the informer.</li> <li>Closes the stopper channel</li> <li>Removes informer, lister and stopper channel references from the   hashmap indexed by the GVK key.</li> </ul>"},{"location":"v0.20/architecture/#status-controller","title":"Status Controller","text":"<p>The status controller watches for WorkStatus objects on the ITS, and for WDS objects propagated by a placement with  the flag <code>wantSingletonReportedState</code> set to true, updates the status of those objects with the corresponding status found in the workstatus object.</p> <p>The WorkStatus objects are created and updated on the ITS by the status add-on. The high-level flow for the singleton status update is described in Figure 4.</p> Figure 4 - Status Controller <p>The status add-on tracks objects applied by the work agent by watching  AppliedManifestWork objects. These objects list the GVR, name and namespace (the latter for namespaced objects) of each object applied by the related ManifestWork. The status add-on then uses this information  to ensure that a singleton informer is started for each GVR,  and to track status updates of each tracked object. The status add-on then creates/updates WorkStatus objects in the ITS with the status of tracked objects in the namespace associated with the WEC cluster. A <code>WorkStatus</code> object contains status for exactly one object, so that  status updates for one object do not require updates of a whole bundle. </p>"},{"location":"v0.20/contributor/","title":"KubeStellar Contributors","text":"<p>NOTE: work in progress... write more</p>"},{"location":"v0.20/contributor/#check-pre-requisites-for-kubestellar","title":"Check pre-requisites for KubeStellar","text":"<p>The check_pre_req script offers a convenient way to check for pre-requisites, needed for KubeStellar deployment and use case scenarios.</p> <p>The check_pre_req script check for a pre-requisite presence in the path, by using the <code>which</code> command, and it can optionally provide version and path information for pre-requisites that are present, or installation information for missing pre-requisites.</p> <p>We envision that the check_pre_req script could be useful for user-side debugging as well as for asserting the presence of pre-requisites in higher-level automation scripts.</p> <p>The check_pre_req script accepts a list of optional flags and arguments.</p> <p>Supported flags:</p> <ul> <li><code>-A|--assert</code>: exits with error code 2 upon finding the fist missing pre-requisite</li> <li><code>-L|--list</code>: prints a list of supported pre-requisites</li> <li><code>-V|--verbose</code>: displays version and path information for installed pre-requisites or installation information for missing pre-requisites</li> <li><code>-X</code>: enable <code>set -x</code> for debugging the script</li> </ul> <p>Supported arguments:</p> <p>The check_pre_req script accepts a list of specific pre-requisites to check, among the list of available ones:</p> <pre><code>$ check_pre_req.sh --list\nargo brew docker go helm jq kflex kind ko kubectl make ocm yq\n</code></pre> <p>For example, list of pre-requisites required by Kubestellar can be checked with the command below (add the <code>-V</code> flag to get the version of each program na dusggestions on how to install missing pre-requisites):</p> <pre><code>$ hack/check_pre_req.sh\nChecking pre-requisites for using KubeStellar:\n\u2714 Docker\n\u2714 kubectl\n\u2714 KubeFlex\n\u2714 OCM CLI\n\u2714 Helm\nChecking additional pre-requisites for running the examples:\n\u2714 Kind\nX ArgoCD CLI\nChecking pre-requisites for building KubeStellar:\n\u2714 GNU Make\n\u2714 Go\n\u2714 KO\n</code></pre> <p>In another example, a specific list of pre-requisites could be asserted by an higher-level script, while providing some installation information, with the command below (note that the script will terminate upon finding a missing pre-requisite):</p> <pre><code>$ check_pre_req.sh --assert --verbose helm argo docker kind\nChecking KubeStellar pre-requisites:\n\u2714 Helm\n  version: version.BuildInfo{Version:\"v3.14.0\", GitCommit:\"3fc9f4b2638e76f26739cd77c7017139be81d0ea\", GitTreeState:\"clean\", GoVersion:\"go1.21.5\"}\npath: /usr/sbin/helm\nX ArgoCD CLI\n  how to install: https://argo-cd.readthedocs.io/en/stable/cli_installation/\n</code></pre>"},{"location":"v0.20/example-wecs/","title":"Create and Register WECs for examples","text":"<p>The following steps show how to create new clusters and register them with the hub as descibed in the official open cluster management docs.</p> <ol> <li>Run the following set of commands for creating a new kind cluster with name \"cluster1\" and registering it with the OCM hub. This cluster will act as a workload cluster.</li> </ol> <pre><code>export CLUSTER=cluster1\nkind create cluster --name ${CLUSTER}\nkubectl config rename-context kind-${CLUSTER} ${CLUSTER}\nclusteradm --context imbs1 get token | grep '^clusteradm join' | sed \"s/&lt;cluster_name&gt;/${CLUSTER}/\" | awk '{print $0 \" --context '${CLUSTER}' --force-internal-endpoint-lookup\"}' | sh\n</code></pre> <p>The last line grabs a token from the hub (<code>imbs1</code> context), and constructs the command to apply on the new cluster    to be registered as a managed cluster on the OCM hub.</p> <ol> <li>Repeat for a second workload cluster:</li> </ol> <pre><code>export CLUSTER=cluster2\nkind create cluster --name ${CLUSTER}\nkubectl config rename-context kind-${CLUSTER} ${CLUSTER}\nclusteradm --context imbs1 get token | grep '^clusteradm join' | sed \"s/&lt;cluster_name&gt;/${CLUSTER}/\" | awk '{print $0 \" --context '${CLUSTER}' --force-internal-endpoint-lookup\"}' | sh\n</code></pre> <ol> <li>Issue the command:</li> </ol> <pre><code>watch kubectl --context imbs1 get csr\n</code></pre> <p>and wait until the certificate signing requests (CSR) for both cluster1 and cluster2 are created, then    ctrl+C.    Note that the CSRs condition is supposed to be <code>Pending</code> until you approve them in step 4.</p> <ol> <li>Once the CSRs are created approve the csrs to complete the cluster registration with the command:</li> </ol> <pre><code>clusteradm --context imbs1 accept --clusters cluster1\nclusteradm --context imbs1 accept --clusters cluster2\n</code></pre> <ol> <li>Check the new clusters are in the OCM inventory and label them:</li> </ol> <pre><code>kubectl --context imbs1 get managedclusters\nkubectl --context imbs1 label managedcluster cluster1 location-group=edge name=cluster1\nkubectl --context imbs1 label managedcluster cluster2 location-group=edge name=cluster2\n</code></pre>"},{"location":"v0.20/examples/","title":"KubeStellar Usage Examples","text":""},{"location":"v0.20/examples/#warning-a-renaming-of-api-group-and-kind-for-kubestellar-apis-is-in-progress-and-these-instructions-are-temporarly-broken","title":"WARNING !!! a renaming of API Group and Kind for KubeStellar APIs is in progress and these instructions are temporarly broken.","text":"<p>NOTE: This is unmaintained material that has only been observed to work for the commit tagged as <code>v0.20.0-alpha.1</code>. CI regularly tests variants of scenarios 1 and 4 that exercise the copy of the repo that they are embedded in (rather than the copy tagged <code>v0.20.0-alpha.1</code>), and contributors can run these tests too; see the e2e tests.</p>"},{"location":"v0.20/examples/#prereqs","title":"Prereqs","text":"<p>See pre-reqs.</p>"},{"location":"v0.20/examples/#common-setup","title":"Common Setup","text":"<p>The following steps establish an initial state used in the examples below.</p> <ol> <li>Create a Kind hosting cluster with nginx ingress controller and KubeFlex operator installed:</li> </ol> <pre><code>kflex init --create-kind\n</code></pre> <ol> <li>Update the post-create-hooks in KubeFlex to install kubestellar with the v0.20.0-alpha.1 images:</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/v0.20.0-alpha.1/config/postcreate-hooks/kubestellar.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/v0.20.0-alpha.1/config/postcreate-hooks/ocm.yaml\n</code></pre> <ol> <li>Create an inventory &amp; mailbox space of type <code>vcluster</code> running OCM (Open Cluster Management) in KubeFlex. Note that <code>-p ocm</code> runs a post-create hook on the vcluster control plane which installs OCM on it.</li> </ol> <pre><code>kflex create imbs1 --type vcluster -p ocm\n</code></pre> <ol> <li>Install status add-on on imbs1:</li> </ol> <p>Wait until the <code>managedclusteraddons</code> resource shows up on <code>imbs1</code>. You can check on that with the command:</p> <pre><code>kubectl --context imbs1 api-resources | grep managedclusteraddons\n</code></pre> <p>and then install the status add-on:</p> <pre><code>helm --kube-context imbs1 upgrade --install status-addon -n open-cluster-management oci://quay.io/pdettori/status-addon-chart --version 0.1.0\n</code></pre> <p>see here for more details on the add-on.</p> <ol> <li>Create a Workload Description Space <code>wds1</code> in KubeFlex. Similarly to before, <code>-p kubestellar</code> runs a post-create hook on the k8s control plane that starts an instance of a KubeStellar controller manager which connects to the <code>wds1</code> front-end and the <code>imbs1</code> OCM control plane back-end.</li> </ol> <pre><code>kflex create wds1 -p kubestellar\n</code></pre> <ol> <li> <p>Follow the steps to create and register two clusters with OCM.</p> </li> <li> <p>(optional) Check all deployments and statefulsets running in the hosting cluster. Expect to see the wds1 kubestellar-controller-manager created in the wds1-system namespace and the imbs1 statefulset created in the imbs1-system namespace.</p> </li> </ol> <pre><code>kubectl --context kind-kubeflex get deployments,statefulsets --all-namespaces\n</code></pre>"},{"location":"v0.20/examples/#scenario-1-multi-cluster-workload-deployment-with-kubectl","title":"Scenario 1 - multi-cluster workload deployment with kubectl","text":"<p>This scenario proceeds from the state established by the common setup.</p> <p>Check for available clusters with label <code>location-group=edge</code></p> <pre><code>kubectl --context imbs1 get managedclusters -l location-group=edge\n</code></pre> <p>Create a placement to deliver an app to all clusters in wds1:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: Placement\nmetadata:\n  name: nginx-placement\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx\"}\nEOF\n</code></pre> <p>This placement configuration determines where to deploy the workload by using the label selector expressions found in clusterSelectors. It also specifies what to deploy through the downsync.labelSelectors expressions. Each matchLabels expression is a criterion for selecting a set of objects based on their labels. Other criteria can be added to filter objects based on their namespace, api group, resource, and name. If these criteria are not specified, all objects with the matching labels are selected. If an object has multiple labels, it is selected only if it matches all the labels in the matchLabels expression. If there are multiple objectSelectors, an object is selected if it matches any of them.</p> <p>Now deploy the app:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx\n  name: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: nginx\n  labels:\n    app.kubernetes.io/name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that manifestworks wrapping the objects have been created in the mailbox namespaces:</p> <pre><code>kubectl --context imbs1 get manifestworks -n cluster1\nkubectl --context imbs1 get manifestworks -n cluster2\n</code></pre> <p>Verify that the deployment has been created in both clusters</p> <pre><code>kubectl --context cluster1 get deployments -n nginx\nkubectl --context cluster2 get deployments -n nginx\n</code></pre> <p>Please note, in line with Kubernetes\u2019 best practices, the order in which you apply a placement and the objects doesn\u2019t affect the outcome. You can apply the placement first followed by the objects, or vice versa. The result remains consistent because the placement controller identifies any changes in either the placement or the objects, triggering the start of the reconciliation loop.</p>"},{"location":"v0.20/examples/#scenario-2-using-the-hosting-cluster-as-wds-to-deploy-a-custom-resource","title":"Scenario 2 - using the hosting cluster as WDS to deploy a custom resource","text":"<p>This scenario follows on from the state established by scenario 1.</p> <p>The hosting cluster can act as a Workload Description Space (WDS) to distribute your workloads to multiple clusters. This feature works well for Custom Resources, but not for standard Kubernetes resources (deployments, pods, replicasets, etc.). The reason is that the hosting cluster\u2019s controller manager creates pods for those resources on the hosting cluster itself, while the Kubestellar controller copies them to the Workload Execution Clusters (WECs). You can use any Custom Resource to wrap any Kubernetes object you want to send to the WECs. But if you have operators or controllers on the hosting cluster that work on the Custom Resource you want to send, make sure they don\u2019t create workloads on the hosting cluster that you did not intend to create there.</p> <p>In order to run this scenario using the post-create-hook method you need the raise the permissions for the kubeflex controller manager:</p> <pre><code>kubectl --context kind-kubeflex apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubeflex-manager-cluster-admin-rolebinding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: kubeflex-controller-manager\n  namespace: kubeflex-system\nEOF\n</code></pre> <p>To create a second WDS based on the hosting cluster, run the command:</p> <pre><code>kflex create wds2 -t host -p kubestellar\n</code></pre> <p>where the <code>-t host</code> option specifies a control plane of type <code>host</code>. You can only create on control plane of type <code>host</code>.</p> <p>Check that the kubestellar controller for wds2 is started:</p> <pre><code>kubectl get deployments.apps -n wds2-system kubestellar-controller-manager\n</code></pre> <p>If desired, you may remove the <code>kubeflex-manager-cluster-admin-rolebinding</code> after the kubestellar-controller-manager is started, with the command <code>kubectl --context kind-kubeflex delete clusterrolebinding kubeflex-manager-cluster-admin-rolebinding</code></p> <p>For this example, we use the <code>AppWrapper</code> custom resource defined in the multi cluster app dispatcher project.</p> <p>Run the following comamand to give permission for the Klusterlet to operate on your cluster resource.</p> <pre><code>clusters=(cluster1 cluster2);\nfor cluster in \"${clusters[@]}\"; do\nkubectl --context ${cluster} apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: appwrappers-access\nrules:\n- apiGroups: [\"mcad.ibm.com\"]\n  resources: [\"appwrappers\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: klusterlet-appwrappers-access\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: appwrappers-access\nsubjects:\n- kind: ServiceAccount\n  name: klusterlet-work-sa\n  namespace: open-cluster-management-agent\nEOF\ndone\n</code></pre> <p>This step will be eventually automated, see this issue for more details.</p> <p>Apply the appwrapper CRD to wds2:</p> <pre><code>kubectl --context wds2 apply -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.33.0/config/crd/bases/mcad.ibm.com_appwrappers.yaml\n</code></pre> <p>Now apply an appwrapper CR to wds2:</p> <pre><code>kubectl --context wds2 apply -f  https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.33.0/test/yaml/0001-aw-generic-deployment-3.yaml\n</code></pre> <p>Label the CRD and the CR:</p> <pre><code>kubectl --context wds2 label crd appwrappers.mcad.ibm.com app.kubernetes.io/part-of=my-appwrapper-app\nkubectl --context wds2 label appwrappers 0001-aw-generic-deployment-3 app.kubernetes.io/part-of=my-appwrapper-app\n</code></pre> <p>Finally, apply the placement:</p> <pre><code>kubectl --context wds2 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: Placement\nmetadata:\n  name: aw-placement\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/part-of\":\"my-appwrapper-app\"}\nEOF\n</code></pre> <p>Check that the app wrapper has been delivered to both clusters:</p> <pre><code>kubectl --context cluster1 get appwrappers\nkubectl --context cluster2 get appwrappers\n</code></pre>"},{"location":"v0.20/examples/#scenario-3-multi-cluster-workload-deployment-with-helm","title":"Scenario 3 - multi-cluster workload deployment with helm","text":"<p>This scenario proceeds from the state established by the common setup.</p> <p>Create a placement for the helm chart app:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: Placement\nmetadata:\n  name: postgres-placement\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\n      \"app.kubernetes.io/managed-by\": Helm,\n      \"app.kubernetes.io/instance\": postgres}\nEOF\n</code></pre> <p>Note that helm sets <code>app.kubernetes.io/instance</code> to the name of the installed release.</p> <p>Create and label the namespace and install the chart:</p> <pre><code>kubectl --context wds1 create ns postgres-system\nkubectl --context wds1 label ns postgres-system app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\nhelm --kube-context wds1 install -n postgres-system postgres oci://registry-1.docker.io/bitnamicharts/postgresql\n</code></pre> <p>Verify that statefulset has been created in both clusters</p> <pre><code>kubectl --context cluster1 get statefulsets -n postgres-system\nkubectl --context cluster2 get statefulsets -n postgres-system\n</code></pre>"},{"location":"v0.20/examples/#optional-propagate-helm-metadata-secret-to-managed-clusters","title":"[Optional] Propagate helm metadata Secret to managed clusters","text":"<p>Run \"helm list\" on the wds1:</p> <pre><code>$ helm --kube-context wds1 list -n postgres-system\nNAME            NAMESPACE       REVISION        UPDATED                                 STATUS       CHART                    APP VERSION\npostgres        postgres-system 1               2023-10-31 13:39:52.550071 -0400 EDT    deployed     postgresql-13.2.0        16.0.0\n</code></pre> <p>And try that on the managed clusters</p> <pre><code>$ helm list --kube-context cluster1 -n postgres-system\n: returns empty\n$ helm list --kube-context cluster2 -n postgres-system\n: returns empty\n</code></pre> <p>This is because Helm creates a <code>Secret</code> object to hold its metadata about a \"release\" (chart instance) but Helm does not apply the usual labels to that object, so it is not selected by the <code>Placement</code> above and thus does not get delivered. The workload is functioning in the WECs, but <code>helm list</code> does not recognize its handiwork there. That labeling could be done for example with:</p> <pre><code>kubectl --context wds1 label secret -n postgres-system $(kubectl --context wds1 get secrets -n postgres-system -l name=postgres -l owner=helm  -o jsonpath='{.items[0].metadata.name}') app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\n</code></pre> <p>Verify that the chart shows up on the managed clusters:</p> <pre><code>helm list --kube-context cluster1 -n postgres-system\nhelm list --kube-context cluster2 -n postgres-system\n</code></pre> <p>Implementing this in a controller for automated propagation of helm metadata is tracked in this issue.</p>"},{"location":"v0.20/examples/#scenario-4-singleton-status","title":"Scenario 4 - Singleton status","text":"<p>This scenario proceeds from the state established by the common setup.</p> <p>This scenario shows how to get the full status updated when setting <code>wantSingletonReportedState</code> in the placement. This still an experimental feature.</p> <p>Apply a placement with the <code>wantSingletonReportedState</code> flag set:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: Placement\nmetadata:\n  name: nginx-singleton-placement\nspec:\n  wantSingletonReportedState: true\n  clusterSelectors:\n  - matchLabels: {\"name\":\"cluster1\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-singleton\"}\nEOF\n</code></pre> <p>Apply a new deployment for the singleton placement:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-singleton-deployment\n  labels:\n    app.kubernetes.io/name: nginx-singleton\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that the status is available in wds1 for the deployment by running the command:</p> <pre><code>kubectl --context wds1 get deployments nginx-singleton-deployment -o yaml\n</code></pre> <p>Finally, scale the deployment from 1 to 2 replicas in wds1:</p> <pre><code>kubectl --context wds1 scale deployment nginx-singleton-deployment --replicas=2\n</code></pre> <p>and verify that replicas has been updated in cluster1 and wds1:</p> <pre><code>kubectl --context cluster1 get deployment nginx-singleton-deployment\nkubectl --context wds1 get deployment nginx-singleton-deployment\n</code></pre>"},{"location":"v0.20/examples/#scenario-5-resiliency-testing","title":"Scenario 5 - Resiliency testing","text":"<p>This is a test that you can do after finishing Scenario 1.</p> <p>TODO: rewrite this so that it makes sense after Scenario 4.</p> <p>Bring down the control plane: stop and restart wds1 and imbs1 API servers, KubeFlex and KubeStellar controllers:</p> <p>First stop all:</p> <pre><code>kubectl --context kind-kubeflex scale deployment -n wds1-system kube-apiserver --replicas=0\nkubectl --context kind-kubeflex scale statefulset -n imbs1-system vcluster --replicas=0\nkubectl --context kind-kubeflex scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=0\nkubectl --context kind-kubeflex scale deployment -n wds1-system kubestellar-controller-manager --replicas=0\n</code></pre> <p>Then restart all:</p> <pre><code>kubectl --context kind-kubeflex scale deployment -n wds1-system kube-apiserver --replicas=1\nkubectl --context kind-kubeflex scale statefulset -n imbs1-system vcluster --replicas=1\nkubectl --context kind-kubeflex scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=1\nkubectl --context kind-kubeflex scale deployment -n wds1-system kubestellar-controller-manager --replicas=1\n</code></pre> <p>Wait for about a minute for all pods to restart, then apply a new placement:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: Placement\nmetadata:\n  name: nginx-res-placement\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-res\"}\nEOF\n</code></pre> <p>and a new workload:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx-res\n  name: nginx-res\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-res-deployment\n  namespace: nginx-res\n  labels:\n    app.kubernetes.io/name: nginx-res\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-res\n  template:\n    metadata:\n      labels:\n        app: nginx-res\n    spec:\n      containers:\n      - name: nginx-res\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that deployment has been created in both clusters</p> <pre><code>kubectl --context cluster1 get deployments -n nginx-res\nkubectl --context cluster2 get deployments -n nginx-res\n</code></pre>"},{"location":"v0.20/examples/#scenario-6-multi-cluster-workload-deployment-of-app-with-serviceaccount-with-argocd","title":"Scenario 6 - multi-cluster workload deployment of app with ServiceAccount with ArgoCD","text":"<p>This scenario is something you can do after the common setup.</p> <p>Before running this scenario, install ArgoCD on the hosting cluster and configure it work with the WDS as outlined here.</p> <p>Including a ServiceAccount tests whether there will be a controller fight over a token Secret for that ServiceAccount, which was observed in some situations with older code.</p> <p>Apply the following placement to wds1:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: Placement\nmetadata:\n  name: argocd-sa-placement\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"argocd.argoproj.io/instance\":\"nginx-sa\"}\nEOF\n</code></pre> <p>Switch context to hosting cluster and argocd namespace (this is required by argo to create an app with the CLI)</p> <pre><code>kubectl config use-context kind-kubeflex\nkubectl config set-context --current --namespace=argocd\n</code></pre> <p>Create a new application in ArgoCD:</p> <pre><code>argocd app create nginx-sa --repo https://github.com/pdettori/sample-apps.git --path nginx --dest-server https://wds1.wds1-system --dest-namespace nginx-sa\n</code></pre> <p>Open browser to Argo UI:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Open the app <code>nginx-sa</code> and sync it by clicking the \"sync\" button and then \"synchronize\".</p> <p>Alternatively, use the CLI to sync the app:</p> <pre><code>argocd app sync nginx-sa\n</code></pre> <p>Finally, check if the app has been deployed to the two clusters.</p> <pre><code>kubectl --context cluster1 -n nginx-sa get deployments,sa,secrets\nkubectl --context cluster2 -n nginx-sa get deployments,sa,secrets\n</code></pre> <p>Repeat multiple syncing on Argo and verify that extra secrets for the service acccount are not created both wds1 and clusters:</p> <pre><code>kubectl --context wds1 -n nginx-sa get secrets\nkubectl --context cluster1 -n nginx-sa get secrets\nkubectl --context cluster2 -n nginx-sa get secrets\n</code></pre>"},{"location":"v0.20/packaging/","title":"Packaging and Delivery","text":""},{"location":"v0.20/packaging/#outline-of-github-repositories","title":"Outline of GitHub repositories","text":"<p>The following is a graph of the GitHub repositories in the <code>kubestellar</code> GitHub organization and the dependencies among them. The repo at the tail of an arrow depends on the repo at the head of the arrow.</p> <p>Note that to use KubeStellar, all of the repos in this graph are involved.</p> <pre><code>flowchart LR\n    kubestellar --&gt; kubeflex\n    kubeflex    --&gt; kubestellar\n    ocm-status-addon     --&gt; kubestellar\n    ocm-transport-plugin --&gt; kubestellar\n</code></pre> <p>Note: the <code>ocm-status-addon</code> repo is in the process of being moved into the kubestellar GitHub organization; the actual dependency right now is on a private repo.</p>"},{"location":"v0.20/packaging/#kubeflex","title":"KubeFlex","text":"<p>See the GitHub repo.</p>"},{"location":"v0.20/packaging/#status-addon","title":"status-addon","text":"<p>The status-addon repo is the source of a RedHat-style operator.</p> <p>The operator is delivered by a Helm chart at <code>quay.io/pdettori/status-addon-chart</code>. Chart versioning? How is the chart built, pushed?</p> <p>There is also a container image involved.</p>"},{"location":"v0.20/packaging/#ocm-transport-plugin","title":"OCM Transport Plugin","text":"<p>.. to be written ...</p>"},{"location":"v0.20/packaging/#kubestellar","title":"KubeStellar","text":""},{"location":"v0.20/packaging/#outline-of-publishing","title":"Outline of publishing","text":"<pre><code>flowchart LR\n    kcm_image[KCM container image] --&gt; github\n    hc_image[published KCM Helm Chart] --&gt; github\n    hc_image -.-&gt; kcm_image\n    PostCreateHook --&gt; github\n    PostCreateHook -.-&gt; hc_image\n</code></pre> <p>The dashed dependencies are at run time, not build time.</p> <p>\"KCM\" is the KubeStellar controller-manager.</p> <p>NOTE: at the present level of development, all versions are hand coded and there is only one public version. This will obviously have to change.</p>"},{"location":"v0.20/packaging/#local-copy-of-kubestellar-git-repo","title":"Local copy of KubeStellar git repo","text":"<p>NOTE: Because of a restriction in one of the code generators that we use, a contributor needs to have their local copy of the git repo in a directory whose pathname ends with the Go package name --- that is, ends with <code>/github.com/kubestellar/kubestellar</code>.</p>"},{"location":"v0.20/packaging/#derived-files","title":"Derived files","text":"<p>Some files in the kubestellar repo are derived from other files there. Contributors are responsible for invoking the commands to (re)derive the derived files as necessary.</p> <p>Some of these derived files are derived by standard generators from the Kubernetes milieu. A contributor can use the following command to make all of those, or use the individual <code>make</code> commands described in the following subsubsections to update particular subsets.</p> <pre><code>make all-generated\n</code></pre> <p>The following command, which we aspire to check in CI, checks whether all those derived files have been correctly derived. It must be invoked in a state where the <code>git status</code> is clean, or at least the dirty files are irrelevant; the current commit is what is checked. This command has side-effects on the filesystem like <code>make all-generated</code>.</p> <pre><code>hack/verify-codegen.sh\n</code></pre>"},{"location":"v0.20/packaging/#files-generated-by-controller-gen","title":"Files generated by controller-gen","text":"<ul> <li> <p><code>make manifests</code> generates the CustomeResourceDefinition files,   which exist in two places:   config/crd/bases and   pkg/crd/files.</p> </li> <li> <p><code>make generate</code> generates the deep copy code, which exists in   <code>zz_generated.deepcopy.go</code> next to the API source.</p> </li> </ul>"},{"location":"v0.20/packaging/#files-generated-by-code-generator","title":"Files generated by code-generator","text":"<p>The files in pkg/generated are generated by k/code-generator. This generation is done at development time by the command <code>make codegenclients</code>.</p>"},{"location":"v0.20/packaging/#kubestellar-controller-manager-container-image","title":"KubeStellar controller-manager container image","text":"<p>KubeStellar has one container image, for what is called the KubeStellar controller-manager. For each WDS, KubeStellar has a pod running that image. It installs the needed custom resource definition objects if they are not already present, and is a controller-manager hosting the per-WDS controllers (placement controller, status controller, and transport controller(TODO: link to doc)).</p> <p>The image repository is <code>ghcr.io/kubestellar/kubestellar/kubestellar-operator</code>. There is currently just one supported tag there, <code>0.20.0-alpha.1</code>. That was built from the git repo contents with the tag <code>v0.20.0-alpha.1</code>.</p> <p><code>make ko-build-push</code> will build and push that image, tagging it with the value of the <code>make</code> variable <code>IMAGE_TAG</code> (which defaults to <code>0.20.0-alpha.1</code>).</p> <p><code>make ko-build-local</code> will make a local image for just the local platform. This is used in local testing.</p>"},{"location":"v0.20/packaging/#kubestellar-controller-manager-helm-chart","title":"KubeStellar controller-manager Helm Chart","text":"<p>There is a Helm chart that is designed to be instantiated in a KubeFlex hosting cluster, once per WDS. The focus of the chart is getting the KubeStellar controller-manager installed.</p> <p>The source for the Helm chart is in core-helm-chart. <code>make chart</code> (re)derives it from local sources. This is not included in <code>make all-generated</code>.</p> <p>This chart creates (among other things) a <code>Deployment</code> object that runs a container from the image <code>ghcr.io/kubestellar/kubestellar/kubestellar-operator:0.20.0-alpha.1</code>.</p> <p>The chart is published at the OCI repository <code>ghcr.io/kubestellar/kubestellar/kubestellar-operator-chart</code>.  What stores the chart there?</p>"},{"location":"v0.20/packaging/#kubeflex-postcreatehooks","title":"KubeFlex PostCreateHooks","text":"<p>There are two <code>PostCreateHook</code> objects defined in config/postcreate-hooks. What uses these copies of those hook definitions?</p> <ul> <li><code>ocm.yaml</code> adds <code>clusteradm</code> by running a container using the image <code>quay.io/kubestellar/clusteradm:0.7.2</code> which is built from the OCM source using the script build-clusteradm-image.sh.</li> <li><code>kubestellar.yaml</code> runs container image <code>quay.io/kubestellar/helm:v3.14.0</code> (which is built from the Helm source by a process that we need to document) to instantiate the chart from <code>oci://ghcr.io/kubestellar/kubestellar/kubestellar-operator-chart</code> with chart version <code>0.20.0-alpha.1</code>, which is built by (what?) from (what? probably answered above).</li> </ul>"},{"location":"v0.20/png-files-readme/","title":"How to edit these pictures","text":"<p>The pictures have been created with draw.io, and have been saved in the editable png format. You can use draw.io to modify and save back these pictures using the png editable format.</p>"},{"location":"v0.20/pre-reqs/","title":"KubeStellar prerequisites","text":"<p>The following prerequisites are required.</p> <p>Use the check_pre_req script to quickly check which pre-requisites are already installed:</p> <pre><code>$ hack/check_pre_req.sh\nChecking pre-requisites for using KubeStellar:\n\u2714 Docker\n\u2714 kubectl\n\u2714 KubeFlex\n\u2714 OCM CLI\n\u2714 Helm\nChecking additional pre-requisites for running the examples:\n\u2714 Kind\nX ArgoCD CLI\nChecking pre-requisites for building KubeStellar:\n\u2714 GNU Make\n\u2714 Go\n\u2714 KO\n</code></pre>"},{"location":"v0.20/pre-reqs/#for-using-kubestellar","title":"For Using KubeStellar","text":"<ul> <li> <p>kubeflex version 0.4.1 or higher     To install kubeflex go to https://github.com/kubestellar/kubeflex/blob/main/docs/users.md#installation. To upgrade from an existing installation, follow these instructions. At the end of the install make sure that the kubeflex CLI, kflex, is in your path.</p> </li> <li> <p>OCM CLI (clusteradm)     To install OCM CLI use:</p> <pre><code>curl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh | bash\n</code></pre> <p>Note that the default installation of clusteradm will install in /usr/local/bin which will require root access. If you prefer to avoid root, you can specify an alternative installation path using the INSTALL_DIR environment variable, as follows:</p> <pre><code>mkdir -p ocm\nexport INSTALL_DIR=\"$PWD/ocm\"\ncurl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh | bash\nexport PATH=$PWD/ocm:$PATH\n</code></pre> <p>At the end of the install make sure that the OCM CLI, clusteradm, is in your path.</p> </li> <li> <p>helm - to deploy the kubestellar and kubeflex charts</p> </li> <li>kubectl - to access the kubernetes clusters</li> <li>docker (or compatible docker engine that works with kind)</li> </ul>"},{"location":"v0.20/pre-reqs/#for-running-the-examples","title":"For running the examples","text":"<ul> <li>kind - to create a few small kubernetes clusters</li> <li>argocd - for the examples that use it</li> </ul>"},{"location":"v0.20/pre-reqs/#for-building-kubestellar","title":"For Building KubeStellar","text":"<ul> <li>go version 1.20 or higher - to build kubestellar</li> <li>make - to build kubestellar and create the kubestellar image</li> <li>ko - to create the kubestellar image</li> </ul>"},{"location":"v0.20/thirdparties/","title":"Third Parties Integrations","text":""},{"location":"v0.20/thirdparties/#install-and-configure-argocd","title":"Install and configure ArgoCD","text":"<p>Install ArgoCD on kind-kubeflex:</p> <pre><code>kubectl --context kind-kubeflex create namespace argocd\nkubectl --context kind-kubeflex apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre> <p>Install CLI:</p> <p>on MacOS:</p> <pre><code>brew install argocd\n</code></pre> <p>on Linux:</p> <pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre> <p>Configure Argo to work with the ingress installed in the hosting cluster:</p> <pre><code>kubectl --context kind-kubeflex apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-server-ingress\n  namespace: argocd\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: argocd.localtest.me\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: argocd-server\n            port:\n              name: https\nEOF\n</code></pre> <p>Open a browser to ArgoCD console:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Note: if you are working on a VM via SSH, just take the IP of the VM (VM_IP) and add the line ' argocd.localtest.me' to your '/etc/hosts' file, replacing  with the actual IP of your desktop. <p>Get the password for Argo with:</p> <pre><code>kubectl config use-context kind-kubeflex\nargocd admin initial-password -n argocd\n</code></pre> <p>Login into the ArgoCD console with <code>admin</code> and the password just retrieved. Type the following on a shell terminal in your desktop (or just enter the address https://argocd.localtest.me:9443 on your browser):</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Also, login with the argocd CLI with the same credentials.</p> <pre><code>argocd login --insecure argocd.localtest.me:9443\n</code></pre> <p>Add the <code>wds1</code> space as cluster to ArgoCD:</p> <pre><code>CONTEXT=wds1\nkubectl config view --minify --context=${CONTEXT} --flatten &gt; /tmp/${CONTEXT}.kubeconfig\nkubectl config --kubeconfig=/tmp/${CONTEXT}.kubeconfig set-cluster ${CONTEXT}-cluster --server=https://${CONTEXT}.${CONTEXT}-system 2&gt;/dev/null\nkubectl config use-context kind-kubeflex\nARGO_SERVER_POD=$(kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o jsonpath={.items[0].metadata.name})\nkubectl cp /tmp/${CONTEXT}.kubeconfig -n argocd ${ARGO_SERVER_POD}:/tmp\nPASSWORD=$(argocd admin initial-password -n argocd | cut -d \" \" -f 1)\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd login argocd-server.argocd --username admin --password $PASSWORD --insecure\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd cluster add ${CONTEXT} --kubeconfig /tmp/${CONTEXT}.kubeconfig -y\n</code></pre> <p>Configure Argo to label resources with the \"argocd.argoproj.io/instance\" label:</p> <pre><code>kubectl --context kind-kubeflex patch cm -n argocd argocd-cm -p '{\"data\": {\"application.instanceLabelKey\": \"argocd.argoproj.io/instance\"}}'\n</code></pre>"}]}