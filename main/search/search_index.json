{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p> Did you know kubernetes manages multiple nodes but does not manage multiple clusters out of the box?  KubeStellar adds multi-cluster support to Kubernetes distributions. Distinguishing features of KubeStellar </p> <ul> <li> multi-cluster down-syncing deploy, configure, and collect status across pre-existing clusters</li> <li>up-syncing from remote clusters (return any object, not just status)</li> <li>lightweight logical cluster support (KubeFlex, kcp, kind, etc.)</li> <li>resiliency to support disconnected operation and intermittent connectivity</li> </ul> <p></p> <p> Additional features </p> <ul> <li>non-wrapped / kubernetes-object-native denaturing (enables hierarchy) (no requirement to wrap objects)</li> <li>rule-based customization (grouping) - automate the customization of your deployments</li> <li>status summarization - summarize the status returned from all your deployments</li> <li>scalability - scale to a large number of objects, overcoming default Kubernetes limitations</li> </ul> <p>Learn more about KubeStellar</p> <p>Try our QuickStart</p> <p>Instantly get access to our documents and meeting invites http://kubestellar.io/joinus</p> <p></p> <p> </p> <p></p>"},{"location":"readme/","title":"Readme","text":""},{"location":"readme/#multi-cluster-configuration-management-for-edge-multi-cloud-and-hybrid-cloud","title":"Multi-cluster Configuration Management for Edge, Multi-Cloud, and Hybrid Cloud","text":""},{"location":"readme/#overview","title":"Overview","text":"<p>KubeStellar is an opensource project focused on concerns arising from multi-cluster configuration management for edge, multi-cloud, and hybrid cloud use cases:</p> <ul> <li>Hierarchy, infrastructure &amp; platform, roles &amp; responsibilities, integration architecture, security issues</li> <li>Runtime in[ter]dependence: An edge location may need to operate independently of the center and other edge locations\u200b</li> <li>Non-namespaced objects: need general support</li> <li>Cardinality of destinations: A source object may propagate to many thousands of destinations. </li> </ul>"},{"location":"readme/#goals","title":"Goals","text":"<ul> <li>Collaboratively design a component set similar to those found in the current kcp TMC implementation (dedicated Workspace type, Where Resolver, syncer-like mechanism, edge placement object definition, status collection strategy, etc.)</li> <li>Specify a multi-phased proof-of-concept inclusive of component architecture, interfaces, and example workloads</li> <li>Validate phases of proof-of-concept with kcp, Kube SIG-Multicluster, and CNCF community members interested in Edge</li> </ul>"},{"location":"readme/#areas-of-exploration","title":"Areas of exploration","text":"<ul> <li>Desired placement expression\u200b: Need a way for one center object to express large number of desired copies\u200b</li> <li>Scheduling/syncing interface\u200b: Need something that scales to large number of destinations\u200b</li> <li>Rollout control\u200b: Client needs programmatic control of rollout, possibly including domain-specific logic\u200b</li> <li>Customization: Need a way for one pattern in the center to express how to customize for all the desired destinations\u200b</li> <li>Status from many destinations\u200b: Center clients may need a way to access status from individual edge copies</li> <li>Status summarization\u200b: Client needs a way to specify how statuses from edge copies are processed/reduced along the way from edge to center\u200b.</li> </ul>"},{"location":"readme/#roadmap-for-the-project","title":"Roadmap for the Project","text":"<p>We have defined and largely completed the PoC2023q1. The current activity is refining the definition of, and producing, the PoC2023q4. Goals not addressed in that PoC are to be explored later.</p>"},{"location":"readme/#quickstart","title":"QuickStart","text":"<p>Checkout our QuickStart Guide</p>"},{"location":"readme/#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"readme/#getting-in-touch","title":"Getting in touch","text":"<p>There are several ways to communicate with us:</p> <p>Instantly get access to our documents and meeting invites http://kubestellar.io/joinus</p> <ul> <li>The <code>#kubestellar-dev</code> channel in the Kubernetes Slack workspace</li> <li>Our mailing lists:<ul> <li>kubestellar-dev for development discussions</li> <li>kubestellar-users for discussions among users and potential users</li> </ul> </li> <li>Subscribe to the community calendar for community meetings and events<ul> <li>The kubestellar-dev mailing list is subscribed to this calendar</li> </ul> </li> <li>See recordings of past KubeStellar community meetings on YouTube</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kubestellar-dev mailing list can view this drive</li> </ul> </li> <li>Read our documentation</li> <li>Follow us on:</li> <li>LinkedIn - #kubestellar</li> <li>Medium - kubestellar.medium.com</li> </ul>"},{"location":"readme/#contributors","title":"\u2764\ufe0f Contributors","text":"<p>Thanks go to these wonderful people:</p> Jun Duan\ud83d\udc40 Braulio Dumba\ud83d\udc40 Mike Spreitzer\ud83d\udc40 Paolo Dettori\ud83d\udc40 Andy Anderson\ud83d\udc40 Franco Stellari\ud83d\udc40 Ezra Silvera\ud83d\udc40 Bob Filepp\ud83d\udc40 Alexei Karve\ud83d\udc40 Maria Camila Ruiz Cardenas\ud83d\udc40 Aleksander Slominski\ud83d\udc40 Aashni Manroa\ud83d\udc40 Kevin Roche\ud83d\udc40 Nick Masluk\ud83d\udc40 Francois Abel\ud83d\udc40"},{"location":"Coding%20Milestones/PoC2023q1/coding-milestone-invite-q1/","title":"Invitation","text":"<p>Dear Contributors,</p> <p>We are excited to invite you to join the first KubeStellar opensource community coding sprint. We will be focus on several key projects that are critical to the development of state-based edge solutions. Our collective work will be showcased to the opensource community on Thursday, April 27th.</p> <p>This coding sprint will provide a great opportunity for you to showcase your skills, learn new techniques, and collaborate with other experienced engineers in the KubeStellar community. We believe that your contributions will be invaluable in helping us achieve our goals and making a lasting impact in the field of state-based edge technology.</p> <p>The coding sprint will be dedicated to completing the following workload management elements:</p> <ul> <li>Implementing a Where Resolver and a Placement Translator, including customization options,</li> <li>Incorporating existing customization API into the KubeStellar repo,</li> <li>Investigating implementation of a status summarizer, starting with basic implicit status, and later adding programmed summarization,</li> <li>Updating summarization API and integrating it into the KubeStellar repo,</li> <li>Defining the API for identifying associated objects and its interaction with summarization, and implementing these,</li> <li>Streamlining the creation of workload management workspaces,</li> <li>Examining the use of Postgresql through Kine instead of etcd for scalability,</li> <li>Revising the milestone outline with regards to defining bootstrapping and support for cluster-scoped resources.</li> </ul> <p>In addition to workload management, we will also be working on inventory management for the demo, as well as designing various demo scenarios, including a baseline demo with kubectl, demos with ArgoCD, FluxCD, and the European Space Agency (ESA). To support the engineers and demonstrations we will also need to automate the process of creating infrastructure, deploying demo pieces and instrumentation, bootstrapping, running scenarios, and collecting data.</p> <p>If you are interested in joining us for this exciting coding sprint, please check out our 'good first issue' list, or slack me @Andy Anderson so I can connect you with others in your area of interest.  There is a place for every skillset to contribute. Not quite sure?  You can join our bi-weekly community meetings to watch our progress.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/","title":"Commands","text":"<p>This PoC includes two sorts of commands for users to use. Most are executables designed to be accessed in the usual ways: by appearing in a directory on the user's <code>$PATH</code> or by the user writing a pathname for the executable on the command line. The one exception is the bootstrap script, which is designed to be fetched from github and fed directly into <code>bash</code>.</p> <p>There are two ways of deploying kcp and the central KubeStellar components: (1) as processes on a machine of supported OS and ISA, and (2) as workload in a Kubernetes (possibly OpenShift) cluster.  The latter is newer and not yet well exercised.</p> <p>This document describes both commands that a platform administrator uses and commands that a platform user uses.</p> <p>Most of these executables require that the kcp server be running and that <code>kubectl</code> is configured to use the kubeconfig file produced by <code>kcp start</code>. That is: either <code>$KUBECONFIG</code> holds the pathname of that kubeconfig file, its contents appear at <code>~/.kube/config</code>, or <code>--kubeconfig $pathname</code> appears on the command line.  The exceptions are the bootstrap script, the kcp control script that runs before starting the kcp server, and the admin commands for deploying KubeStellar in a Kubernetes cluster.</p> <p>NOTE: most of the kubectl plugin usages described here certainly or potentially change the setting of which kcp workspace is \"current\" in your chosen kubeconfig file; for this reason, they are not suitable for executing concurrently with anything that depends on that setting in that file.  The exceptions are the admin commands for deploying KubeStellar in a Kubernetes cluster.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#bare-process-deployment","title":"Bare process deployment","text":""},{"location":"Coding%20Milestones/PoC2023q1/commands/#kcp-process-control","title":"kcp process control","text":"<p>KubeStellar has some commands to support situations in which some clients of the kcp server need to connect to it by opening a connection to a DNS domain name rather than an IP address that the server can bind its socket to.  The key idea is using the <code>--tls-sni-cert-key</code> command line flag of <code>kcp start</code> to configure the server to respond with a bespoke server certificate in TLS handshakes in which the client addresses the server by a given domain name.</p> <p>These commands are used separately from starting the kcp server and are designed so that they can be used multiple times if there are multiple sets of clients that need to use a distinct domain name. Starting the kcp server is not described here, beyond the particular consideration needed for the <code>--tls-sni-cert-key</code> flag, because it is otherwise ordinary.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-ensure-kcp-server-creds","title":"kubestellar-ensure-kcp-server-creds","text":""},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-ensure-kcp-server-creds-pre-reqs","title":"kubestellar-ensure-kcp-server-creds pre-reqs","text":"<p>The <code>kubestellar-ensure-kcp-server-creds</code> command requires that Easy-RSA is installed.  As outlined in https://easy-rsa.readthedocs.io/en/latest/#obtaining-and-using-easy-rsa, this involves selecting a release archive from the list on GitHub, unpacking it, and adding the EasyRSA directory to your <code>$PATH</code>; <code>easyrsa</code> is a bash script, so you do not need to worry about building or fetching a binary specific to your OS or computer architecture.</p> <p>Easy-RSA uses OpenSSL, so you will need that installed too.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-ensure-kcp-server-creds-usage","title":"kubestellar-ensure-kcp-server-creds usage","text":"<p>This command is given exactly one thing on the command line, a DNS domain name.  This command creates --- or re-uses if it finds already existing --- a private key and public X.509 certificate for the kcp server to use.  The certificate has exactly one SubjectAlternativeName, which is of the DNS form and specifies the given domain name.  For example: <code>kubestellar-ensure-kcp-server-creds foo.bar</code> creates a certificate with one SAN, commonly rendered as <code>DNS:foo.bar</code>.</p> <p>This command uses a Public Key Infrastructure (PKI) and Certificate Authority (CA) implemented by easy-rsa, rooted at the subdirectory <code>pki</code> of the current working directory.  This command will create the PKI if it does not already exist, and will initialize the CA if that has not already been done.  The CA's public certificate appears at the usual place for easy-rsa: <code>pki/ca.crt</code>.</p> <p>This command prints some stuff --- mostly progress remarks from easy-rsa --- to stderr and prints one line of results to stdout.  The <code>bash</code> shell will parse that one line as three words.  Each is an absolute pathname of one certificate or key.  The three are as follows.</p> <ol> <li>The CA certificate for the client to use in verifying the server.</li> <li>The X.509 certificate for the server to put in its ServerHello in a    TLS handshake in which the ClientHello has a Server Name Indicator    (SNI) that matches the given domain name.</li> <li>The private key corresponding to that server certificate.</li> </ol> <p>Following is an example of invoking this command and examining its results.</p> <pre><code>bash-5.2$ eval pieces=($(kubestellar-ensure-kcp-server-creds yep.yep))\nRe-using PKI at /Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki\nRe-using CA at /Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/private/ca.key\nAccepting existing credentials\nbash-5.2$ echo ${pieces[0]}\n/Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/ca.crt\nbash-5.2$ echo ${pieces[1]}\n/Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/issued/kcp-DNS-yep.yep.crt\nbash-5.2$ echo ${pieces[2]}\n/Users/mspreitz/go/src/github.com/kubestellar/kubestellar/pki/private/kcp-DNS-yep.yep.key\n</code></pre> <p>Following is an example of using those results in launching the kcp server.  The <code>--tls-sni-cert-key</code> flag can appear multiple times on the command line, configuring the server to respond in a different way to each of multiple different SNIs.</p> <pre><code>bash-5.2$ kcp start --tls-sni-cert-key ${pieces[1]},${pieces[2]} &amp;&gt; /tmp/kcp.log &amp;\n[1] 66974\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#wait-and-switch-domain","title":"wait-and-switch-domain","text":"<p>This command is for using after the kcp server has been launched. Since the <code>kcp start</code> command really means <code>kcp run</code>, all usage of that server has to be done by concurrent processes.  The <code>wait-and-switch-domain</code> command bundles two things: waiting for the kcp server to start handling kubectl requests, and making an alternate kubeconfig file for a set of clients to use.  This command is pointed at an existing kubeconfig file and reads it but does not write it; the alternate config file is written (its directory must already exist and be writable).  This command takes exactly six command line positional arguments, as follows.</p> <ol> <li>Pathname (absolute or relative) of the input kubeconfig.</li> <li>Pathname (absolute or relative) of the output kubeconfig.</li> <li>Name of the kubeconfig \"context\" that identifies what to replace.</li> <li>Domain name to put in the replacement server URLs.</li> <li>Port number to put in the replacement server URLs.</li> <li>Pathname (absolute or relative) of a file holding the CA    certificate to put in the alternate kubeconfig file.</li> </ol> <p>Creation of the alternate kubeconfig file starts by looking in the input kubeconfig file for the \"context\" with the given name, to find the name of a \"cluster\".  The server URL of that cluster is examined, and its <code>protocol://host:port</code> prefix is extracted.  The alternate kubeconfig will differ from the input kubeconfig in the contents of the cluster objects whose server URLs start with that same prefix. There will be the following two differences.</p> <ol> <li>In the server URL's <code>protocol://host:port</code> prefix, the host will be    replaced by the given domain name and the port will be replaced by    the given port.</li> <li>The cluster will be given a <code>certificate-authority-data</code> that holds    the contents (base64 encoded, as usual) of the given CA certificate    file.</li> </ol> <p>Following is an example of using this command and examining the results.  The context and port number chosen work for the kubeconfig file that <code>kcp start</code> (kcp release v0.11.0) creates by default.</p> <pre><code>bash-5.2$ wait-and-switch-domain .kcp/admin.kubeconfig test.yaml root yep.yep 6443 ${pieces[0]}\nbash-5.2$ diff -w .kcp/admin.kubeconfig test.yaml\n4,5c4,5\n&lt;     certificate-authority-data: LS0...LQo=\n&lt;     server: https://192.168.something.something:6443\n---\n&gt;       certificate-authority-data: LS0...LQo=\n&gt;       server: https://yep.yep:6443\n8,9c8,9\n&lt;     certificate-authority-data: LS0...LQo=\n&lt;     server: https://192.168.something.something:6443/clusters/root\n---\n&gt;       certificate-authority-data: LS0...LQo=\n&gt;       server: https://yep.yep:6443/clusters/root\n</code></pre> <p>Following is an example of using the alternate kubeconfig file, in a context where the domain name \"yep.yep\" resolves to an IP address of the network namespace in which the kcp server is running.</p> <pre><code>bash-5.2$ KUBECONFIG=.kcp-yep.yep/admin.kubeconfig kubectl ws .\nCurrent workspace is \"root\".\n</code></pre> <p>Because this command reads the given kubeconfig file, it is important to invoke this command while nothing is concurrently writing it and while the caller reliably knows the name of a kubeconfig context that identifies what to replace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#switch-domain","title":"switch-domain","text":"<p>This command is the second part of <code>wait-and-switch-domain</code>: the part of creating the alternate kubeconfig file.  It has the same inputs and outputs and concurrency considerations.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-process-control","title":"KubeStellar process control","text":"<p>The <code>kubestellar</code> command has three subcommands, one to finish setup and two for process control.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubestellar [flags] subcommand [flags]\n</code></pre> <p>This command accepts the following command line flags, which can appear before and/or after the subcommand.  The <code>--log-folder</code> flag is only meaningful for the <code>start</code> subcommand. The <code>--local-kcp</code> flag is not meaningful for the <code>stop</code> subcommand. The <code>--ensure-imw</code> and  <code>--ensure-wmw</code> flags are only meaningful for the <code>start</code> or <code>init</code> subcommands.</p> <ul> <li><code>-V</code> or <code>--verbose</code>: calls for more verbose output.  This is a   binary choice, not a matter of degree.</li> <li><code>-X</code>: turns on echoing of script lines</li> <li><code>--log-folder $pathname</code>: says where to put the logs from the   controllers.  Will be <code>mkdir -p</code> if absent.  Defaults to   <code>${PWD}/kubestellar-logs</code>.</li> <li><code>--local-kcp $bool</code>: says whether to expect to find a local process   named \"kcp\".  Defaults to \"true\".</li> <li><code>--ensure-imw</code>: provide a comma separated list of pathnames for inventory workspaces, e.g. \"root:imw1,root:imw2\". Defaults to \"root:imw1\". To prevent the creation of any inventory workspace, then pass \"\".</li> <li><code>--ensure-wmw</code>: provide a comma separated list of pathnames for workload management workspaces, e.g. \"root:wmw1,root:imw2\". Defaults to \"root:wmw1\". To prevent the creation of any workload management workspace, then pass \"\".</li> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-init","title":"Kubestellar init","text":"<p>This subcommand is used after installation to finish setup and does the following five things.</p> <ol> <li> <p>Waits for the kcp server to be in service and the <code>root:compute</code>    workspace to be usable.</p> </li> <li> <p>Ensure that the edge service provider workspace (ESPW) exists and has the required contents.</p> </li> <li> <p>Ensure that the <code>root:compute</code> workspace has been extended with the RBAC objects that enable the syncer to propagate reported state for downsynced objects defined by the APIExport from that workspace of a subset of the Kubernetes API for managing containerized workloads.</p> </li> <li> <p>Ensure the existence of zero, one, or more inventory management workspaces depending on the value of <code>--ensure-imw</code> flag. Default is one inventory management workspaces at pathname \"root:imw1\".</p> </li> <li> <p>Ensure the existence of zero, one, or more workload management workspaces  depending on the value of <code>--ensure-wmw</code> flag. Default is one workload management workspaces at pathname \"root:wmw1\". The workload management workspaceshace APIBindings that import the namespaced Kubernetes resources (kinds of objects) for management of containerized workloads. At the completion of <code>kubestellar init</code> the current workspace will be \"root\".</p> </li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-start","title":"KubeStellar start","text":"<p>This subcommand is used after installation or process stops.</p> <p>This subcommand stops any running kubestellar controllers and then starts them all.  It also does the same thing as <code>kubestellar init</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-stop","title":"KubeStellar stop","text":"<p>This subcommand undoes the primary function of <code>kubestellar start</code>, stopping any running KubeStellar controllers.  It does not tear down the ESPW.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#deployment-into-a-kubernetes-cluster","title":"Deployment into a Kubernetes cluster","text":"<p>These commands administer a deployment of the central components --- the kcp server, PKI, and the central KubeStellar components --- in a Kubernetes cluster that will be referred to as \"the hosting cluster\". These commands are framed as \"kubectl plugins\" and thus need to be explicitly or implicitly given a kubeconfig file for the hosting cluster.</p> <p>You need a Kubernetes cluster with an Ingress controller deployed and configured in a way that does not terminate TLS connections (this abstinence is often called \"SSL passthrough\"). An OpenShift cluster would be one qualifying thing. Another would be an ordinary Kubernetes cluster with the nginx Ingress controller deployed and configured appropriately. Please note that special considerations apply when deploying an ingress controller in <code>kind</code>. See a fully worked example with kind and nginx. You will need to know the port number at which the Ingress controller is listening for HTTPS connections.</p> <p>IF your Kubernetes cluster has any worker nodes --- real or virtual --- with the x86_64 instruction set, they need to support the extended instruction set known as \"x64-64-v2\". If using hardware bought in the last 10 years, you can assume that is true. If using emulation, you need to make sure that your emulator is emulating that extended instruction set --- some emulators do not do this by default. See QEMU configuration recommendations, for example.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that gets to the Ingress controller's listening socket.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#deploy-to-cluster","title":"Deploy to cluster","text":"<p>Deployment is done with a \"kubectl plugin\" that is invoked as <code>kubectl kubestellar deploy</code> and creates a Helm \"release\" in the hosting cluster. As such, it relies on explicit (on the command line) or implicit (in environment variables and/or <code>~/.kube/config</code>) configuration needed to execute Helm commands. The following flags can appear on the command line, in any order.</p> <ul> <li><code>--openshift $bool</code>, saying whether the hosting cluster is an   OpenShift cluster.  If so then a Route will be created to the kcp   server; otherwise, an Ingress object will direct incoming TLS   connections to the kcp server.  The default is <code>false</code>.</li> <li><code>--external-endpoint $domain_name:$port</code>, saying how the kcp server   will be reached from outside the cluster.  The given domain name   must be something that the external clients will resolve to an IP   address where the cluster's Ingress controller or OpenShift router   will be listening, and the given port must be the corresponding TCP   port number.  For a plain Kubernetes cluster, this must be   specified.  For an OpenShift cluster this may be omitted, in which   case the command will (a) assume that the external port number is   443 and (b) extract the external hostname from the Route object   after it is created and updated by OpenShift.  FYI, that external   hostname will start with a string derived from the Route in the   chart (currently \"kubestellar-route-kubestellar\") and continue with   \".\" and then the ingress domain name for the cluster.</li> <li>a command line flag for the <code>helm upgrade</code> command. This includes   the usual control over namespace: you can set it on the command   line, otherwise the namespace that is current in your kubeconfig   applies.</li> <li><code>-X</code> turns on debug echoing of all the commands in the script that   implements this command.</li> <li><code>-h</code> prints a brief usage message and terminates with success.</li> </ul> <p>For example, to deploy to a plain Kubernetes cluster whose Ingress controller can be reached at <code>my-long-application-name.my-region.some.cloud.com:1234</code>, you would issue the following command.</p> <pre><code>kubectl kubestellar deploy --external-endpoint my-long-application-name.my-region.some.cloud.com:1234\n</code></pre> <p>The Helm chart takes care of setting up the KubeStellar MCCM core, accomplishing the same thing as the kubestellar start command above.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#fetch-kubeconfig-for-internal-clients","title":"Fetch kubeconfig for internal clients","text":"<p>To fetch a kubeconfig for use by clients inside the hosting cluster, use the <code>kubectl kubestellar get-internal-kubeconfig</code> command.  It takes the following on the command line.</p> <ul> <li><code>-o $output_pathname</code>, saying where to write the kubeconfig. This   must appear exactly once on the command line.</li> <li>a <code>kubectl</code> command line flag, for accessing the hosting   cluster. This includes the usual control over namespace.</li> <li><code>-X</code> turns on debug echoing of all the commands in the script that   implements this command.</li> <li><code>-h</code> prints a brief usage message and terminates with success.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#fetch-kubeconfig-for-external-clients","title":"Fetch kubeconfig for external clients","text":"<p>To fetch a kubeconfig for use by clients outside of the hosting cluster --- those that will reach the kcp server via the external endpoint specified in the deployment command --- use the <code>kubectl kubestellar get-external-kubeconfig</code> command.  It takes the following on the command line.</p> <ul> <li><code>-o $output_pathname</code>, saying where to write the kubeconfig. This   must appear exactly once on the command line.</li> <li>a <code>kubectl</code> command line flag, for accessing the hosting cluster.   This includes the usual control over namespace.</li> <li><code>-X</code> turns on debug echoing of all the commands in the script that   implements this command.</li> <li><code>-h</code> prints a brief usage message and terminates with success.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#fetch-a-log-from-a-kubestellar-runtime-container","title":"Fetch a log from a KubeStellar runtime container","text":"<p>The logs of each runtime container in the KubeStellar application pods can be access this way:</p> <pre><code>kubectl logs $(kubectl get pod --selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) -n kubestellar -c kcp\nkubectl logs $(kubectl get pod --selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) -n kubestellar -c init\nkubectl logs $(kubectl get pod --selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) -n kubestellar -c mailbox-controller\nkubectl logs $(kubectl get pod --selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) -n kubestellar -c where-resolver\nkubectl logs $(kubectl get pod --selector=app=kubestellar -o jsonpath='{.items[0].metadata.name}' -n kubestellar) -n kubestellar -c placement-translator\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#remove-deployment-to-a-kubernetes-cluster","title":"Remove deployment to a Kubernetes cluster","text":"<p>The deployment of kcp and KubeStellar as a Kubernetes workload is done by making a \"release\" (this is a technical term in Helm whose meaning might surprise you) of a Helm chart.  This release is named \"kubestellar\".  To undo the deployment, just use the Helm command to delete that release.</p> <pre><code>helm delete kubestellar\n</code></pre> <p>NOTE: this is a detail that might change in the future.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-platform-user-commands","title":"KubeStellar platform user commands","text":"<p>The remainder of the commands in this document are for users rather than administrators of the service that KubeStellar provides.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-release","title":"KubeStellar-release","text":"<p>This command just echoes the semantic version of the release used.  This command is only available in archives built for a release.  Following is an example usage.</p> <p><pre><code>kubestellar-release\n</code></pre> <pre><code>v0.2.3-preview\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-version","title":"Kubestellar-version","text":"<p>This executable prints information about itself captured at build time.  If built by <code>make</code> then this is information conveyed by the Makefile; otherwise it is the Kubernetes defaults.</p> <p>It will either print one requested property or a JSON object containing many.</p> <p><pre><code>kubestellar-version help\n</code></pre> <pre><code>Invalid component requested: \"help\"\nUsage: kubestellar-version [buildDate|gitCommit|gitTreeState|platform]\n</code></pre></p> <p><pre><code>kubestellar-version buildDate\n</code></pre> <pre><code>2023-05-19T02:54:01Z\n</code></pre></p> <p><pre><code>kubestellar-version gitCommit\n</code></pre> <pre><code>1747254b\n</code></pre></p> <p><pre><code>kubestellar-version\n</code></pre> <pre><code>{\"major\":\"1\",\"minor\":\"24\",\"gitVersion\":\"v1.24.3+kcp-v0.2.1-20-g1747254b880cb7\",\"gitCommit\":\"1747254b\",\"gitTreeState\":\"dirty\",\"buildDate\":\"2023-05-19T02:54:01Z\",\"goVersion\":\"go1.19.9\",\"compiler\":\"gc\",\"platform\":\"darwin/amd64\"}\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-synctargetlocation-pairs","title":"Creating SyncTarget/Location pairs","text":"<p>In this PoC, the interface between infrastructure and workload management is inventory API objects.  Specifically, for each edge cluster there is a unique pair of SyncTarget and Location objects in a so-called inventory management workspace.  These kinds of objects were originally defined in kcp TMC, and now there is a copy of those definitions in KubeStellar.  It is the definitions in KubeStellar that should be referenced.  Those are in the Kubernetes API group <code>edge.kubestellar.io</code>, and they are exported from the KubeStellar Core Space (KCS)) (the kcp workspace named <code>root:espw</code>).</p> <p>The following command helps with making that SyncTarget and Location pair and adding the APIBinding to <code>root:espw:edge.kubestellar.io</code> if needed.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar ensure location flag... objname labelname=labelvalue...\n</code></pre> <p>Here <code>objname</code> is the name for the SyncTarget object and also the name for the Location object.  This command ensures that these objects exist and have at least the given labels.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--imw workspace_path</code>: specifies which workspace to use as the   inventory management workspace.  The default value is the current   workspace.</li> <li><code>-X</code>: turn on debug echoing of the commands inside the script that   implements this command.</li> </ul> <p>The current workspaces does not matter if the IMW is explicitly specified.  Upon completion, the current workspace will be your chosen IMW.</p> <p>This command does not depend on the action of any of the KubeStellar controllers but does require that the KubeStellar Core Space (KCS) has been set up.</p> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar ensure location --imw root:imw-1 demo1 foo=bar the-word=the-bird\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nsynctarget.workload.kcp.io/demo1 created\nlocation.scheduling.kcp.io/demo1 created\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\n</code></pre></p> <p>The above example shows using this script to create a SyncTarget and a Location named <code>demo1</code> with labels <code>foo=bar</code> and <code>the-word=the-bird</code>. This was equivalent to the following commands.</p> <pre><code>kubectl ws root:imw-1\nkubectl create -f -&lt;&lt;EOF\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\n  name: demo1\n  labels:\n    id: demo1\n    foo: bar\n    the-word: the-bird\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\n  name: demo1\n  labels:\n    foo: bar\n    the-word: the-bird\nspec:\n  resource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\n  instanceSelector:\n    matchLabels: {\"id\":\"demo1\"}\nEOF\n</code></pre> <p>The creation of the APIBinding is equivalent to the following command (in the same workspace).</p> <pre><code>kubectl create -f &lt;&lt;EOF\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: edge.kubestellar.io\nspec:\n  reference:\n    export:\n      path: root:espw\n      name: edge.kubestellar.io\nEOF\n</code></pre> <p>This command operates in idempotent style, making whatever changes (if any) are needed to move from the current state to the desired state. Current limitation: it does not cast a skeptical eye on the spec of a pre-existing Location or APIBinding object.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-synctargetlocation-pairs","title":"Removing SyncTarget/Location pairs","text":"<p>The following script undoes whatever remains from a corresponding usage of <code>kubectl kubestellar ensure location</code>.  It has all the same command line syntax and semantics except that the <code>labelname=labelvalue</code> pairs do not appear.</p> <p>This command does not depend on the action of any of the KubeStellar controllers.</p> <p>The following session demonstrates usage, including idempotency.</p> <p><pre><code>kubectl ws root:imw-1\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\n</code></pre></p> <p><pre><code>kubectl kubestellar remove location demo1\n</code></pre> <pre><code>synctarget.workload.kcp.io \"demo1\" deleted\nlocation.scheduling.kcp.io \"demo1\" deleted\n</code></pre></p> <pre><code>kubectl kubestellar remove location demo1\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#syncer-preparation-and-installation","title":"Syncer preparation and installation","text":"<p>The syncer runs in each edge cluster and also talks to the corresponding mailbox workspace.  In order for it to be able to do that, there is some work to do in the mailbox workspace to create a ServiceAccount for the syncer to authenticate as and create RBAC objects to give the syncer the privileges that it needs.  The following script does those things and also outputs YAML to be used to install the syncer in the edge cluster.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar prep-for-syncer flag... synctarget_name\n</code></pre> <p>Here <code>synctarget_name</code> is the name of the <code>SyncTarget</code> object, in the relevant IMW, corresponding to the relevant edge cluster.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--imw workspace_path</code>: specifies which workspace holds the relevant   SyncTarget object.  The default value is the current workspace.</li> <li><code>--espw workspace_path</code>: specifies where to find the edge service   provider workspace.  The default is the standard location,   <code>root:espw</code>.</li> <li><code>--syncer-image image_ref</code>: specifies the container image that runs   the syncer.  The default is <code>quay.io/kubestellar/syncer:v0.9.0</code>.</li> <li><code>-o output_pathname</code>: specifies where to write the YAML definitions   of the API objects to create in the edge cluster in order to deploy   the syncer there.  The default is <code>synctarget_name +   \"-syncer.yaml\"</code>.</li> <li><code>-s</code>: exceptionally low info output.</li> <li><code>-X</code>: turn on debug echoing of commands inside the script that   implements this command.</li> </ul> <p>The current workspaces does not matter if the IMW is explicitly specified.  Upon completion, the current workspace will be what it was when the command started.</p> <p>This command will only succeed if the mailbox controller has created and conditioned the mailbox workspace for the given SyncTarget.  This command will wait for 10 to 70 seconds for that to happen.</p> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 demo1\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\"\nCurrent workspace is \"root:espw:4yqm57kx0m6mn76c-mb-406c54d1-64ce-4fdc-99b3-cef9c4fc5010\" (type root:universal).\nCreating service account \"kubestellar-syncer-demo1-28at01r3\"\nCreating cluster role \"kubestellar-syncer-demo1-28at01r3\" to give service account \"kubestellar-syncer-demo1-28at01r3\"\n1. write and sync access to the synctarget \"kubestellar-syncer-demo1-28at01r3\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-demo1-28at01r3\" to bind service account \"kubestellar-syncer-demo1-28at01r3\" to cluster role \"kubestellar-syncer-demo1-28at01r3\".\n\nWrote WEC manifest to demo1-syncer.yaml for namespace \"kubestellar-syncer-demo1-28at01r3\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"demo1-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-demo1-28at01r3\" kubestellar-syncer-demo1-28at01r3\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Once that script has run, the YAML for the objects to create in the edge cluster is in your chosen output file.  The default for the output file is the name of the SyncTarget object with \"-syncer.yaml\" appended.</p> <p>Create those objects with a command like the following; adjust as needed to configure <code>kubectl</code> to modify the edge cluster and read your chosen output file.</p> <pre><code>KUBECONFIG=$demo1_kubeconfig kubectl apply -f demo1-syncer.yaml\n</code></pre> <p>You can preserve this file for future use if you want to remove the syncer later.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#syncer-removal","title":"Syncer removal","text":"<p>The previous section shows the two steps involved in establishing a syncer: one modifies the core and one adds stuff into the WEC. If for any reason your core gets deleted (e.g., deliberate replacement) and you want to undo the addition of the syncer into the WEC, you can use the YAML file generated above with <code>kubectl delete</code> instead of <code>kubectl apply</code> (of course using the right kubeconfig and context therein).</p> <p>A syncer that is configured to work with an extinct core is harmless except that its pod will be doing some futile work (failing to make requests on the core and logging error messages).</p> <p>If you need to manually remove the syncer from your WEC, following is the list of things that went into the WEC.</p> <ul> <li>A namespace named   <code>kubestellar-syncer-${SyncTarget_Name}-${UID}</code>. The UID is a random   ID that is uniquely generated each time you run prep-for-syncer.</li> <li>A ServiceAccount (with associated token Secret) in that namespace.</li> <li>Another Secret in that namespace.</li> <li>A Deployment in that namespace</li> <li>A ClusterRole with the same name as the namespace.</li> <li>A ClusterRoleBinding with the same name as the namespace.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#edge-cluster-on-boarding","title":"Edge cluster on-boarding","text":"<p>The following command is a combination of <code>kubectl kubestellar ensure-location</code> and <code>kubectl kubestellar prep-for-syncer</code>, and takes the union of their command line flags and arguments.  Upon completion, the kcp current workspace will be what it was at the start.</p> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar prep-for-cluster --imw root:imw-1 demo2 key1=val1\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nsynctarget.workload.kcp.io/demo2 created\nlocation.scheduling.kcp.io/demo2 created\nsynctarget.workload.kcp.io/demo2 labeled\nlocation.scheduling.kcp.io/demo2 labeled\nCurrent workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1cpf1cd4ydy13vo1-mb-3c354acd-ed86-45bb-a60d-cee8e59973f7\" (type root:universal).\nCreating service account \"kubestellar-syncer-demo2-15nq4e94\"\nCreating cluster role \"kubestellar-syncer-demo2-15nq4e94\" to give service account \"kubestellar-syncer-demo2-15nq4e94\"\n1. write and sync access to the synctarget \"kubestellar-syncer-demo2-15nq4e94\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-demo2-15nq4e94\" to bind service account \"kubestellar-syncer-demo2-15nq4e94\" to cluster role \"kubestellar-syncer-demo2-15nq4e94\".\n\nWrote WEC manifest to demo2-syncer.yaml for namespace \"kubestellar-syncer-demo2-15nq4e94\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"demo2-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-demo2-15nq4e94\" kubestellar-syncer-demo2-15nq4e94\n\nto verify the syncer pod is running.\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-a-workload-management-workspace","title":"Creating a Workload Management Workspace","text":"<p>Such a workspace needs not only to be created but also populated with an <code>APIBinding</code> to the edge API and, if desired, some <code>APIBindings</code> to the Kubernetes API for management of containerized workloads.</p> <p>Every workload management workspace is a child of the <code>root</code> workspace.</p> <p>The usage synopsis for this command is as follows.</p> <pre><code>kubectl kubestellar ensure wmw flag... wm_workspace_name\n</code></pre> <p>Here <code>wm_workspace_name</code> is the name (not pathname, just a bare one-segment name) of the WMW to ensure.  Thus, the pathname of the WMW will be <code>root:wm_workspace_name</code>.</p> <p>Upon completion, the WMW will be the current workspace.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--with-kube boolean</code>: specifies whether or not the WMW should   include an APIBindings to the supported subset of the Kubernetes API   for management of containerized workloads. See the categorization   in the design for details.</li> <li><code>-X</code>: turn on debug echoing of the commands inside the script that   implements this command.</li> </ul> <p>This script works in idempotent style, doing whatever work remains to be done.</p> <p>The following session shows some example usages, including demonstration of idempotency and changing whether the kube APIBinding is included.</p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw\n</code></pre> <pre><code>Current workspace is \"root\".\nWorkspace \"example-wmw\" (type root:universal) created. Waiting for it to be ready...\nWorkspace \"example-wmw\" (type root:universal) is ready to use.\nCurrent workspace is \"root:example-wmw\" (type root:universal).\napibinding.apis.kcp.io/bind-espw created\napibinding.apis.kcp.io/bind-kubernetes created\napibinding.apis.kcp.io/bind-apiregistration.k8s.io created\napibinding.apis.kcp.io/bind-apps created\napibinding.apis.kcp.io/bind-autoscaling created\napibinding.apis.kcp.io/bind-batch created\napibinding.apis.kcp.io/bind-core.k8s.io created\napibinding.apis.kcp.io/bind-discovery.k8s.io created\napibinding.apis.kcp.io/bind-flowcontrol.apiserver.k8s.io created\napibinding.apis.kcp.io/bind-networking.k8s.io created\napibinding.apis.kcp.io/bind-node.k8s.io created\napibinding.apis.kcp.io/bind-policy created\napibinding.apis.kcp.io/bind-scheduling.k8s.io created\napibinding.apis.kcp.io/bind-storage.k8s.io created\n</code></pre></p> <p><pre><code>kubectl ws ..\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw\n</code></pre> <pre><code>Current workspace is \"root\".\nCurrent workspace is \"root:example-wmw\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl ws ..\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw --with-kube false\n</code></pre> <pre><code>Current workspace is \"root\".\nCurrent workspace is \"root:example-wmw\" (type root:universal).\napibinding.apis.kcp.io \"bind-kubernetes\" deleted\napibinding.apis.kcp.io \"bind-apiregistration.k8s.io\" deleted\napibinding.apis.kcp.io \"bind-apps\" deleted\napibinding.apis.kcp.io \"bind-autoscaling\" deleted\napibinding.apis.kcp.io \"bind-batch\" deleted\napibinding.apis.kcp.io \"bind-core.k8s.io\" deleted\napibinding.apis.kcp.io \"bind-discovery.k8s.io\" deleted\napibinding.apis.kcp.io \"bind-flowcontrol.apiserver.k8s.io\" deleted\napibinding.apis.kcp.io \"bind-networking.k8s.io\" deleted\napibinding.apis.kcp.io \"bind-node.k8s.io\" deleted\napibinding.apis.kcp.io \"bind-policy\" deleted\napibinding.apis.kcp.io \"bind-scheduling.k8s.io\" deleted\napibinding.apis.kcp.io \"bind-storage.k8s.io\" deleted\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-a-workload-management-workspace","title":"Removing a Workload Management Workspace","text":"<p>Deleting a WMW can be done by simply deleting its <code>Workspace</code> object from the parent.</p> <p><pre><code>kubectl ws root\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl delete Workspace example-wmw\n</code></pre> <pre><code>workspace.tenancy.kcp.io \"example-wmw\" deleted\n</code></pre></p> <p>Alternatively, you can use the following command line whose design completes the square here.  Invoke it when the current workspace is the parent of the workload management workspace to delete.</p> <p><pre><code>kubectl kubestellar remove wmw -h\n</code></pre> <pre><code>Usage: kubectl kubestellar remove wmw [-X] kubectl_flag... wm_workspace_name\n</code></pre></p> <p><pre><code>kubectl kubestellar remove wmw demo1\n</code></pre> <pre><code>Current workspace is \"root\".\nworkspace.tenancy.kcp.io \"demo1\" deleted\n</code></pre></p> <p><pre><code>kubectl ws .\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl kubestellar remove wmw demo1\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#bootstrap","title":"Bootstrap","text":"<p>This is a combination of some installation and setup steps, for use in the QuickStart.</p> <p>The script can be read directly from https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh and does the following things.</p> <ol> <li>Downloads and installs kcp executables if they are not already    evident on <code>$PATH</code>.</li> <li>Downloads and installs kubestellar executables if they are not    already evident on <code>$PATH</code>.</li> <li>Ensures that kcp and KubeStellar are deployed (i.e., their    processes are running and their initial configurations have been    established) either as bare processes or as workload in a    pre-existing Kubernetes cluster.</li> </ol> <p>This script accepts the following command line flags; all are optional.  The <code>--os</code>, <code>--arch</code>, and <code>--bind-address</code> flags are only useful when deploying as bare processes.  The deployment will be into a Kubernetes cluster if either <code>--external-endpoint</code> or <code>--openshift true</code> is given.</p> <ul> <li><code>--kubestellar-version $version</code>: specifies the release of   KubeStellar to use.  When using a specific version, include the   leading \"v\".  The default is the latest regular release, and the   value \"latest\" means the same thing.</li> <li><code>--kcp-version $version</code>: specifies the kcp release to use.  The   default is the one that works with the chosen release of   KubeStellar.</li> <li><code>--openshift $bool</code>: specifies whether to the hosting cluster is an   OpenShift cluster. The default value is <code>false</code>.</li> <li><code>--endpoint-address $domain_name:$port</code>: specifies where an Ingress   controller or OpenShift router is listening for incoming TLS   connections from external (to the hosting cluster) clients of kcp   and KubeStellar.</li> <li><code>--os $OS</code>: specifies the operating system to use in selecting the   executables to download and install.  Choices are <code>linux</code> and   <code>darwin</code>.  Auto-detected if omitted.</li> <li><code>--arch $IAS</code>: specifies the instruction set architecture to use in   selecting the executables to download and install.  Choices are   <code>amd64</code> and <code>arm64</code>.  Auto-detected if omitted.</li> <li><code>--bind-address $IPADDR</code>: directs that the kcp server (a) write that   address for itself in the kubeconfig file that it constructs and (b)   listens only at that address.  The default is to pick one of the   host's non-loopback addresses to write into the kubeconfig file and   not bind a listening address.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies the parent folder   for downloads.  Will be <code>mkdir -p</code>.  The default is the current   working directory.  The download of kcp, if any, will go in   <code>$install_parent_dir/kcp</code>.  The download of KubeStellar will go in   <code>$install_parent_dir/kubestellar</code>.</li> <li><code>--host-ns $namespace_in_hosting_cluster</code>: specifies the namespace   in the hosting cluster where the core will be deployed. Defaults to   \"kubestellar\".</li> <li><code>-V</code> or <code>--verbose</code>: increases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here \"install\" means only to (a) unpack the distribution archives into the relevant places under <code>$install_parent_dir</code> and (b) enhance the <code>PATH</code>, and <code>KUBECONFIG</code> in the case of kcp, environment variables in the shell running the script.  Of course, if you run the script in a sub-shell then those environment effects terminate with that sub-shell; this script also prints out messages showing how to update the environment in another shell.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/","title":"Extended Example","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre></p> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/example1.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre> <p>This doc shows a detailed example usage of the KubeStellar components.</p> <p>This example involves two edge clusters and two workloads.  One workload goes on both edge clusters and one workload goes on only one edge cluster.  Nothing changes after the initial activity.</p> <p>This example is presented in stages.  The controllers involved are always maintaining relationships.  This document focuses on changes as they appear in this example.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#how-to-install-pre-requisites-for-a-windows-subsystem-for-linux-wsl-envronment-using-an-ubuntu-220401-distribution","title":"How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution","text":"<p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#1-if-youre-using-a-vpn-turn-it-off","title":"1. If you're using a VPN, turn it off","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#2-install-ubuntu-into-wsl","title":"2. Install Ubuntu into WSL","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#20-if-wsl-is-not-yet-installed-open-a-powershell-administrator-window-and-run-the-following","title":"2.0 If wsl is not yet installed, open a powershell administrator window and run the following","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#21-reboot-your-system","title":"2.1 reboot your system","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#22-in-a-windows-command-terminal-run-the-following-to-list-all-the-linux-distributions-that-are-available-online","title":"2.2 In a Windows command terminal run the following to list all the linux distributions that are available online","text":"<pre><code>wsl -l -o\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#23-select-a-linux-distribution-and-install-it-into-wsl","title":"2.3 Select a linux distribution and install it into WSL","text":"<p><pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#24-enter-your-new-username-and-password-at-the-prompts-and-you-will-eventually-see-something-like","title":"2.4 Enter your new username and password at the prompts, and you will eventually see something like:","text":"<pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#25-click-on-the-windows-start-icon-and-type-in-the-name-of-your-distribution-into-the-search-box","title":"2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box.","text":"<p>Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#3-install-pre-requisites-into-your-new-vm","title":"3. Install pre-requisites into your new VM","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#31-update-and-apply-apt-get-packages","title":"3.1 update and apply apt-get packages","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#32-install-golang","title":"3.2 Install golang","text":"<pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#33-install-ko-but-dont-do-ko-set-action-step","title":"3.3 Install ko (but don't do ko set action step)","text":"<pre><code>go install github.com/google/ko@latest\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#34-install-gcc","title":"3.4 Install gcc","text":"<p>Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#35-install-make-if-you-installed-build-essential-this-may-already-be-installed","title":"3.5 Install make (if you installed build-essential this may already be installed)","text":"<pre><code>apt install make\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#36-install-jq","title":"3.6 Install jq","text":"<pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#37-install-kubectl","title":"3.7 install kubectl","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#38-install-helm-required-when-deploying-as-workload","title":"3.8 install helm (required when deploying as workload)","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#how-to-install-docker-and-kind-into-a-windows-subsystem-for-linux-wsl-environment-using-an-ubuntu-220401-distribution","title":"How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#10-start-a-vm-terminal-by-clicking-on-the-app-you-configured-using-the-instructions-in-the-general-pre-requisites-described-above","title":"1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#20-install-docker","title":"2.0 Install docker","text":"<p>The installation instructions from docker are not sufficient to get docker working with WSL</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#21-follow-instructions-here-to-install-docker-httpsdocsdockercomengineinstallubuntu","title":"2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/","text":"<p>Here some additonal steps you will need to take:</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#22-ensure-that-etcwslconf-is-configured-so-that-systemd-will-run-on-booting","title":"2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting.","text":"<p>If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#23-edit-etcsudoers-it-is-strongly-recommended-to-not-add-directives-directly-to-etcsudoers-but-instead-to-put-them-in-files-in-etcsudoersd-which-are-auto-included-so-makemodify-a-new-file-via","title":"2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via","text":"<p><pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#24-add-your-user-to-the-docker-group","title":"2.4 Add your user to the docker group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#25-if-dockerd-is-already-running-then-stop-it-and-restart-it-as-follows-note-the-new-dockerd-instance-will-be-running-in-the-foreground","title":"2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground):","text":"<pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#251-if-you-encounter-an-iptables-issue-which-is-described-here-httpsgithubcommicrosoftwslissues6655","title":"2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655","text":"<p>The following commands will fix the issue:  <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#3-you-will-now-need-to-open-new-terminals-to-access-the-vm-since-dockerd-is-running-in-the-foreground-of-this-terminal","title":"3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#31-in-your-new-terminal-install-kind","title":"3.1 In your new terminal, install kind","text":"<pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-1","title":"Stage 1","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#deploy-kcp-and-kubestellar","title":"Deploy kcp and KubeStellar","text":"<p>You need kcp and KubeStellar and can deploy them in either of two ways: as bare processes on whatever host you are using to run this example, or as workload in a Kubernetes cluster (an OpenShift cluster qualifies).  Do one or the other, not both.</p> <p>KubeStellar only works with release <code>v0.11.0</code> of kcp.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.  See the start of the kcp quickstart for instructions on that, but get release v0.11.0 rather than the latest (the downloadable assets appear after the long list of changes/features/etc).</p> <p>Clone the v0.11.0 branch of the kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> and build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Running the kcp server creates a hidden subdirectory named <code>.kcp</code> to hold all sorts of state related to the server. If you have run it before and want to start over from scrath then you should <code>rm -rf .kcp</code> first.</p> <p>Use the following commands to: (a) run the kcp server in a forked command, (b) update your <code>KUBECONFIG</code> environment variable to configure <code>kubectl</code> to use the kubeconfig produced by the kcp server, and (c) wait for the kcp server to get through some initialization. The choice of <code>-v=3</code> for the kcp server makes it log a line for every HTTP request (among other things).</p> <pre><code>kcp start -v=3 &amp;&gt; /tmp/kcp.log &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\n# wait until KCP is ready checking availability of ws resource\nwhile ! kubectl ws tree &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that the kcp server creates. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the Edge Service Provider Workspace (ESPW), which exports the KubeStellar API, and also augments the <code>root:compute</code> workspace from kcp TMC as needed here. That augmentation consists of adding authorization to update the relevant <code>/status</code> and <code>/scale</code> subresources (missing in kcp TMC) and extending the supported subset of the Kubernetes API for managing containerized workloads from the four resources built into kcp TMC (<code>Deployment</code>, <code>Pod</code>, <code>Service</code>, and <code>Ingress</code>) to the other ones that are meaningful in KubeStellar.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>kubectl ws root:imw1\nkubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"decribe the florin location object\"\nkubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects plus the kcp <code>APIBinding</code> objects that import the definition of the KubeStellar API.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 10\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox workspaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox workspaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>kubectl ws root\nwhile [ $(kubectl ws tree | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into your shell if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI0721 17:37:10.186848  189094 main.go:206] \"Found APIExport view\" exportName=\"e\ndge.kubestellar.io\" serverURL=\"https://10.0.2.15:6443/services/apiexport/cseslli1ddit3s\na5/edge.kubestellar.io\"\n...\nI0721 19:17:21.906984  189094 controller.go:300] \"Created APIBinding\" worker=1\nmbwsName=\"1d55jhazpo3d3va6-mb-551bebfd-b75e-47b1-b2e0-ff0a4cb7e006\" mbwsCluster\n=\"32x6b03ixc49cj48\" bindingName=\"bind-edge\" resourceVersion=\"1247\"\n...\nI0721 19:18:56.203057  189094 controller.go:300] \"Created APIBinding\" worker=0\nmbwsName=\"1d55jhazpo3d3va6-mb-732cf72a-1ca9-4def-a5e7-78fd0e36e61c\" mbwsCluster\n=\"q31lsrpgur3eg9qk\" bindingName=\"bind-edge\" resourceVersion=\"1329\"\n^C\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>A mailbox workspace name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl ws root\nkubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE          REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal              Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal              Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\ncompute                                                    universal              Ready   https://172.20.144.39:6443/clusters/root:compute        6m8s\nespw                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:espw           2m4s\nimw1                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:imw1           1m9s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\ncompute                                                    &lt;none&gt;       mqnl7r5f56hswewy\nespw                                                       &lt;none&gt;       2n88ugkhysjbxqp5\nimw1                                                       &lt;none&gt;       4d2r9stcyy2qq5c1\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox workspace name is $GUILDER_WS\n</code></pre> <pre><code>The guilder mailbox workspace name is 1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox workspace name is $FLORIN_WS\n</code></pre> <pre><code>The florin mailbox workspace name is 1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kubestellar.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\", \"bind-apiregistration.k8s.io\" ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    namespaceSelectors: []\n    objectNames: [ \"v1090.example.my\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver; it requires the ESPW to be the current kcp workspace at start time.</p> <pre><code>kubectl ws root:espw\nkubestellar-where-resolver &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>kubectl ws root:wmw-c\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nkubectl ws root:wmw-s\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I0423 01:33:37.036752   11305 main.go:212] \"Found APIExport view\" exportName=\"edge.kubestellar.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kubestellar.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl ws root:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox workspaces and create <code>SyncerConfig</code> objects there.</p> <p>If you have deployed the KubeStellar core as workload in a Kubernetes cluster then the placement translator is running in a Pod there. If instead you are running the core controllers as bare processes then use the following commands to launch the placement translator; it requires the ESPW to be current at start time.</p> <pre><code>kubectl ws root:espw\nplacement-translator &amp;\nsleep 10\n</code></pre> <p>The following commands wait for the placement translator to get its job done for this example.</p> <pre><code># wait until SyncerConfig, ReplicaSets and Deployments are ready\nmbxws=($FLORIN_WS $GUILDER_WS)\nfor ii in \"${mbxws[@]}\"; do\nkubectl ws root:$ii\n# wait for SyncerConfig resource\nwhile ! kubectl get SyncerConfig the-one &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* SyncerConfig resource exists\"\n# wait for ReplicaSet resource\nwhile ! kubectl get rs &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* ReplicaSet resource exists\"\n# wait until ReplicaSet running\nwhile [ \"$(kubectl get rs -n commonstuff commond -o 'jsonpath={.status.readyReplicas}')\" != 1 ]; do\nsleep 10\ndone\necho \"* commonstuff ReplicaSet running\"\ndone\n# check for deployment in guilder\nwhile ! kubectl get deploy -A &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* Deployment resource exists\"\nwhile [ \"$(kubectl get deploy -n specialstuff speciald -o 'jsonpath={.status.availableReplicas}')\" != 1 ]; do\nsleep 10\ndone\necho \"* specialstuff Deployment running\"\n</code></pre> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize the name of the mailbox workspace for florin (which you stored in Stage 1) here.</p> <pre><code>kubectl ws root:$FLORIN_WS\n</code></pre> <pre><code>Current workspace is \"root:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\n</code></pre> <pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    resource: configmaps\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>You can check that the workload got there too.</p> <p><pre><code>kubectl get ns\n</code></pre> <pre><code>NAME          STATUS   AGE\ncommonstuff   Active   6m34s\ndefault       Active   32m\n</code></pre></p> <p><pre><code>kubectl get replicasets -A\n</code></pre> <pre><code>NAMESPACE     NAME      DESIRED   CURRENT   READY   AGE\ncommonstuff   commond   0         1         1       10m\n</code></pre></p> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the mailbox workspace name that you stored in Stage 1.</p> <p><pre><code>kubectl ws root:$GUILDER_WS\n</code></pre> <pre><code>Current workspace is \"root:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  clusterScope:\n  - apiVersion: v1\n    group: apiregistration.k8s.io\n    objects:\n    - v1090.example.my\n    resource: apiservices\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - speciald\n      namespace: specialstuff\n    resource: deployments\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    - names:\n      - httpd-htdocs\n      namespace: specialstuff\n    resource: configmaps\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p><pre><code>kubectl get deployments,replicasets -A\n</code></pre> <pre><code>NAMESPACE      NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff   deployment.apps/speciald   0/0     1            0           12m\n\nNAMESPACE     NAME                      DESIRED   CURRENT   READY   AGE\ncommonstuff   replicaset.apps/commond   0         1         1       7m4s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>( KUBECONFIG=~/.kube/config\n  let tries=1\nwhile ! kubectl --context kind-florin get ns commonstuff &amp;&gt; /dev/null; do\nif (( tries &gt;= 30)); then\necho 'The commonstuff namespace failed to appear in florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n    sleep 10\ndone\nkubectl --context kind-florin get ns\n)\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy,rs -A | egrep 'NAME|stuff'\n</code></pre> <pre><code>NAMESPACE                            NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\nNAMESPACE                            NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                          replicaset.apps/commond                                         1         1         1       13m\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces and both Deployments.</p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy,rs -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff                          deployment.apps/speciald                              1/1     1            1           23m\nNAMESPACE                             NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                           replicaset.apps/commond                                         1         1         1       23m\nspecialstuff                          replicaset.apps/speciald-76cdbb69b5                             1         1         1       14s\n</code></pre></p> <p>Examine the <code>APIService</code> objects in the guilder cluster, find the one named <code>v1090.example.my</code>. It is broken because it refers to a Service object that we have not bothered to create.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get apiservices | grep 1090\n</code></pre> <pre><code>v1090.example.my                       my-example/my-service   False (ServiceNotFound)   2m39s\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get rs -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8094 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>let tries=1\nwhile ! curl http://localhost:8097 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The special workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8096 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-5","title":"Stage 5","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1/#singleton-reported-state-return","title":"Singleton reported state return","text":"<p>The two <code>EdgePlacement</code> objects above assert that the expected number of executing copies of their matching workload objects is 1 and request return of reported state to the WDS when the number of executing copies is exactly 1.</p> <p>For the common workload, that assertion is not correct: the number of executing copies should be 2.  The assertion causes the actual number of executing copies to be reported.  Check that the reported number is 2.</p> <pre><code>kubectl ws root:wmw-c\nkubectl get rs -n commonstuff commond -o yaml | grep 'kubestellar.io/executing-count: \"2\"' || { kubectl get rs -n commonstuff commond -o yaml; false; }\n</code></pre> <p>For the special workload, the number of executing copies should be 1. Check that the reported number agrees.</p> <pre><code>kubectl ws root:wmw-s\nkubectl get deploy -n specialstuff speciald -o yaml | grep 'kubestellar.io/executing-count: \"1\"' || { kubectl get deploy -n specialstuff speciald -o yaml; false; }\n</code></pre> <p>Look at the status section of the \"speciald\" <code>Deployment</code> and see that it has been filled in with the information from the guilder cluster.</p> <pre><code>kubectl get deploy -n specialstuff speciald -o yaml\nkubectl get deploy -n specialstuff speciald -o yaml | grep 'readyReplicas: 1'\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#status-summarization-aspirational","title":"Status Summarization (aspirational)","text":"<p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the special workload, creates a status summary object in the specialstuff namespace in the special workload workspace holding a summary of the corresponding Deployment objects. In this case there is just one such object, in the mailbox workspace for the guilder cluster.</p> <p></p> <p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the common workload, creates a status summary object in the commonstuff namespace in the common workload workspace holding a summary of the corresponding Deployment objects.  Those are the <code>commond</code> Deployment objects in the two mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace imw1\nkubectl delete workspace $FLORIN_WS\nkubectl delete workspace $GUILDER_WS\nkubectl kubestellar remove wmw wmw-c\nkubectl kubestellar remove wmw wmw-s\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/","title":"KubeStellar-Syncer","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre></p> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/kubestellar-syncer.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre> <p>KubeStellar-Syncer runs in the target cluster and sync kubernetes resource objects from the target cluster to a mailbox workspace and vice versa.</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-install-pre-requisites-for-a-windows-subsystem-for-linux-wsl-envronment-using-an-ubuntu-220401-distribution","title":"How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution","text":"<p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#1-if-youre-using-a-vpn-turn-it-off","title":"1. If you're using a VPN, turn it off","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#2-install-ubuntu-into-wsl","title":"2. Install Ubuntu into WSL","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#20-if-wsl-is-not-yet-installed-open-a-powershell-administrator-window-and-run-the-following","title":"2.0 If wsl is not yet installed, open a powershell administrator window and run the following","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#21-reboot-your-system","title":"2.1 reboot your system","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#22-in-a-windows-command-terminal-run-the-following-to-list-all-the-linux-distributions-that-are-available-online","title":"2.2 In a Windows command terminal run the following to list all the linux distributions that are available online","text":"<pre><code>wsl -l -o\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#23-select-a-linux-distribution-and-install-it-into-wsl","title":"2.3 Select a linux distribution and install it into WSL","text":"<p><pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#24-enter-your-new-username-and-password-at-the-prompts-and-you-will-eventually-see-something-like","title":"2.4 Enter your new username and password at the prompts, and you will eventually see something like:","text":"<pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#25-click-on-the-windows-start-icon-and-type-in-the-name-of-your-distribution-into-the-search-box","title":"2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box.","text":"<p>Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#3-install-pre-requisites-into-your-new-vm","title":"3. Install pre-requisites into your new VM","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#31-update-and-apply-apt-get-packages","title":"3.1 update and apply apt-get packages","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#32-install-golang","title":"3.2 Install golang","text":"<pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#33-install-ko-but-dont-do-ko-set-action-step","title":"3.3 Install ko (but don't do ko set action step)","text":"<pre><code>go install github.com/google/ko@latest\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#34-install-gcc","title":"3.4 Install gcc","text":"<p>Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#35-install-make-if-you-installed-build-essential-this-may-already-be-installed","title":"3.5 Install make (if you installed build-essential this may already be installed)","text":"<pre><code>apt install make\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#36-install-jq","title":"3.6 Install jq","text":"<pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#37-install-kubectl","title":"3.7 install kubectl","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#38-install-helm-required-when-deploying-as-workload","title":"3.8 install helm (required when deploying as workload)","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-install-docker-and-kind-into-a-windows-subsystem-for-linux-wsl-environment-using-an-ubuntu-220401-distribution","title":"How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#10-start-a-vm-terminal-by-clicking-on-the-app-you-configured-using-the-instructions-in-the-general-pre-requisites-described-above","title":"1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#20-install-docker","title":"2.0 Install docker","text":"<p>The installation instructions from docker are not sufficient to get docker working with WSL</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#21-follow-instructions-here-to-install-docker-httpsdocsdockercomengineinstallubuntu","title":"2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/","text":"<p>Here some additonal steps you will need to take:</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#22-ensure-that-etcwslconf-is-configured-so-that-systemd-will-run-on-booting","title":"2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting.","text":"<p>If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#23-edit-etcsudoers-it-is-strongly-recommended-to-not-add-directives-directly-to-etcsudoers-but-instead-to-put-them-in-files-in-etcsudoersd-which-are-auto-included-so-makemodify-a-new-file-via","title":"2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via","text":"<p><pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#24-add-your-user-to-the-docker-group","title":"2.4 Add your user to the docker group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#25-if-dockerd-is-already-running-then-stop-it-and-restart-it-as-follows-note-the-new-dockerd-instance-will-be-running-in-the-foreground","title":"2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground):","text":"<pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#251-if-you-encounter-an-iptables-issue-which-is-described-here-httpsgithubcommicrosoftwslissues6655","title":"2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655","text":"<p>The following commands will fix the issue:  <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#3-you-will-now-need-to-open-new-terminals-to-access-the-vm-since-dockerd-is-running-in-the-foreground-of-this-terminal","title":"3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#31-in-your-new-terminal-install-kind","title":"3.1 In your new terminal, install kind","text":"<pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#steps-to-try-the-syncer","title":"Steps to try the Syncer","text":"<p>The KubeStellar-Syncer can be exercised after setting up KubeStellar mailbox workspaces. Firstly we'll follow to similar steps in example1 until <code>The mailbox controller</code> in stage 2. </p> <p></p> <p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deploy-kcp-and-kubestellar","title":"Deploy kcp and KubeStellar","text":"<p>You need kcp and KubeStellar and can deploy them in either of two ways: as bare processes on whatever host you are using to run this example, or as workload in a Kubernetes cluster (an OpenShift cluster qualifies).  Do one or the other, not both.</p> <p>KubeStellar only works with release <code>v0.11.0</code> of kcp.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.  See the start of the kcp quickstart for instructions on that, but get release v0.11.0 rather than the latest (the downloadable assets appear after the long list of changes/features/etc).</p> <p>Clone the v0.11.0 branch of the kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> and build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Running the kcp server creates a hidden subdirectory named <code>.kcp</code> to hold all sorts of state related to the server. If you have run it before and want to start over from scrath then you should <code>rm -rf .kcp</code> first.</p> <p>Use the following commands to: (a) run the kcp server in a forked command, (b) update your <code>KUBECONFIG</code> environment variable to configure <code>kubectl</code> to use the kubeconfig produced by the kcp server, and (c) wait for the kcp server to get through some initialization. The choice of <code>-v=3</code> for the kcp server makes it log a line for every HTTP request (among other things).</p> <pre><code>kcp start -v=3 &amp;&gt; /tmp/kcp.log &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\n# wait until KCP is ready checking availability of ws resource\nwhile ! kubectl ws tree &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that the kcp server creates. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the Edge Service Provider Workspace (ESPW), which exports the KubeStellar API, and also augments the <code>root:compute</code> workspace from kcp TMC as needed here. That augmentation consists of adding authorization to update the relevant <code>/status</code> and <code>/scale</code> subresources (missing in kcp TMC) and extending the supported subset of the Kubernetes API for managing containerized workloads from the four resources built into kcp TMC (<code>Deployment</code>, <code>Pod</code>, <code>Service</code>, and <code>Ingress</code>) to the other ones that are meaningful in KubeStellar.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>kubectl ws root:imw1\nkubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"decribe the florin location object\"\nkubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects plus the kcp <code>APIBinding</code> objects that import the definition of the KubeStellar API.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 10\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox workspaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox workspaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>kubectl ws root\nwhile [ $(kubectl ws tree | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into your shell if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI0721 17:37:10.186848  189094 main.go:206] \"Found APIExport view\" exportName=\"e\ndge.kubestellar.io\" serverURL=\"https://10.0.2.15:6443/services/apiexport/cseslli1ddit3s\na5/edge.kubestellar.io\"\n...\nI0721 19:17:21.906984  189094 controller.go:300] \"Created APIBinding\" worker=1\nmbwsName=\"1d55jhazpo3d3va6-mb-551bebfd-b75e-47b1-b2e0-ff0a4cb7e006\" mbwsCluster\n=\"32x6b03ixc49cj48\" bindingName=\"bind-edge\" resourceVersion=\"1247\"\n...\nI0721 19:18:56.203057  189094 controller.go:300] \"Created APIBinding\" worker=0\nmbwsName=\"1d55jhazpo3d3va6-mb-732cf72a-1ca9-4def-a5e7-78fd0e36e61c\" mbwsCluster\n=\"q31lsrpgur3eg9qk\" bindingName=\"bind-edge\" resourceVersion=\"1329\"\n^C\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>A mailbox workspace name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl ws root\nkubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE          REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal              Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal              Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\ncompute                                                    universal              Ready   https://172.20.144.39:6443/clusters/root:compute        6m8s\nespw                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:espw           2m4s\nimw1                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:imw1           1m9s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\ncompute                                                    &lt;none&gt;       mqnl7r5f56hswewy\nespw                                                       &lt;none&gt;       2n88ugkhysjbxqp5\nimw1                                                       &lt;none&gt;       4d2r9stcyy2qq5c1\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox workspace name is $GUILDER_WS\n</code></pre> <pre><code>The guilder mailbox workspace name is 1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox workspace name is $FLORIN_WS\n</code></pre> <pre><code>The florin mailbox workspace name is 1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#register-kubestellar-syncer-on-the-target-clusters","title":"Register KubeStellar-Syncer on the target clusters","text":"<p>Once KubeStellar setup is done, KubeStellar-Syncer can be deployed on the target cluster easily by the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#for-the-target-cluster-of-guilder","title":"For the target cluster of <code>guilder</code>,","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw1\nkubectl get SyncTargets\nkubectl get synctargets.edge.kubestellar.io\nkubectl describe synctargets.edge.kubestellar.io guilder\n#kubectl describe Synctarget guilder\nmbws=`kubectl get synctargets.edge.kubestellar.io guilder -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:$mbws\n./bin/kubectl-kubestellar-syncer_gen guilder --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o guilder-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\".\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                           coredns                               2/2     2            2           35m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#for-the-target-cluster-of-florin","title":"For the target cluster of <code>florin</code>,","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw1\nmbws=`kubectl get synctargets.edge.kubestellar.io florin -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:$mbws\n./bin/kubectl-kubestellar-syncer_gen florin --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o florin-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\".\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-florin-32uaph9l    kubestellar-syncer-florin-32uaph9l    1/1     1            1           42s\nkube-system                           coredns                               2/2     2            2           41m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           41m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl kubestellar remove wmw example-wmw\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deep-dive","title":"Deep-dive","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#the-details-about-the-registration-of-kubestellar-syncer-on-an-edge-cluster-and-a-workspace","title":"The details about the registration of KubeStellar-Syncer on an Edge cluster and a workspace","text":"<p>KubeStellar-Syncer is deployed on an Edge cluster easily by the following steps.</p> <ol> <li>Create SyncTarget and Location<ul> <li>Mailbox controller creates the mailbox workspace automatically </li> </ul> </li> <li>Get the mailbox workspace name</li> <li>Use the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer     <pre><code>kubectl ws &lt;mb-ws name&gt;\nbin/kubectl-kubestellar-syncer_gen &lt;Edge Sync Target name&gt; --syncer-image &lt;KubeStellar-Syncer image&gt; -o kubestellar-syncer.yaml\n</code></pre>     Here, <code>bin/kubectl-kubestellar-syncer_gen</code> refers to a special variant of KubeStellar's     kubectl plugin that includes the implementation of the functionality needed     here.  This variant, under the special name shown here, is a normal part of     the <code>bin</code> of KubeStellar.     For the KubeStellar-Syncer image, please select an official image from https://quay.io/repository/kubestellar/syncer?tab=tags. For example, <code>--syncer-image quay.io/kubestellar/syncer:git-08289ea05-clean</code>. You can also create a syncer image from the source following Build KubeStellar-Syncer Image.</li> <li>Deploy KubeStellar-Syncer on an Edge cluster</li> <li>Syncer starts to run on the Edge cluster<ul> <li>KubeStellar-Syncer starts watching and consuming SyncerConfig</li> </ul> </li> </ol> <p>The overall diagram is as follows:</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#what-kubestellar-syncer-gen-plugin-does","title":"What KubeStellar syncer-gen plugin does","text":"<p>In order for Syncer to sync resources between upstream (workspace) and downstream (workload execution cluster), access information for both is required. </p> <p>For the upstream access, Syncer's registration command (<code>kubectl kubestellar syncer-gen</code>) creates a ServiceAccount, ClusterRole, and ClusterRoleBinding in the workspace, and then generates a kubeconfig manifest from the ServiceAccount token, KCP server URL, and the server certificates. The kubeconfig manifest is embedded in a secret manifest and the secret is mounted to <code>/kcp/</code> in Syncer pod. The command generates such deployment manifest as Syncer reads <code>/kcp/</code> for the upstream Kubeconfig. </p> <p>On the other hand, for the downstream part, in addition to the deployment manifest, the command generates a ServiceAccount, Role/ClusterRole, RoleBinding/ClusterRoleBinding for Syncer to access resources on the WEC. These resources for the downstream part are the resources to be deployed to the downstream cluster. The ServiceAccount is set to <code>serviceAccountName</code> in the deployment manifest.</p> <p>Note: In addition to that, the command creates an EdgeSyncConfig CRD if it does not exist, and creates a default EdgeSyncConfig resource with the name specified in the command (<code>kubectl kubestellar syncer-gen &lt;name&gt;</code>). The default EdgeSyncConfig is no longer needed since Syncer now consumes all EdgeSyncConfigs in the workspace. Furthermore, creation of the EdgeSyncConfig CRD will also no longer be needed since we will switch to using SyncerConfig rather than EdgeSyncConfig in near future.</p> <p>The source code of the command is https://github.com/kubestellar/kubestellar/blob/main/pkg/cliplugins/kubestellar/syncer-gen/edgesync.go.</p> <p>The equivalent manual steps are as follows: </p> <p>Generate UUID for Syncer identification. <pre><code>syncer_id=\"syncer-\"`uuidgen | tr '[:upper:]' '[:lower:]'`\n</code></pre></p> <p>Go to a workspace. <pre><code>kubectl ws root\nkubectl ws create ws1 --enter\n</code></pre></p> <p>Create the following APIBinding in the workspace (Note that in the case of mailbox workspaces, it's done by mailbox controller at creating the mailbox workspace.) <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: bind-espw\nspec:\n  reference:\n    export:\n      path: root:espw\n      name: edge.kubestellar.io\nEOL\n</code></pre></p> <p>Create a serviceaccount in the workspace. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: $syncer_id\nEOL\n</code></pre></p> <p>Create clusterrole and clusterrolebinding to bind the serviceaccount to the role. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: $syncer_id\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- nonResourceURLs: [\"/\"]\n  verbs: [\"access\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: $syncer_id\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: $syncer_id\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: $syncer_id\n  namespace: default\nEOL\n</code></pre></p> <p>Get the serviceaccount token that will be set in the upstream kubeconfig manifest. <pre><code>secret_name=`kubectl get secret -o custom-columns=\":.metadata.name\"| grep $syncer_id`\ntoken=`kubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d`\n</code></pre></p> <p>Get the certificates that will be set in the upstream kubeconfig manifest. <pre><code>cacrt=`kubectl config view --minify --raw | yq \".clusters[0].cluster.certificate-authority-data\"`\n</code></pre></p> <p>Get server_url that will be set in the upstream kubeconfig manifest. <pre><code>server_url=`kubectl config view --minify --raw | yq \".clusters[0].cluster.server\" | sed -e 's|https://\\(.*\\):\\([^/]*\\)/.*|https://\\1:\\2|g'`\n</code></pre></p> <p>Set some other parameters. a. downstream_namespace where Syncer Pod runs <pre><code>downstream_namespace=\"kubestellar-$syncer_id\"\n</code></pre> b. Syncer image <pre><code>image=\"quay.io/kubestellar/syncer:v0.2.2\"\n</code></pre></p> <p>Download manifest template. <pre><code>curl -LO https://raw.githubusercontent.com/kubestellar/kubestellar/main/pkg/syncer/scripts/kubestellar-syncer-bootstrap.template.yaml\n</code></pre></p> <p>Generate manifests to bootstrap KubeStellar-Syncer. <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image envsubst &lt; kubestellar-syncer-bootstrap.template.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubestellar-syncer-9ee90de6-eb76-4ddb-9346-c4c8d92075e1\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n...\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deploy-workload-objects-from-kubestellar-to-edge-cluster","title":"Deploy workload objects from KubeStellar to Edge cluster","text":"<p>To deploy resources to Edge clusters, create the following in workload management workspace - workload objects   - Some objects are denatured if needed.   - Other objects are as it is - APIExport/API Schema corresponding to CRD such as Kubernetes ClusterPolicyReport.   - TBD: Conversion from CRD to APIExport/APISchema could be automated by using MutatingAdmissionWebhook on workload management workspace. This automation is already available (see the script here).  - EdgePlacement</p> <p></p> <p>After this, KubeStellar will put the following in the mailbox workspace. - Workload objects (both denatured one and not-denatured one) - SyncerConfig CR</p> <p>TODO: This is something we should clarify..e.g. which existing controller(s) in KubeStellar will cover, or just create a new controller to handle uncovered one. @MikeSpreitzer gave the following suggestions.   - The placement translator will put the workload objects and syncer config into the mailbox workspaces.   - The placement translator will create syncer config based on the EdgePlacement objects and what they match.   - The mailbox controller will put API Binding into the mailbox workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#edgesyncconfig-will-be-replaced-to-syncerconfig","title":"EdgeSyncConfig (will be replaced to SyncerConfig)","text":"<ul> <li>The example of EdgeSyncConfig CR is here. Its CRD is here.</li> <li>The CR here is used from edge syncer. </li> <li>The CR is placed at mb-ws to define</li> <li>object selector</li> <li>need of renaturing</li> <li>need of returning reported states of downsynced objects</li> <li>need of delete propagation for downsyncing</li> <li>The CR is managed by KubeStellar (placement transformer).</li> <li>At the initial implementation before KubeStellar side controller become ready, we assume SyncerConfig is on workload management workspace (wm-ws), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how KubeStellar-Syncer behaves, and will be extended/split/merged according to further design discussion.</li> <li>One CR is initially created by the command for KubeStellar-Syncer enablement in mb-ws (<code>kubectl kubestellar syncer-gen &lt;name&gt;</code>)</li> <li>The CR name is <code>&lt;name&gt;</code> and the contents are empty.</li> <li>This name is registered in the bootstrap manifests for KubeStellar-Syncer install and KubeStellar-Syncer is told to watch the CR of this name.</li> <li>Currently KubeStellar-Syncer watches all CRs in the workspace</li> <li>KubeStellar-Syncer merges them and decides which resources are down/up synced based on the merged information. </li> <li>This behavior may be changed to only watching the default CR once Placement Translator is to be the component that generates the CR from EdgePlacement: related issue</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#syncerconfig","title":"SyncerConfig","text":"<ul> <li>The spec is defined in https://github.com/kubestellar/kubestellar/blob/main/pkg/apis/edge/v2alpha1/syncer-config.go</li> <li><code>namespaceScope</code> field is for namespace scoped objects.<ul> <li><code>namespaces</code> is field for which namespaces to be downsynced.</li> <li><code>resources</code> is field for what resource's objects in the above namespaces are downsynced. All objects in the selected resource are downsynced.</li> </ul> </li> <li><code>clusterScope</code> field is for cluster scoped objects<ul> <li>It's an array of <code>apiVersion</code>, <code>group</code>, <code>resource</code>, and <code>objects</code>.</li> <li><code>objects</code> can be specified by wildcard (<code>*</code>) meaning all objects.</li> </ul> </li> <li><code>upsync</code> field is for upsynced objects including both namespace and cluster scoped objects.<ul> <li>It's an array of <code>apiGroup</code>, <code>resources</code>, <code>namespaces</code>, and <code>names</code>.</li> <li><code>apiGroup</code> is group.</li> <li><code>resources</code> is an array of upsynced resource.</li> <li><code>namespaces</code> is an array of namespace for namespace objects.</li> <li><code>names</code> is an array of upsynced object name. Wildcard (<code>*</code>) is available.</li> </ul> </li> <li>The example CR is https://github.com/kubestellar/kubestellar/blob/main/test/e2e/kubestellar-syncer/testdata/kyverno/syncer-config.yaml</li> <li>The CR is used from KubeStellar-Syncer</li> <li>The CR is placed in mb-ws to define</li> <li>object selector</li> <li>need of renaturing (May not scope in PoC2023q1)</li> <li>need of returning reported states of downsynced objects (May not scope in PoC2023q1)</li> <li>need of delete propagation for downsyncing (May not scope in PoC2023q1)</li> <li>The CR is managed by KubeStellar (placement translator).</li> <li>At the initial implementation before KubeStellar side controller become ready, we assume SyncerConfig is on workload management workspace (wm-ws), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how KubeStellar-Syncer behaves, and will be extended/split/merged according to further design discussion.</li> <li>Currently KubeStellar-Syncer watches all CRs in the workspace</li> <li>KubeStellar-Syncer merges them and decides which resources are down/up synced based on the merged information. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#downsyncing","title":"Downsyncing","text":"<ul> <li>KubeStellar-Syncer does downsyncing, which copy workload objects on mailbox workspace to Edge cluster</li> <li>If workload objects are deleted on mailbox workspace, the corresponding objects on the Edge cluster will be also deleted according to SyncerConfig. </li> <li>SyncerConfig specifies which objects should be downsynced.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation</li> <li>Cover cluster-scope objects and CRD</li> <li>CRD needs to be denatured if downsyncing is required. (May not scope in PoC2023q1 since no usage)</li> <li>Renaturing is applied if required (specified in SyncerConfig). (May not scope in PoC2023q1 since no usage)</li> <li>Current implementation is using polling to detect changes on mailbox workspace, but will be changed to use Informers. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#renaturing-may-not-scope-in-poc2023q1-since-no-usage","title":"Renaturing (May not scope in PoC2023q1 since no usage)","text":"<ul> <li>KubeStellar-Syncer does renaturing, which converts workload objects to different forms of objects on a Edge cluster. </li> <li>The conversion rules (downstream/upstream mapping) is specified in SyncerConfig.</li> <li>Some objects need to be denatured. </li> <li>CRD needs to be denatured when conflicting with APIBinding.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#return-of-reported-state","title":"Return of reported state","text":"<ul> <li>KubeStellar-Syncer return the reported state of downsynced objects at Edge cluster to the status of objects on the mailbox workspace periodically. </li> <li>TODO: Failing to returning reported state of some resources (e.g. deployment and service). Need more investigation. </li> <li>reported state returning on/off is configurable in SyncerConfig. (default is on)</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#resource-upsyncing","title":"Resource Upsyncing","text":"<ul> <li>KubeStellar-Syncer does upsyncing resources at Edge cluster to the corresponding mailbox workspace periodically. </li> <li>SyncerConfig specifies which objects should be upsynced from Edge cluster.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation (, and more such as ownership reference?)</li> <li>Upsyncing CRD is out of scope for now. This means when upsyncing a CR, corresponding APIBinding (not CRD) is available on the mailbox workspace. This limitation might be revisited later. </li> <li>~Upsynced objects can be accessed from APIExport set on the workload management workspace bound to the mailbox workspace (with APIBinding). This access pattern might be changed when other APIs such as summarization are provided in KubeStellar.~ =&gt; Upsynced objects are accessed through Mailbox informer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#feasibility-study","title":"Feasibility study","text":"<p>We will verify if the design described here could cover the following 4 scenarios.  - I can register a KubeStellar-Syncer on a Edge cluster to connect a mailbox workspace specified by name. (KubeStellar-Syncer registration) - I can deploy Kyverno and its policy from mailbox workspace to Edge cluster just by using manifests (generated from Kyverno helm chart) rather than using OLM. (workload deployment by KubeStellar-Syncer's downsyncing) - I can see the policy report generated at Edge cluster via API Export on workload management workspace. (resource upsyncing by KubeStellar-Syncer)  - I can deploy the denatured objects on mailbox workspace to Edge cluster by renaturing them automatically in KubeStellar-Syncer. (workload deployment by renaturing)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#build-kubestellar-syncer-image","title":"Build KubeStellar-Syncer image","text":"<p>Prerequisite - Install ko (https://ko.build/install/)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-build-the-image-in-your-local","title":"How to build the image in your local","text":"<ol> <li><code>make build-kubestellar-syncer-image-local</code> e.g. <pre><code>$ make build-kubestellar-syncer-image-local\n2023/04/24 11:50:37 Using base distroless.dev/static:latest@sha256:81018475098138883b80dcc9c1242eb02b53465297724b18e88591a752d2a49c for github.com/kubestellar/kubestellar/cmd/syncer\n2023/04/24 11:50:38 Building github.com/kubestellar/kubestellar/cmd/syncer for linux/arm64\n2023/04/24 11:50:39 Loading ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Loaded ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Adding tag latest\n2023/04/24 11:50:40 Added tag latest\nkubestellar-syncer image:\nko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n</code></pre> <code>ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96</code> is the image stored in your local Docker registry.</li> </ol> <p>You can also set a shell variable to the output of this Make task.</p> <p>For example <pre><code>image=`make build-kubestellar-syncer-image-local`\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-build-the-image-with-multiple-architectures-and-push-it-to-docker-registry","title":"How to build the image with multiple architectures and push it to Docker registry","text":"<ol> <li><code>make build-kubestellar-syncer-image</code></li> </ol> <p>The behavior can be modified with some make variables; their default values are what get used in a normal build.  The variables are as follows.</p> <ul> <li><code>DOCKER_REPO</code>, <code>IMAGE_TAG</code>: the built multi-platform manifest will   be pushed to <code>$DOCKER_REPO:$IMAGE_TAG</code>.  The default for   <code>DOCKER_REPO</code> is <code>quay.io/kubestellar/syncer</code>.  The default for   <code>IMAGE_TAG</code> is the concatenation of: \"git-\", a short ID of the   current git commit, \"-\", and either \"clean\" or \"dirty\" depending on   what <code>git status</code> has to say about it.</li> <li><code>SYNCER_PLATFORMS</code>: a   comma-separated list of <code>docker build</code> \"platforms\".  The default is   \"linux/amd64,linux/arm64,linux/s390x\".</li> <li><code>ADDITIONAL_ARGS</code>: a word that will be added into the <code>ko build</code> command line.   The default is the empty string.</li> </ul> <p>For example <pre><code>$ make build-kubestellar-syncer-image DOCKER_REPO=ghcr.io/yana1205/kubestellar/syncer IMAGE_TAG=dev-2023-04-24-x SYNCER_PLATFORMS=linux/amd64,linux/arm64\n2023/04/24 11:50:16 Using base distroless.dev/static:latest@sha256:81018475098138883b80dcc9c1242eb02b53465297724b18e88591a752d2a49c for github.com/kubestellar/kubestellar/cmd/syncer\n2023/04/24 11:50:17 Building github.com/kubestellar/kubestellar/cmd/syncer for linux/arm64\n2023/04/24 11:50:17 Building github.com/kubestellar/kubestellar/cmd/syncer for linux/amd64\n2023/04/24 11:50:18 Publishing ghcr.io/yana1205/kubestellar/syncer:dev-2023-04-24-x\n2023/04/24 11:50:19 existing blob: sha256:85a5162a65b9641711623fa747dab446265400043a75c7dfa42c34b740dfdaba\n2023/04/24 11:50:20 pushed blob: sha256:00b7b3ca30fa5ee9336a9bc962efef2001c076a3149c936b436f409df710b06f\n2023/04/24 11:50:21 ghcr.io/yana1205/kubestellar/syncer:sha256-a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510.sbom: digest: sha256:4b1407327a486c0506188b67ad24222ed7924ba57576e47b59a4c1ac73dacd40 size: 368\n2023/04/24 11:50:21 Published SBOM ghcr.io/yana1205/kubestellar/syncer:sha256-a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510.sbom\n2023/04/24 11:50:21 existing blob: sha256:930413008565fd110e7ab2d37aab538449f058e7d83e7091d1aa0930a0086f58\n2023/04/24 11:50:22 pushed blob: sha256:bd830efcc6c0a934a273202ffab27b1a8927368a7b99c4ae0cf850fadb865ead\n2023/04/24 11:50:23 ghcr.io/yana1205/kubestellar/syncer:sha256-02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe.sbom: digest: sha256:5c79e632396b893c3ecabf6b9ba43d8f20bb3990b0c6259f975bf81c63f0e41e size: 369\n2023/04/24 11:50:23 Published SBOM ghcr.io/yana1205/kubestellar/syncer:sha256-02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe.sbom\n2023/04/24 11:50:24 existing blob: sha256:bb5ef9628a98afa48a9133f5890c43ed1499eb82a33fe173dd9067d7a9cdfb0a\n2023/04/24 11:50:25 pushed blob: sha256:61f19080792ae91e8b37ecf003376497b790a411d7a8fa4435c7457b0e15874c\n2023/04/24 11:50:25 ghcr.io/yana1205/kubestellar/syncer:sha256-c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96.sbom: digest: sha256:8d82388bb534933d7193c661743fca8378cc561a2ad8583c0107f687acb37c1b size: 369\n2023/04/24 11:50:25 Published SBOM ghcr.io/yana1205/kubestellar/syncer:sha256-c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96.sbom\n2023/04/24 11:50:26 existing manifest: sha256:02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe\n2023/04/24 11:50:26 existing manifest: sha256:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:27 ghcr.io/yana1205/kubestellar/syncer:dev-2023-04-24-x: digest: sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510 size: 690\n2023/04/24 11:50:27 Published ghcr.io/yana1205/kubestellar/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510\necho KO_DOCKER_REPO=ghcr.io/yana1205/kubestellar/syncer ko build --platform=linux/amd64,linux/arm64 --bare --tags ./cmd/syncer\nKO_DOCKER_REPO=ghcr.io/yana1205/kubestellar/syncer ko build --platform=linux/amd64,linux/arm64 --bare --tags ./cmd/syncer\nkubestellar-syncer image\nghcr.io/yana1205/kubestellar/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510\n</code></pre> <code>ghcr.io/yana1205/kubestellar/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510</code> is the image pushed to the registry.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-the-environment_1","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl kubestellar remove wmw example-wmw\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-bare-processes_1","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-kubernetes-workload_1","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/","title":"KubeStellar Mailbox Controller","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre></p> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/mailbox-controller.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#how-to-install-pre-requisites-for-a-windows-subsystem-for-linux-wsl-envronment-using-an-ubuntu-220401-distribution","title":"How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution","text":"<p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#1-if-youre-using-a-vpn-turn-it-off","title":"1. If you're using a VPN, turn it off","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#2-install-ubuntu-into-wsl","title":"2. Install Ubuntu into WSL","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#20-if-wsl-is-not-yet-installed-open-a-powershell-administrator-window-and-run-the-following","title":"2.0 If wsl is not yet installed, open a powershell administrator window and run the following","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#21-reboot-your-system","title":"2.1 reboot your system","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#22-in-a-windows-command-terminal-run-the-following-to-list-all-the-linux-distributions-that-are-available-online","title":"2.2 In a Windows command terminal run the following to list all the linux distributions that are available online","text":"<pre><code>wsl -l -o\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#23-select-a-linux-distribution-and-install-it-into-wsl","title":"2.3 Select a linux distribution and install it into WSL","text":"<p><pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#24-enter-your-new-username-and-password-at-the-prompts-and-you-will-eventually-see-something-like","title":"2.4 Enter your new username and password at the prompts, and you will eventually see something like:","text":"<pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#25-click-on-the-windows-start-icon-and-type-in-the-name-of-your-distribution-into-the-search-box","title":"2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box.","text":"<p>Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#3-install-pre-requisites-into-your-new-vm","title":"3. Install pre-requisites into your new VM","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#31-update-and-apply-apt-get-packages","title":"3.1 update and apply apt-get packages","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#32-install-golang","title":"3.2 Install golang","text":"<pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#33-install-ko-but-dont-do-ko-set-action-step","title":"3.3 Install ko (but don't do ko set action step)","text":"<pre><code>go install github.com/google/ko@latest\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#34-install-gcc","title":"3.4 Install gcc","text":"<p>Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#35-install-make-if-you-installed-build-essential-this-may-already-be-installed","title":"3.5 Install make (if you installed build-essential this may already be installed)","text":"<pre><code>apt install make\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#36-install-jq","title":"3.6 Install jq","text":"<pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#37-install-kubectl","title":"3.7 install kubectl","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#38-install-helm-required-when-deploying-as-workload","title":"3.8 install helm (required when deploying as workload)","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#how-to-install-docker-and-kind-into-a-windows-subsystem-for-linux-wsl-environment-using-an-ubuntu-220401-distribution","title":"How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#10-start-a-vm-terminal-by-clicking-on-the-app-you-configured-using-the-instructions-in-the-general-pre-requisites-described-above","title":"1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#20-install-docker","title":"2.0 Install docker","text":"<p>The installation instructions from docker are not sufficient to get docker working with WSL</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#21-follow-instructions-here-to-install-docker-httpsdocsdockercomengineinstallubuntu","title":"2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/","text":"<p>Here some additonal steps you will need to take:</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#22-ensure-that-etcwslconf-is-configured-so-that-systemd-will-run-on-booting","title":"2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting.","text":"<p>If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#23-edit-etcsudoers-it-is-strongly-recommended-to-not-add-directives-directly-to-etcsudoers-but-instead-to-put-them-in-files-in-etcsudoersd-which-are-auto-included-so-makemodify-a-new-file-via","title":"2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via","text":"<p><pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#24-add-your-user-to-the-docker-group","title":"2.4 Add your user to the docker group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#25-if-dockerd-is-already-running-then-stop-it-and-restart-it-as-follows-note-the-new-dockerd-instance-will-be-running-in-the-foreground","title":"2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground):","text":"<pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#251-if-you-encounter-an-iptables-issue-which-is-described-here-httpsgithubcommicrosoftwslissues6655","title":"2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655","text":"<p>The following commands will fix the issue:  <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#3-you-will-now-need-to-open-new-terminals-to-access-the-vm-since-dockerd-is-running-in-the-foreground-of-this-terminal","title":"3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#31-in-your-new-terminal-install-kind","title":"3.1 In your new terminal, install kind","text":"<pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#linking-synctarget-with-mailbox-workspace","title":"Linking SyncTarget with Mailbox Workspace","text":"<p>For a given SyncTarget T, the mailbox controller currently chooses the name of the corresponding workspace to be the concatenation of the following:</p> <ul> <li>the ID of the logical cluster containing T</li> <li>the string \"-mb-\"</li> <li>T's UID</li> </ul> <p>The mailbox workspace gets an annotation whose key is <code>edge.kubestellar.io/sync-target-name</code> and whose value is the name of the workspace object (as seen in its parent workspace, the edge service provider workspace).</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#usage","title":"Usage","text":"<p>The mailbox controller needs three Kubernetes client configurations. One --- concerned with reading inventory --- is to access the APIExport view of the <code>edge.kubestellar.io</code> API group, to read the <code>SyncTarget</code> objects.  This must be a client config that is pointed at the workspace (which is always <code>root:espw</code>, as far as I know) that has this APIExport and is authorized to read its view.  Another client config is needed to give read/write access to all the mailbox workspaces, so that the controller can create <code>APIBinding</code> objects to the edge APIExport in those workspaces; this should be a client config that is able to read/write in all clusters.  For example, that is in the kubeconfig context named <code>base</code> in the kubeconfig created by <code>kcp start</code>.  Finally, the controller also needs a kube client config that is pointed at the root workspace and is authorized to consume the <code>Workspace</code> objects from there.</p> <p>The command line flags, beyond the basics, are as follows.</p> <pre><code>      --concurrency int                  number of syncs to run in parallel (default 4)\n--espw-path string                 the pathname of the edge service provider workspace (default \"root:espw\")\n--mbws-cluster string              The name of the kubeconfig cluster to use for access to mailbox workspaces (really all clusters)\n--mbws-context string              The name of the kubeconfig context to use for access to mailbox workspaces (really all clusters) (default \"base\")\n--mbws-kubeconfig string           Path to the kubeconfig file to use for access to mailbox workspaces (really all clusters)\n--mbws-user string                 The name of the kubeconfig user to use for access to mailbox workspaces (really all clusters)\n--server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10203)\n--root-cluster string              The name of the kubeconfig cluster to use for access to the root workspace\n      --root-context string              The name of the kubeconfig context to use for access to the root workspace (default \"root\")\n--root-kubeconfig string           Path to the kubeconfig file to use for access to the root workspace\n      --root-user string                 The name of the kubeconfig user to use for access to the root workspace\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#try-out-the-mailbox-controller","title":"Try out the mailbox controller","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#pull-the-kcp-and-kubestellar-source-code-build-the-kubectl-ws-binary-and-start-kcp","title":"Pull the kcp and KubeStellar source code, build the kubectl-ws binary, and start kcp","text":"<p>Open a terminal window(1) and clone the latest KubeStellar source:</p> <p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window). Set your <code>KUBECONFIG</code> environment variable to name the kubernetes client config file that <code>kcp</code> generates. <pre><code>kcp start &amp;&gt; /dev/null &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#create-the-edge-service-provider-workspace-espw","title":"Create the Edge Service Provider Workspace (ESPW)","text":"<p>Open another terminal window(2) and point <code>$KUBECONFIG</code> to the admin kubeconfig for the kcp server and include the location of kubectl-ws in <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>Next, use the command that makes sure the Edge Service Provider Workspace (ESPW), which is <code>root:espw</code>, and the TMC provider workspace (<code>root:compute</code>) are properly set up.</p> <pre><code>kubestellar init\n</code></pre> <p>After that, a run of the controller should look like the following.</p> <pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>I0305 18:06:20.046741   85556 main.go:110] \"Command line flag\" add_dir_header=\"false\"\nI0305 18:06:20.046954   85556 main.go:110] \"Command line flag\" alsologtostderr=\"false\"\nI0305 18:06:20.046960   85556 main.go:110] \"Command line flag\" concurrency=\"4\"\nI0305 18:06:20.046965   85556 main.go:110] \"Command line flag\" inventory-context=\"root\"\nI0305 18:06:20.046971   85556 main.go:110] \"Command line flag\" inventory-kubeconfig=\"\"\nI0305 18:06:20.046976   85556 main.go:110] \"Command line flag\" log_backtrace_at=\":0\"\nI0305 18:06:20.046980   85556 main.go:110] \"Command line flag\" log_dir=\"\"\nI0305 18:06:20.046985   85556 main.go:110] \"Command line flag\" log_file=\"\"\nI0305 18:06:20.046989   85556 main.go:110] \"Command line flag\" log_file_max_size=\"1800\"\nI0305 18:06:20.046993   85556 main.go:110] \"Command line flag\" logtostderr=\"true\"\nI0305 18:06:20.046997   85556 main.go:110] \"Command line flag\" one_output=\"false\"\nI0305 18:06:20.047002   85556 main.go:110] \"Command line flag\" server-bind-address=\":10203\"\nI0305 18:06:20.047006   85556 main.go:110] \"Command line flag\" skip_headers=\"false\"\nI0305 18:06:20.047011   85556 main.go:110] \"Command line flag\" skip_log_headers=\"false\"\nI0305 18:06:20.047015   85556 main.go:110] \"Command line flag\" stderrthreshold=\"2\"\nI0305 18:06:20.047019   85556 main.go:110] \"Command line flag\" v=\"2\"\nI0305 18:06:20.047023   85556 main.go:110] \"Command line flag\" vmodule=\"\"\nI0305 18:06:20.047027   85556 main.go:110] \"Command line flag\" workload-context=\"\"\nI0305 18:06:20.047031   85556 main.go:110] \"Command line flag\" workload-kubeconfig=\"\"\nI0305 18:06:20.070071   85556 main.go:247] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\nI0305 18:06:20.072088   85556 shared_informer.go:282] Waiting for caches to sync for mailbox-controller\nI0305 18:06:20.172169   85556 shared_informer.go:289] Caches are synced for mailbox-controller\nI0305 18:06:20.172196   85556 main.go:210] \"Informers synced\"\n</code></pre> <p>In a separate terminal window(3), create an inventory management workspace as follows.</p> <pre><code>kubectl ws \\~\nkubectl ws create imw --enter\nkubectl kcp bind apiexport root:espw:edge.kubestellar.io\n</code></pre> <p>Then in that workspace, run the following command to create a <code>SyncTarget</code> object.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\n  name: stest1\nspec:\n  cells:\n    foo: bar\nEOF\n</code></pre> <p>That should provoke logging like the following from the mailbox controller.</p> <pre><code>I0305 18:07:20.490417   85556 main.go:369] \"Created missing workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>And you can verify that as follows:</p> <pre><code>kubectl ws .\nkubectl get synctargets.edge.kubestellar.io\n</code></pre> <p><pre><code>kubectl ws root\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl ws tree kubectl get workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\nniqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368   universal            Ready   https://192.168.58.123:6443/clusters/0ay27fcwuo2sv6ht   22s\n</code></pre></p> <p>FYI, if you look inside that workspace you will see an <code>APIBinding</code> named <code>bind-edge</code> that binds to the <code>APIExport</code> named <code>edge.kubestellar.io</code> from the edge service provider workspace (and this is why the controller needs to know the pathname of that workspace), so that the edge API is available in the mailbox workspace.</p> <p>Next, <code>kubectl delete</code> that workspace, and watch the mailbox controller wait for it to be gone and then re-create it.</p> <pre><code>I0305 18:08:15.428884   85556 main.go:369] \"Created missing workspace\" worker=2 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>Finally, go back to your inventory workspace to delete the <code>SyncTarget</code>:</p> <p><pre><code>kubectl ws \\~\nkubectl ws .\nkubectl ws imw\nkubectl ws .\nkubectl get synctargets.edge.kubestellar.io\nkubectl delete synctargets.edge.kubestellar.io stest1\n</code></pre> and watch the mailbox controller react as follows.</p> <pre><code>I0305 18:08:44.380421   85556 main.go:352] \"Deleted unwanted workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl kubestellar remove wmw example-wmw\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/outline/","title":"Details","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-of-this-memo","title":"Status of this memo","text":"<p>This summarizes the design for a Proof-of-Concept intended to be completed in early 2023.  It has been completed except for the summarization feature.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#introduction","title":"Introduction","text":"<p>This is a quick demo of a fragment of what we think is needed for edge multi-cluster.  It is intended to demonstrate the following points.</p> <ul> <li>Separation of infrastructure and workload management.</li> <li>The focus here is on workload management, and that strictly reads   an inventory of infrastructure.</li> <li>What passes from inventory to workload management is kcp TMC   Location and SyncTarget objects.</li> <li>Use of a kcp workspace as the container for the central spec of a workload.</li> <li>Propagation of desired state from center to edge, as directed by   EdgePlacement objects and the Location and SyncTarget objects they reference.</li> <li>Interfaces designed for a large number of edge clusters.</li> <li>Interfaces designed with the intention that edge clusters operate   independently of each other and the center (e.g., can tolerate only   occasional connectivity) and thus any \"service providers\" (in the   technical sense from kcp) in the center or elsewhere.</li> <li>Rule-based customization of desired state.</li> <li>Propagation of reported state from edge to center.</li> <li>Summarization of reported state in the center.</li> <li>Return and/or summarization of state from associated objects (e.g.,   ReplicaSet or Pod objects associated with a given Deployment   object).</li> <li>The edge opens connections to the center, not vice-versa.</li> <li>An edge computing platform \"product\" that can be deployed (as   opposed to a service that is used).</li> </ul> <p>Some important things that are not attempted in this PoC include the following.</p> <ul> <li>An implementation that supports a large number of edge clusters or   any other thing that requires sharding for scale. In this PoC we   will use a single kcp server to hold all the workspaces, and will   not shard any controller.</li> <li>More than one SyncTarget per Location.</li> <li>A hierarchy with more than two levels.</li> <li>User control over ordering of propagation from center to edge,   either among destinations or kinds of objects.</li> <li>More than baseline security (baseline being, e.g., HTTPS, Secret   objects, non-rotating bearer token based service authentication).</li> <li>A good design for bootstrapping the workload management in the edge   clusters.</li> <li>Very strong isolation between tenants in the edge computing   platform.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#development-roadmap-for-this-poc","title":"Development Roadmap for this PoC","text":"<p>Some features will get implemented later than others, so that we can start being able to run interesting end-to-end scenarios relatively soon.  Following is a list of features that will not be implemented at first.</p> <p>Of the following features, customization will be needed before the others.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#customization","title":"Customization","text":"<p>We can have a complete system that ignores customization, as long as it is only used for workloads that need no customization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#summarization","title":"Summarization","text":"<p>We can omit summarization at first.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#return-andor-summarization-of-state-from-associated-objects","title":"Return and/or summarization of state from associated objects","text":"<p>This will involve both defining a scalable interface for declaring what should be returned as well as implementing it.  This will certainly affect the syncer between mailbox workspace and edge cluster, and the summarization part will affect the status summarizer.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#good-handling-of-workload-conflicts","title":"Good handling of workload conflicts","text":"<p>We could start by handling workload conflicts in a very simple way: treating each as an error.  Later development can handle them better, as outlined later.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#denaturingrenaturing","title":"Denaturing/renaturing","text":"<p>We could start by not doing this.  For some resources, the effect of leaving these resources natured in the center is only to add authorizations in the center that are not needed and are undesired in a well-secured environment but tolerable in early demonstrations --- provided that there is not a conflict with an object of the same name that is positively desired in the center.  In particular, these are: <code>ClusterRole</code>, <code>ClusterRoleBinding</code>, <code>Role</code>, <code>RoleBinding</code>, and, depending on the Kubernetes release and usage style, <code>ServiceAccount</code>. The extra consideration for <code>ServiceAccount</code> is when an associated <code>Secret</code> is a natural consequence.  However, that is not a practical problem because such <code>Secret</code> objects are recognized as system infrastructure (see below).  Another consideration for <code>ServiceAccount</code> objects, as for <code>Secret</code> and <code>ConfigMap</code> objects, is that some are in some sense \"reverse-natured\": some are created by some other thing as part of the nature of that other thing (object or external system).  Another way of looking at these particular objects is that they are system infrastructure.</p> <p>For some kinds of object, lack of denaturing/renaturing means that KubeStellar will simply not be able to support workloads that contain such objects.  These are: <code>MutatingWebhookConfiguration</code>, <code>ValidatingWebhookConfiguration</code>, <code>LimitRange</code>, <code>ResourceQuota</code>.</p> <p>For some resources, the need to denature is only a matter of anticipation.  <code>FlowSchema</code> and <code>PriorityLevelConfiguration</code> currently are not interpreted by kcp servers and so are effectively already denatured there.  Hopefully they will be given interpretations in the future, and then those resources will join the previous category.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#roles-and-responsibilities","title":"Roles and Responsibilities","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developersdeployersadminsusers-of-the-infrastructure-management-layer","title":"Developers/deployers/admins/users of the infrastructure management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developers-of-the-workload-management-layer","title":"Developers of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#deployersadmins-of-the-workload-management-layer","title":"Deployers/admins of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#users-of-the-workload-management-layer","title":"Users of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#design-overview","title":"Design overview","text":"<p>In very brief: the design approach is to achieve the multicast semantics of edge placement by two layers of activity.  Between the two layers sit mailbox workspaces: these exist in the center, and there is one for each edge cluster.  One layer of activity runs in the center and relates the edge placement problems to mailbox workspace contents.  The other layer is syncers, one in each edge cluster, that relate the corresponding mailbox contents with their local clusters.</p> <p>As in TMC, in this design we have downsync and upsync --- but they are a little more complicated here.  Downsync involves propagation of desired state from workload management workspace through mailbox workspaces to edge and return/summarization of reported state.  Upsync involves return/summarization of desired and reported state of objects born on the edge clusters.  On the inward path, the reported or full state goes from edge to the mailbox workspace and then is summarized to the workload management workspace.  State propagation is maintained in an eventually consistent way, it is not just one-and-done.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-management-workspaces-imw","title":"Inventory Management workspaces (IMW)","text":"<p>In this design the primary interface between infrastructure management and workload management is API objects in inventory management workspaces.  We use the <code>Location</code> and <code>SyncTarget</code> object types from KubeStellar's edge.kubestellar.io api group (created by <code>kubestellar init</code>) for this purpose.  The people doing infrastructure management are responsible for creating the inventory management workspaces and populating them with <code>Location</code> and <code>SyncTarget</code> objects, one <code>Location</code> and one <code>SyncTarget</code> per edge cluster.  These inventory management workspaces need to use APIBindings to APIExports defining <code>Location</code> and <code>SyncTarget</code> so that the workload management layer can use one APIExport view for each API group to read those objects.</p> <p>To complete the plumbing of the syncers, each inventory workspace that contains a SyncTarget needs to also contain the following associated objects.  FYI, these are the things that <code>kubectl kcp workload sync</code> directly creates besides the SyncTarget.  Ensuring their presence is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ol> <li>A ServiceAccount that the syncer will authenticate as.</li> <li>A ClusterRole manipulating that SyncTarget and the    APIResourceImports (what are these?).</li> <li>A ClusterRoleBinding that links that ServiceAccount with that    ClusterRole.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-service-provider-workspace-espw","title":"Edge Service Provider workspace (ESPW)","text":"<p>The edge multi-cluster service is provided by one workspace that includes the following things.</p> <ul> <li>An APIExport of the edge API group.</li> <li>The edge controllers: Where Resolver, Placement Translator, Mailbox   Controller, and Status Summarizer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-management-workspaces-wmw","title":"Workload Management workspaces (WMW)","text":"<p>The users of edge multi-cluster primarily maintain these.  Each one of these has both control (API objects that direct the behavior of the edge computing platform) and data (API objects that hold workload desired and reported state).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#data-objects","title":"Data objects","text":"<p>The workload desired state is represented by kube-style API objects, in the way that is usual in the Kubernetes milieu.  For edge computing we need to support both cluster-scoped (AKA non-namespaced) kinds as well as namespaced kinds of objects.</p> <p>The use of a workspace as a mere container presents a challenge, because some kinds of kubernetes API objects at not merely data but also modify the behavior of the apiserver holding them.  To resolve this dilemma, the edge users of such a workspace will use a special view of the workspace that holds only data objects.  The ones that modify apiserver behavior will be translated by the view into \"denatured\" versions of those objects in the actual workspace so that they have no effect on it.  And for these objects, the transport from center-to-edge will do the inverse: translate the denatured versions into the regular (\"natured\"?) versions for appearance in the edge cluster.  Furthermore, for some kinds of objects that modify apiserver behavior we want them \"natured\" at both center and edge.  There are thus a few categories of kinds of objects.  Following is a listing, with with the particular kinds that appear in kcp or plain kubernetes.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-denatured-in-center-natured-in-edge","title":"Needs to be denatured in center, natured in edge","text":"<p>For these kinds of objects: clients of the real workload management workspace can manipulate some such objects that will modify the behavior of the workspace, while clients of the edge computing view will manipulate distinct objects that have no effect on the behavior of the workspace.  These are kinds of objects to which kcp normally associates some behavior.  To be fully precise, the concern here is with behavior that is externally visible (including externally visible behavior of the server itself); we do not care to dissociate server-internal behavior such as storing encrypted at rest.  The edge computing platform will have to implement that view which dissociates the normal kcp behavior.</p> APIVERSION KIND NAMESPACED admissionregistration.k8s.io/v1 MutatingWebhookConfiguration false admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration false apiregistration.k8s.io/v1 APIService false flowcontrol.apiserver.k8s.io/v1beta2 FlowSchema false flowcontrol.apiserver.k8s.io/v1beta2 PriorityLevelConfiguration false rbac.authorization.k8s.io/v1 ClusterRole false rbac.authorization.k8s.io/v1 ClusterRoleBinding false rbac.authorization.k8s.io/v1 Role true rbac.authorization.k8s.io/v1 RoleBinding true v1 LimitRange true v1 ResourceQuota true v1 ServiceAccount true <p>NOTE: The denaturing described here is not implemented yet.  The kinds of objects listed above can be put into a workload management workspace and it will give them its usual interpretation. For ones that add authorizations, this will indeed add authorizations but not otherwise break something. The kcp server does not implement <code>FlowSchema</code> nor <code>PriorityLevelConfiguration</code>; those will indeed be uninterpreted. For objects that configure calls to other servers, these will fail unless the user arranges for them to work when made in the center as well in the edge clusters.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-and-edge","title":"Needs to be natured in center and edge","text":"<p>These should have their usual effect in both center and edge; they need no distinct treatment.</p> <p>Note, however, that they do have some sequencing implications.  They have to be created before any dependent objects, deleted after all dependent objects.</p> APIVERSION KIND NAMESPACED apiextensions.k8s.io/v1 CustomResourceDefinition false v1 Namespace false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-not-destined-for-edge","title":"Needs to be natured in center, not destined for edge","text":"APIVERSION KIND NAMESPACED apis.kcp.io/v1alpha1 APIBinding false <p>A workload management workspace needs an APIBinding to the APIExport of the API group <code>edge.kubestellar.io</code> from the edge service provider workspace, in order to be able to contain EdgePlacement and related objects.  These objects and that APIBinding are not destined for the edge clusters.</p> <p>The edge clusters are not presumed to be kcp workspaces, so APIBindings do not propagate to the edge clusters.  However, it is possible that APIBindings for workload APIs may exist in a workload management workspace and be selected for downsync to mailbox workspaces while the edge clusters have the same resources defined by CRDs (as mentioned later in the discussion of built-in resources and namespaces).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#for-features-not-supported","title":"For features not supported","text":"<p>These are part of k8s or kcp APIs that are not supported by the edge computing platform.</p> APIVERSION KIND NAMESPACED apiresource.kcp.io/v1alpha1 APIResourceImport false apiresource.kcp.io/v1alpha1 NegotiatedAPIResource false apis.kcp.io/v1alpha1 APIConversion false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#not-destined-for-edge","title":"Not destined for edge","text":"<p>These kinds of objects are concerned with either (a) TMC control or (b) workload data that should only exist in the edge clusters.  These will not be available in the view used by edge clients to maintain their workload desired and reported state.</p> APIVERSION KIND NAMESPACED apis.kcp.io/v1alpha1 APIExport false apis.kcp.io/v1alpha1 APIExportEndpointSlice false apis.kcp.io/v1alpha1 APIResourceSchema false apps/v1 ControllerRevision true authentication.k8s.io/v1 TokenReview false authorization.k8s.io/v1 LocalSubjectAccessReview true authorization.k8s.io/v1 SelfSubjectAccessReview false authorization.k8s.io/v1 SelfSubjectRulesReview false authorization.k8s.io/v1 SubjectAccessReview false certificates.k8s.io/v1 CertificateSigningRequest false coordination.k8s.io/v1 Lease true core.kcp.io/v1alpha1 LogicalCluster false core.kcp.io/v1alpha1 Shard false events.k8s.io/v1 Event true ~~scheduling.kcp.io/v1alpha1~~ ~~Location~~ ~~false ~~ scheduling.kcp.io/v1alpha1 Placement false tenancy.kcp.io/v1alpha1 ClusterWorkspace false tenancy.kcp.io/v1alpha1 Workspace false tenancy.kcp.io/v1alpha1 WorkspaceType false topology.kcp.io/v1alpha1 Partition false topology.kcp.io/v1alpha1 PartitionSet false v1 Binding true v1 ComponentStatus false v1 Event true v1 Node false ~~workload.kcp.io/v1alpha1~~ ~~SyncTarget~~ ~~false~~"},{"location":"Coding%20Milestones/PoC2023q1/outline/#already-denatured-in-center-want-natured-in-edge","title":"Already denatured in center, want natured in edge","text":"<p>These are kinds of objects that kcp already gives no interpretation to, and that is what KubeStellar needs from the center workspaces.</p> <p>This is the default category of kind of object --- any kind of data object not specifically listed in another category is implicitly in this category.  Following are the kinds from k8s and kcp that fall in this category.</p> APIVERSION KIND NAMESPACED apps/v1 DaemonSet true apps/v1 Deployment true apps/v1 ReplicaSet true apps/v1 StatefulSet true autoscaling/v2 HorizontalPodAutoscaler true batch/v1 CronJob true batch/v1 Job true networking.k8s.io/v1 Ingress true networking.k8s.io/v1 IngressClass false networking.k8s.io/v1 NetworkPolicy true node.k8s.io/v1 RuntimeClass false policy/v1 PodDisruptionBudget true scheduling.k8s.io/v1 PriorityClass false storage.k8s.io/v1 CSIDriver false storage.k8s.io/v1 CSINode false storage.k8s.io/v1 CSIStorageCapacity true storage.k8s.io/v1 StorageClass false storage.k8s.io/v1 VolumeAttachment false v1 ConfigMap true v1 Endpoints true v1 PersistentVolume false v1 PersistentVolumeClaim true v1 Pod true v1 PodTemplate true v1 ReplicationController true v1 Secret true v1 Service true <p>Note that some <code>ConfigMap</code> and <code>Secret</code> objects are treated differently, as explained in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#system-infrastructure-objects","title":"System infrastructure objects","text":"<p>Even in a kcp workspace, some certain objects --- called here \"system infrastructure objects\" --- are created as a consequence of certain other objects or things.  The system infrastructure objects are tolerated in the center and do not propagate toward the edge.  Here is an initial list of system infrastructure objects:</p> <ul> <li><code>Secret</code> objects whose type is   <code>kubernetes.io/service-account-token</code> (these are automatically   created to support a <code>ServiceAccount</code> in some circumstances) or   <code>bootstrap.kubernetes.io/token</code>;</li> <li><code>ConfigMap</code> objects named <code>kube-root-ca.crt</code>;</li> <li><code>ServiceAcount</code> objects named <code>default</code> (these are automatically   created as a consequence of a namespace being created).</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#built-in-resources-and-namespaces","title":"Built-in resources and namespaces","text":"<p>An edge cluster has some built-in resources (i.e, kinds of objects) and namespaces.  A resource may be built-in by any of several ways: it can be built-in to the apiserver, it can be defined by a CRD, its API group can be delegated by an APIService to a custom external server (each of the latter two is sometimes called \"aggregation\").  Note also that a resource may be defined in edge clusters one way (e.g., by being built into kube-apiserver) and in the workload management workspace another way (e.g., by a CustomResourceDefinition).</p> <p>In this PoC, all edge clusters are considered to have the same built-in resources and namespaces.</p> <p>As a matter of scoping the work here, it is also assumed that each API group built into the edge clusters supports the API versions chosen by the conflict resolution rules below when they are applied to the workload sources.</p> <p>At deployment time the workload management platform is configured with lists of resources and namespaces built into the edge clusters.</p> <p>Propagation from center to edge does not attempt to manage the resource and namespace definitions that are built into the edge clusters.</p> <p>The mailbox workspaces will have built-in resources and namespaces that are a subset of those built into the edge clusters.  The propagation from workload management workspace to mailbox workspace does not attempt to manage the resource and namespace definitions that are built into the mailbox workspaces.</p> <p>The above wording is deliberately restrained, for the sake of flexibility regarding resources that are defined one way in the edge clusters and another way in workload management workspace.  For example, the following scenario is allowed.</p> <ul> <li>Some central team owns an API group and produces some   CustomResourceDefinition (CRD) objects that populate that API group.</li> <li>That team derives APIResourceSchemas from those CRDs and a   corresponding APIExport of their API group.</li> <li>That team maintains a kcp workspace holding those APIResourceSchemas   and that APIExport.</li> <li>Some workload management workspaces have APIBindings to that   APIExport, and EdgePlacement objects that (1) select those   APIBinding objects for downsync and (2) select objects of kinds   defined through those APIBindings for either downsync or upsync.</li> <li>Those resources are built into the edge clusters by pre-deploying   the aforementioned CRDs there.</li> <li>Those resources are not built into the mailbox workspaces.  In   this case the APIBindings would propagate from workload management   workspace to mailbox workspaces but not edge clusters.</li> <li>As a consequence of those propagated APIBindings, the APIExport's   view includes all of the objects (in workload management workspaces,   in mailbox workspaces, and in any other workspaces where they   appear) whose kind is defined through those APIBindings.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#control-objects","title":"Control objects","text":"<p>These are the EdgePlacement objects, their associated SinglePlacementSlice objects, and the objects that direct customization and summarization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edgeplacement-objects","title":"EdgePlacement objects","text":"<p>One of these is a binding between a \"what\" predicate and a \"where\" predicate.</p> <p>Overlaps between EdgePlacement objects are explicitly allowed.  Two EdgePlacement objects may have \"where\" predicates that both match some of the same destinations.  Two EdgePlacement objects may have \"what\" predicates that match some of the same workload descriptions.  Two EdgePlacement objects may overlap in both ways.</p> <p>An EdgePlacement object deliberately only binds \"what\" and \"where\", without any adverbs (such as prescriptions of customization or summarization).  This means that overlapping EdgePlacement objects can not conflict in those adverbs.</p> <p>However, another sort of conflict remains possible.  This is because the user controls the IDs --- that is, the names --- of the parts of the workload.  In full, a Kubernetes API object is identified by API group, API major version, Kind (equivalently, resource name), namespace if relevant, and name.  For simplicity in this PoC we will not worry about differences in API major version; each API group in Kubernetes and/or kcp currently has only one major version.</p> <p>Two different workload descriptions can have objects with the same ID (i.e., if they appear in different workspaces).  These objects, when rendered to the same API version, might have different values.  And the objects may be available in different API versions in different source workspaces.  See client-go for what an API server says about which versions it can serve for a given API group, and meta/v1 for the supporting details on an APIGroup struct.</p> <p>When multiple workload objects with the same APIGroup, Kind, namespace (if namespaced), and name are directed to the same edge cluster, they are merged with conflicts handled by (a) a rule for resolution and (b) reporting via both error logging and Kubernetes Event creation.  These conflicts are serious matters: they mean user expectations are not being met (because they are inconsistent); this is why the placement translator tries hard to make the user aware.</p> <p>The first part of merging a set of objects is to read them all at the same API version.  The placement translator solves the problem of picking API version at the level of API groups rather than object-by-object.  The API version for an given API group is chosen as follows.  First, take the intersection of the supported versions from the various sources.  If this intersection is empty then this is a conflict.  It is resolved by throwing out the APIGroup with the lowest version and repeating with the reduced set of APIGroup structs. Next, take the union of the preferred versions. If this union has a non-empty intersection with the intersection of the supported versions, take the following steps with this intersection; otherwise proceed with just the intersection of the supported versions.  When first (since process startup) presented with an instance of this problem, the placement translator picks the highest version from this intersection.  Subsequently for the same API group, the placement translator sticks with its previous decision as long as that is still in the intersection.  If the previous choice is no longer available, the highest version is picked.  This preference for highest version is based on the expectation that rolling forward will be more common than rolling back; using the intersection ensures that both work (as long as the collection of sources has an overlap in supported versions, which is basic sanity).</p> <p>A workload prescription object that is in the process of graceful deletion (i.e., with <code>DeletionTimestamp</code> set to something) is considered here to already be gone.</p> <p>Once they have been read at a consistent API version, merging of multiple objects is done as follows.  Different parts of the object are handled differently.</p> <ul> <li>TypeMeta.  This can not conflict because it is part of what   identifies an object.</li> <li>ObjectMeta.</li> <li>Labels and Annotations.  These are handled on a key-by-key     basis.  Distinct keys do not conflict.  When multiple objects have     a label or annotation with the same key, the corresponding value     in the result is the value from the most recently updated of those     objects.</li> <li>OwnerReferences.  This is handled analogously to labels and     annotations.  The key is the combination of APIVersion, Kind, and     Name.</li> <li>Finalizers.  This is simply a set of strings.  The result of     merging is the union of the sets.</li> <li>ManagedFields.  This is metadata that is not propagated.</li> <li>Spec.  Beyond TypeMeta and ObjectMeta, the remaining object   fields are specific to the kind of object.  Many have a field named   \"Spec\" in the Go language source, \"spec\" in the JSON representation.   For objects that have Spec fields, merging has a conflict if those   field values are not all equal when considered as JSON data, and the   resolution is to take the value from the most recently updated   object.</li> <li>Status.  Status is handled analogously to Spec.  For both, we   consider a missing field to be the same as a field with a value of   <code>nil</code>.  That is expected to be the common case for the Status of   these workload prescription objects.</li> <li>Other fields.  If all the values are maps (objects in   JavaScript) then they are merged key-by-key, as for labels and   annotations.  Otherwise they are treated as monoliths, as for Spec   and Status.</li> </ul> <p>For the above, the most recently updated object is determined by parsing the ResourceVersion as an <code>int64</code> and picking the highest value.  This is meaningful under the assumption that all the source workspaces are from the same kcp server --- which will be true for this PoC but is not a reasonable assumption in general.  Also: interpreting ResourceVersion breaks a rule for Kubernetes clients --- but this is dismayingly common.  Beyond this PoC we could hope to do better by looking at the ManagedFields.  But currently kcp does not include https://github.com/kubernetes/kubernetes/pull/110058 so the ManagedFields often do not properly reflect the update times.  Even so, those timestamps have only 1-second precision --- so conflicts will remain possible (although, hopefully, unlikely).</p> <p>There is special handling for Namespace objects.  When a workload includes namespaced objects, the propagation has to include ensuring that the corresponding Namespace object exists in the destination.  An EdgePlacement's \"what\" predicate MAY fail to match a relevant Namespace object.  This is taken to mean that this EdgePlacement is not requesting propagation of the details (Spec, labels, etc.) of that Namespace object but only expects propagation to somehow ensure that the namespace exists.  When merging overlapping workloads that have namespaces in common, only the Namespace objects that come from matching a \"what\" predicate need to be merged.</p> <p>The above also provide an answer to the question of what version is used when writing to the mailbox workspace and edge cluster.  The version used for that is the version chosen above.  In the case of no conflicts, this means that the writes are done using the preferred version from the API group from the workload management workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-workspaces","title":"Mailbox workspaces","text":"<p>The mailbox controller maintains one mailbox workspace for each SyncTarget.  A mailbox workspace acts as a workload source for the corresponding syncer, prescribing the workload to go to the corresponding edge cluster and the <code>SyncerConfig</code> object that guides the syncer.</p> <p>A mailbox workspace contains the following items.</p> <ol> <li>APIBindings (maintained by the mailbox controller) to APIExports of    workload object types.</li> <li>Workload objects, post customization in the case of downsynced    objects.</li> <li>A <code>SyncerConfig</code> object.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-cluster","title":"Edge cluster","text":"<p>Also called workload execution cluster.</p> <p>One of these contains the following items.  FYI, these are the things in the YAML output by <code>kubectl kcp workload edge-sync</code>.  The responsibility for creating and maintaining these objects is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ul> <li>A namespace that holds the syncer and associated objects.</li> <li>A ServiceAccount that the syncer authenticates as when accessing the   views of the center and when accessing the edge cluster.</li> <li>A Secret holding that ServiceAccount's authorization token.</li> <li>A ClusterRole listing the non-namespaced privileges that the   syncer will use in the edge cluster.</li> <li>A ClusterRoleBinding linking the syncer's ServiceAccount and ClusterRole.</li> <li>A Role listing the namespaced privileges that the syncer will use in   the edge cluster.</li> <li>A RoleBinding linking the syncer's ServiceAccount and Role.</li> <li>A Secret holding the kubeconfig that the syncer will use to access   the edge cluster.</li> <li>A Deployment of the syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-controller","title":"Mailbox Controller","text":"<p>This controller maintains one mailbox workspace per SyncTarget.  Each of these mailbox workspaces is used for a distinct syncing problem: downsynced objects go here from their workload management workspaces, and upsynced objects go here from the edge cluster.  These workspaces are all children of the edge service provider workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#where-resolver","title":"Where Resolver","text":"<p>This controller monitors the EdgePlacement, Location, and SyncTarget objects and maintains the results of matching.  For each EdgePlacement object this controller maintains an associated collection of SinglePlacementSlice objects holding the matches for that EdgePlacement.  These SinglePlacementSlice objects appear in the same workspace as the corresponding EdgePlacement; the remainder of how they are linked is TBD.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#placement-translator","title":"Placement Translator","text":"<p>This controller continually monitors all the EdgePlacement objects, SinglePlacementSlice objects, and related workload objects, and maintains the proper projections of those into mailbox workspace contents.  The customization, if any, is done in this process.  Note also that everything that has to be denatured in the workload management workspace also has to be denatured in the mailbox workspace.</p> <p>The job of the placement translator can be broken down into the following five parts.</p> <ul> <li>Resolve each EdgePlacement's \"what\" part to a list of particular   workspace items (namespaces and non-namespaced objects).</li> <li>Monitor the SinglePlacementSlice objects that report the Where Resolver's   resolutions of the \"where\" part of the EdgePlacement objects.</li> <li>Maintain the association between the resolved \"where\" (from the   Where Resolver) and the resolved what.</li> <li>Maintain the copies, with customization, of the workload objects   from source workspace to mailbox workspaces.</li> <li>Maintain the SyncerConfig object in each mailbox workspace to direct   the corresponding syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#syncers","title":"Syncers","text":"<p>In this PoC there is a 1:1:1 relation between edge cluster, mailbox workspace, and syncer.  The syncer runs in the edge cluster and does downsync from and upsync to the mailbox workspace.  The syncer monitors a SyncerConfig object in the mailbox workspace to know what to downsync and upsync.</p> <p>For those familiar with kcp's TMC syncer, note that the edge syncer differs in the following ways.</p> <ul> <li>Create self-sufficient edge clusters.</li> <li>Re-nature objects that KubeStellar forcibly denatures at the center.</li> <li>Return reported state from associated objects.</li> <li>Does not access the SyncTarget object.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-summarizer","title":"Status Summarizer","text":"<p>For each EdgePlacement object and related objects this controller maintains the directed status summary objects.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#usage-scenario","title":"Usage Scenario","text":"<p>The usage scenario breaks, at the highest level, into two parts: inventory and workload.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-usage","title":"Inventory Usage","text":"<p>A user with infrastructure authority creates one or more inventory management workspaces.  Each such workspace needs to have the following items, which that user will create if they are not pre-populated by the workspace type.</p> <ul> <li>An APIBinding to the <code>workload.kcp.io</code> APIExport to get   <code>SyncTarget</code>.</li> <li>An APIBinding to the <code>scheduling.kcp.io</code> APIExport to get   <code>Location</code>.</li> <li>A ServiceAccount (with associated token-bearing Secret) (details   TBD) that the mailbox controller authenticates as.</li> <li>A ClusterRole and ClusterRoleBinding that authorize said   ServiceAccount to do what the mailbox controller needs to do.</li> </ul> <p>This user also creates one or more edge clusters.</p> <p>For each of those edge clusters, this user creates the following.</p> <ul> <li>a corresponding SyncTarget, in one of those inventory management   workspaces;</li> <li>a Location, in the same workspace, that matches only that   SyncTarget.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-usage","title":"Workload usage","text":"<p>A user with workload authority starts by creating one or more workload management workspaces.  Each needs to have the following, which that user creates if the workload type did not already provide.</p> <ul> <li>An APIBinding to the APIExport of <code>edge.kubestellar.io</code> from the edge   service provider workspace.</li> <li>For each of the Where Resolver, the Placement Translator, and the   Status Summarizer:</li> <li>A ServiceAccount for that controller to authenticate as;</li> <li>A ClusterRole granting the privileges needed by that controller;</li> <li>A ClusterRoleBinding that binds those two.</li> </ul> <p>This user also uses the edge-workspace-as-container view of each such workspace to describe the workload desired state.</p> <p>This user creates one or more EdgePlacement objects to say which workload goes where.  These may be accompanied by API objects that specify rule-based customization, specify how status is to be summarized.</p> <p>The KubeStellar implementation propagates the desired state from center to edge and collects the specified information from edge to center.</p> <p>The edge user monitors status summary objects in their workload management workspaces.</p> <p>The status summaries may include limited-length lists of broken objects.</p> <p>Full status from the edge is available in the mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/","title":"KubeStellar Placement Translator","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre></p> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/placement-translator.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre> <p>The placement translator runs in the center and translates EMC placement problems into edge sync problems.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#how-to-install-pre-requisites-for-a-windows-subsystem-for-linux-wsl-envronment-using-an-ubuntu-220401-distribution","title":"How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution","text":"<p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#1-if-youre-using-a-vpn-turn-it-off","title":"1. If you're using a VPN, turn it off","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#2-install-ubuntu-into-wsl","title":"2. Install Ubuntu into WSL","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#20-if-wsl-is-not-yet-installed-open-a-powershell-administrator-window-and-run-the-following","title":"2.0 If wsl is not yet installed, open a powershell administrator window and run the following","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#21-reboot-your-system","title":"2.1 reboot your system","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#22-in-a-windows-command-terminal-run-the-following-to-list-all-the-linux-distributions-that-are-available-online","title":"2.2 In a Windows command terminal run the following to list all the linux distributions that are available online","text":"<pre><code>wsl -l -o\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#23-select-a-linux-distribution-and-install-it-into-wsl","title":"2.3 Select a linux distribution and install it into WSL","text":"<p><pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#24-enter-your-new-username-and-password-at-the-prompts-and-you-will-eventually-see-something-like","title":"2.4 Enter your new username and password at the prompts, and you will eventually see something like:","text":"<pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#25-click-on-the-windows-start-icon-and-type-in-the-name-of-your-distribution-into-the-search-box","title":"2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box.","text":"<p>Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#3-install-pre-requisites-into-your-new-vm","title":"3. Install pre-requisites into your new VM","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#31-update-and-apply-apt-get-packages","title":"3.1 update and apply apt-get packages","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#32-install-golang","title":"3.2 Install golang","text":"<pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#33-install-ko-but-dont-do-ko-set-action-step","title":"3.3 Install ko (but don't do ko set action step)","text":"<pre><code>go install github.com/google/ko@latest\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#34-install-gcc","title":"3.4 Install gcc","text":"<p>Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#35-install-make-if-you-installed-build-essential-this-may-already-be-installed","title":"3.5 Install make (if you installed build-essential this may already be installed)","text":"<pre><code>apt install make\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#36-install-jq","title":"3.6 Install jq","text":"<pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#37-install-kubectl","title":"3.7 install kubectl","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#38-install-helm-required-when-deploying-as-workload","title":"3.8 install helm (required when deploying as workload)","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#how-to-install-docker-and-kind-into-a-windows-subsystem-for-linux-wsl-environment-using-an-ubuntu-220401-distribution","title":"How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#10-start-a-vm-terminal-by-clicking-on-the-app-you-configured-using-the-instructions-in-the-general-pre-requisites-described-above","title":"1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#20-install-docker","title":"2.0 Install docker","text":"<p>The installation instructions from docker are not sufficient to get docker working with WSL</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#21-follow-instructions-here-to-install-docker-httpsdocsdockercomengineinstallubuntu","title":"2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/","text":"<p>Here some additonal steps you will need to take:</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#22-ensure-that-etcwslconf-is-configured-so-that-systemd-will-run-on-booting","title":"2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting.","text":"<p>If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#23-edit-etcsudoers-it-is-strongly-recommended-to-not-add-directives-directly-to-etcsudoers-but-instead-to-put-them-in-files-in-etcsudoersd-which-are-auto-included-so-makemodify-a-new-file-via","title":"2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via","text":"<p><pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#24-add-your-user-to-the-docker-group","title":"2.4 Add your user to the docker group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#25-if-dockerd-is-already-running-then-stop-it-and-restart-it-as-follows-note-the-new-dockerd-instance-will-be-running-in-the-foreground","title":"2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground):","text":"<pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#251-if-you-encounter-an-iptables-issue-which-is-described-here-httpsgithubcommicrosoftwslissues6655","title":"2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655","text":"<p>The following commands will fix the issue:  <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#3-you-will-now-need-to-open-new-terminals-to-access-the-vm-since-dockerd-is-running-in-the-foreground-of-this-terminal","title":"3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#31-in-your-new-terminal-install-kind","title":"3.1 In your new terminal, install kind","text":"<pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#status","title":"Status","text":"<p>The placement translator is a work in progress.  It maintains <code>SyncerConfig</code> objects and downsynced objects in mailbox workspaces, albeit with limitations discussed in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#additional-design-details","title":"Additional Design Details","text":"<p>The placement translator maintains one <code>SyncerConfig</code> object in each mailbox workspace.  That object is named <code>the-one</code>.  Other <code>SyncerConfig</code> objects may exist; the placement translator ignores them.</p> <p>The placement translator responds to each resource discovery independently.  This makes the behavior jaggy and the logging noisy. For example, it means that the <code>SyncerConfig</code> objects may be rewritten for each resource discovery.  But eventually the right things happen.</p> <p>The placement translator does not yet attempt the full prescribed technique for picking the API version to use when reading and writing. Currently it looks only at the preferred version reported in each workload management workspace, and only succeeds if they all agree.</p> <p>One detail left vague in the design outline is what constitutes the \"desired state\" that propagates from center to edge.  The easy obvious answer is the \"spec\" section of downsynced objects, but that answer ignores some issues.  Following is the current full answer.</p> <p>When creating a workload object in a mailbox workspace, the placement translator uses a copy of the object read from the workload management workspace but with the following changes.</p> <ul> <li>The <code>metadata.managedFields</code> is emptied.</li> <li>The <code>metadata.resourceVersion</code> is emptied.</li> <li>The <code>metadata.selfLlink</code> is emptied.</li> <li>The <code>metadata.uid</code> is emptied.</li> <li>The <code>metadata.ownerReferences</code> is emptied.  (Doing better would   require tracking UID mappings from WMW to MBWS.)</li> <li>In <code>metadata.labels</code>, <code>edge.kubestellar.io/projected=yes</code> is added.</li> </ul> <p>The placement translator does not react to changes to the workload objects in the mailbox workspace.</p> <p>When downsyncing desired state and the placement translator finds the object already exists in the mailbox workspace, the placement translator does an HTTP PUT (<code>Update</code> in the <code>k8s.io/client-go/dynamic</code> package) using an object value --- called below the \"destination\" object --- constructed by reading the object from the MBWS and making the following changes.</p> <ul> <li>For top-level sections in the source object other than <code>apiVersion</code>,   <code>kind</code>, <code>metadata</code>, and <code>status</code>, the destination object gets the   same contents for that section.</li> <li>If the source object has some annotations then they are merged into   the destination object annotations as follows.</li> <li>A destination annotation that has no corresponding annotation in     the source is unchanged.</li> <li>A destination annotation that has the same value as the     corresponding annotation in the source is unchanged.</li> <li>A \"system\" annotation is unchanged.  The system annotations are     those whose key (a) starts with <code>kcp.io/</code> or other stuff followed     by <code>.kcp.io/</code> and (b) does not start with <code>edge.kubestellar.io/</code>.</li> <li>The source object's labels are merged into the destination object   using the same rules as for annotations, and <code>edge.kubestellar.io/projected</code>   is set to <code>yes</code>.</li> <li>The remainder of the <code>metadata</code> is unchanged.</li> </ul> <p>For objects --- other than <code>Namespace</code> objects --- that exist in a mailbox workspace and whose API GroupResource has been relevant to the placement translator since it started, ones that have the <code>edge.kubestellar.io/projected=yes</code> label but are not currently desired are deleted.  The exclusion for <code>Namespace</code> objects is there because the placement translator does not take full ownership of them, rather it takes the position that there might be other parties that create <code>Namespace</code> objects or rely on their existence.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#usage","title":"Usage","text":"<p>The placement translator needs two kube client configurations.  One points to the edge service provider workspace and provides authority to (a) read the APIExport view of the edge API and (b) write into the mailbox workspaces.  The other points to the kcp server base (i.e., does not identify a particular logical cluster nor <code>*</code>) and is authorized to read all clusters.  In the kubeconfig created by <code>kcp start</code> that is satisfied by the context named <code>system:admin</code>.</p> <p>The command line flags, beyond the basics, are as follows.  For a string parameter, if no default is explicitly stated then the default is the empty string, which usually means \"not specified here\".  For both kube client configurations, the usual rules apply: first consider command line parameters, then <code>$KUBECONFIG</code>, then <code>~/.kube/config</code>.</p> <pre><code>      --allclusters-cluster string       The name of the kubeconfig cluster to use for access to all clusters\n      --allclusters-context string       The name of the kubeconfig context to use for access to all clusters (default \"system:admin\")\n--allclusters-kubeconfig string    Path to the kubeconfig file to use for access to all clusters\n      --allclusters-user string          The name of the kubeconfig user to use for access to all clusters\n\n--espw-cluster string              The name of the kubeconfig cluster to use for access to the edge service provider workspace\n      --espw-context string              The name of the kubeconfig context to use for access to the edge service provider workspace\n      --espw-kubeconfig string           Path to the kubeconfig file to use for access to the edge service provider workspace\n      --espw-user string                 The name of the kubeconfig user to use for access to the edge service provider workspace\n\n--root-cluster string              The name of the kubeconfig cluster to use for access to root workspace\n      --root-context string              The name of the kubeconfig context to use for access to root workspace (default \"root\")\n--root-kubeconfig string           Path to the kubeconfig file to use for access to root workspace\n      --root-user string                 The name of the kubeconfig user to use for access to root workspace\n\n--server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10204)\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#try-it","title":"Try It","text":"<p>The nascent placement translator can be exercised following the scenario in example1.  You will need to run the where resolver and mailbox controller long enough for them to create what this scenario calls for, but they can be terminated after that.</p> <p></p> <p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#deploy-kcp-and-kubestellar","title":"Deploy kcp and KubeStellar","text":"<p>You need kcp and KubeStellar and can deploy them in either of two ways: as bare processes on whatever host you are using to run this example, or as workload in a Kubernetes cluster (an OpenShift cluster qualifies).  Do one or the other, not both.</p> <p>KubeStellar only works with release <code>v0.11.0</code> of kcp.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.  See the start of the kcp quickstart for instructions on that, but get release v0.11.0 rather than the latest (the downloadable assets appear after the long list of changes/features/etc).</p> <p>Clone the v0.11.0 branch of the kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> and build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Running the kcp server creates a hidden subdirectory named <code>.kcp</code> to hold all sorts of state related to the server. If you have run it before and want to start over from scrath then you should <code>rm -rf .kcp</code> first.</p> <p>Use the following commands to: (a) run the kcp server in a forked command, (b) update your <code>KUBECONFIG</code> environment variable to configure <code>kubectl</code> to use the kubeconfig produced by the kcp server, and (c) wait for the kcp server to get through some initialization. The choice of <code>-v=3</code> for the kcp server makes it log a line for every HTTP request (among other things).</p> <pre><code>kcp start -v=3 &amp;&gt; /tmp/kcp.log &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\n# wait until KCP is ready checking availability of ws resource\nwhile ! kubectl ws tree &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that the kcp server creates. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the Edge Service Provider Workspace (ESPW), which exports the KubeStellar API, and also augments the <code>root:compute</code> workspace from kcp TMC as needed here. That augmentation consists of adding authorization to update the relevant <code>/status</code> and <code>/scale</code> subresources (missing in kcp TMC) and extending the supported subset of the Kubernetes API for managing containerized workloads from the four resources built into kcp TMC (<code>Deployment</code>, <code>Pod</code>, <code>Service</code>, and <code>Ingress</code>) to the other ones that are meaningful in KubeStellar.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>kubectl ws root:imw1\nkubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"decribe the florin location object\"\nkubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects plus the kcp <code>APIBinding</code> objects that import the definition of the KubeStellar API.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p> <p>Continue to follow the steps until the start of Stage 3 of the exercise.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 10\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox workspaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox workspaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>kubectl ws root\nwhile [ $(kubectl ws tree | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into your shell if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI0721 17:37:10.186848  189094 main.go:206] \"Found APIExport view\" exportName=\"e\ndge.kubestellar.io\" serverURL=\"https://10.0.2.15:6443/services/apiexport/cseslli1ddit3s\na5/edge.kubestellar.io\"\n...\nI0721 19:17:21.906984  189094 controller.go:300] \"Created APIBinding\" worker=1\nmbwsName=\"1d55jhazpo3d3va6-mb-551bebfd-b75e-47b1-b2e0-ff0a4cb7e006\" mbwsCluster\n=\"32x6b03ixc49cj48\" bindingName=\"bind-edge\" resourceVersion=\"1247\"\n...\nI0721 19:18:56.203057  189094 controller.go:300] \"Created APIBinding\" worker=0\nmbwsName=\"1d55jhazpo3d3va6-mb-732cf72a-1ca9-4def-a5e7-78fd0e36e61c\" mbwsCluster\n=\"q31lsrpgur3eg9qk\" bindingName=\"bind-edge\" resourceVersion=\"1329\"\n^C\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>A mailbox workspace name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl ws root\nkubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE          REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal              Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal              Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\ncompute                                                    universal              Ready   https://172.20.144.39:6443/clusters/root:compute        6m8s\nespw                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:espw           2m4s\nimw1                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:imw1           1m9s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\ncompute                                                    &lt;none&gt;       mqnl7r5f56hswewy\nespw                                                       &lt;none&gt;       2n88ugkhysjbxqp5\nimw1                                                       &lt;none&gt;       4d2r9stcyy2qq5c1\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox workspace name is $GUILDER_WS\n</code></pre> <pre><code>The guilder mailbox workspace name is 1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox workspace name is $FLORIN_WS\n</code></pre> <pre><code>The florin mailbox workspace name is 1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kubestellar.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\", \"bind-apiregistration.k8s.io\" ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    namespaceSelectors: []\n    objectNames: [ \"v1090.example.my\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver; it requires the ESPW to be the current kcp workspace at start time.</p> <pre><code>kubectl ws root:espw\nkubestellar-where-resolver &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>kubectl ws root:wmw-c\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nkubectl ws root:wmw-s\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I0423 01:33:37.036752   11305 main.go:212] \"Found APIExport view\" exportName=\"edge.kubestellar.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kubestellar.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl ws root:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p> <p>Finally run the placement translator from the command line.  That should look like the following (possibly including some complaints, which do not necessarily indicate real problems).</p> <pre><code>kubectl ws root:espw\nplacement-translator &amp;\nsleep 120\n</code></pre> <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\nI0412 15:15:57.970014   94634 shared_informer.go:289] Caches are synced for what-resolver\nI0412 15:15:57.970178   94634 shared_informer.go:282] Waiting for caches to sync for where-resolver\nI0412 15:15:57.970192   94634 shared_informer.go:289] Caches are synced for where-resolver\n...\nI0412 15:15:57.972185   94634 map-types.go:338] \"Put\" map=\"where\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val=\"[&amp;{SinglePlacementSlice edge.kubestellar.io/v2alpha1} {edge-placement-s    e1b1033d-49f2-45e8-8a90-6d0295b644b6 1184 1 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:r0bdh9oumjkoag3s] [{edge.kubestellar.io/v2alpha1 EdgePlacement edge-placement-s 0e718a31-db21-47f1-b789-cd55835b1418 &lt;nil&gt; &lt;nil&gt;}] []  [{where-resolver Update edge.kubestellar.io/v2alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"0e718a31-db21-47f1-b789-cd55835b1418\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\nI0412 15:15:57.973740   94634 map-types.go:338] \"Put\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val=\"[&amp;{SinglePlacementSlice edge.kubestellar.io/v2alpha1} {edge-placement-c    c446ca9b-8937-4751-89ab-058bcfb079c1 1183 3 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:1i1weo8uoea04wxr] [{edge.kubestellar.io/v2alpha1 EdgePlacement edge-placement-c c1e038b9-8bd8-4d22-8ab8-916e40c794d1 &lt;nil&gt; &lt;nil&gt;}] []  [{where-resolver Update edge.kubestellar.io/v2alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"c1e038b9-8bd8-4d22-8ab8-916e40c794d1\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-f sync-target-f e6efb8bd-6755-45ac-b44d-5d38f978f990} {1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\n...\nI0412 15:15:58.173974   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[{APIGroup: Resource:namespaces Name:commonstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:group1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group2.test Resources:[cogs] Namespaces:[] Names:[William]}]}\nI0412 15:15:58.180380   94634 map-types.go:338] \"Put\" map=\"what\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val={Downsync:map[{APIGroup: Resource:namespaces Name:specialstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:group1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group3.test Resources:[widgets] Namespaces:[] Names:[*]}]}\n...\n</code></pre> <p>The \"Put\" log entries with <code>map=\"what\"</code> show what the \"what resolver\" is reporting.  This reports mappings from <code>ExternalName</code> of an <code>EdgePlacement</code> object to the workload parts that that <code>EdgePlacement</code> says to downsync and upsync.</p> <p>The \"Put\" log entries with <code>map=\"where\"</code> show the <code>SinglePlacementSlice</code> objects associated with each <code>EdgePlacement</code>.</p> <p>Next, using a separate shell, examine the SyncerConfig objects in the mailbox workspaces.  Make sure to use the same kubeconfig as you use to run the placement translator, or any other that is pointed at the edge service provider workspace. The following with switch the focus to mailbox workspace(s).</p> <p>You can get a listing of mailbox workspaces, as follows.</p> <p><pre><code>kubectl ws root\nkubectl get workspace\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19   universal            Ready   https://192.168.58.123:6443/clusters/12zzf3frkqz2yj39   36m\n1xpg93182scl85te-mb-e6efb8bd-6755-45ac-b44d-5d38f978f990   universal            Ready   https://192.168.58.123:6443/clusters/2v6wl3x41zxmpmhr   36m\n</code></pre></p> <p>Next switch to one of the mailbox workspaces (in my case I picked the one for the guilder cluster) and examine the <code>SyncerConfig</code> object. That should look like the following.</p> <p><pre><code>kubectl ws $(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>Current workspace is \"root:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml                           </code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\ngeneration: 2\nname: the-one\n  resourceVersion: \"1249\"\nuid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: events\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n  upsync:\n  - apiGroup: group2.test\n    names:\n    - William\n    resources:\n    - cogs\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre></p> <p>At this point you might veer off from the example scenario and try tweaking things.  For example, try deleting an EdgePlacement as follows.</p> <p><pre><code>kubectl ws root:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:work-c\"\n</code></pre> <pre><code>kubectl delete EdgePlacement edge-placement-c\n</code></pre> <pre><code>edgeplacement.edge.kubestellar.io \"edge-placement-c\" deleted\n</code></pre></p> <p>That will cause the placement translator to log updates, as follows.</p> <pre><code>I0412 15:20:43.129842   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[] Upsync:[]}\nI0412 15:20:43.241674   94634 map-types.go:342] \"Delete\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\"\n</code></pre> <p>After that, the SyncerConfig in the florin mailbox should be empty, as in the following (you mailbox workspace names may be different).</p> <p><pre><code>kubectl ws root\n</code></pre> <pre><code>Current workspace is \"root\".\n</code></pre></p> <p><pre><code>kubectl ws $(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\n</code></pre> <pre><code>Current workspace is \"root:2lplrryirmv4xug3-mb-89c08764-01ae-4117-8fb0-6b752e76bc2f\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 2cow9p3xogak4n0u\n  creationTimestamp: \"2023-04-11T04:34:22Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"2130\"\nuid: 2b66b4bc-4130-4bf0-8524-73d6885f2ad8\nspec:\n  namespaceScope: {}\nstatus: {}\n</code></pre></p> <p>And the SyncerConfig in the guilder mailbox workspace should reflect only the special workload.  That would look something like the following.</p> <p><pre><code>kubectl ws root\nkubectl ws $(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>Current workspace is \"root:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml                           </code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1254\"\nuid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: events\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl kubestellar remove wmw example-wmw\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/","title":"Use Cases Affecting the Roadmap for PoC2023q1","text":""},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#background","title":"Background","text":"<p>The outline mentions features that need not be implement at first.  In the following sections we consider some particular use cases.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mvi","title":"MVI","text":"<p>MVI needs customization.  We can demo an MVI scenario without: self-sufficient edge clusters, summarization, upsync (Return and/or summarization of reported state from associated objects), sophisticated handling of workload conflicts.</p> <p>What about denaturing?</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#compliance-to-policy","title":"Compliance-to-Policy","text":"<p>I am not sure what is workable here.  Following are some possibilities.  They vary in two dimensions.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#how-c2p-controller-consumes-reports","title":"How C2P controller consumes reports","text":""},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-workload-apiexport-and-workload-apibindings","title":"Through view of workload APIExport and workload APIBindings","text":"<p>As outlined in PR 241: - the C2P team maintains CRDs, APIResourceSchemas, and an APIExport for   the policy and report resources; - the C2P team puts those APIResourceSchemas and that APIExport in a   kcp workspace of their choice; - the workload management workspace has an APIBinding to that APIExport; - the EdgePlacement selects that APIBinding for downsync; - the APIBinding goes to the mailbox workspace but not the edge cluster; - those CRDs are pre-installed on the edge clusters; - the APIExport's view shows the report objects in the mailbox   workspaces (as well as anywhere else they exist).</p> <p>This is not a great choice because of \"those CRDs are pre-installed on the edge clusters\".</p> <p>It is also not a great choice because it requires the C2P team to maintain two copies of the Kyverno resource definitions.</p> <p>This is a bad choice because it is not consistent with the preferred way to demonstrate installation of Kyverno, which is to have Helm install Kyverno into the workload management workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-a-new-kind-of-view","title":"Through a new kind of view","text":"<p>We could define a new kind of view that does what we want.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-emc-apiexport-and-mailbox-apibindings","title":"Through view of EMC APIExport and mailbox APIBindings","text":"<p>This approach also uses APIExport and APIBinding objects but in a different way than above.  In this approach the placement translator maintains one APIExport in the edge service provider workspace and a corresponding APIBinding object in each mailbox workspace, and they work together as follows.</p> <p>The APIExport has an empty LatestResourceSchemas but a large dynamic PermissionClaims slice.  In particular, there is a PermissionClaim for every resource involved in downsync or upsync in any EdgePlacement object.  Some day we might try something more granular, but today is not that day.</p> <p>In each mailbox workspace, the corresponding APIBinding's list of accepted PermissionClaims has an entry for every resource downsynced or upsynced to that workspace.</p> <p>As a consequence, the APIExport's view holds all the objects whose kind/resource is defined by those APIBindings.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-an-api-for-consuming-from-mailboxes","title":"Through an API for consuming from mailboxes","text":"<p>The C2P Controller uses the API proposed in PR 240 to read the report objects from the mailbox workspaces.  This has the downside of exposing the mailbox workspaces as part of the KubeStellar interface --- which they were NOT originally intended to be.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#c2p-controller-consumes-report-summaries-prepared-by-kubestellar","title":"C2P Controller consumes report summaries prepared by KubeStellar","text":"<p>In this scenario: - we have defined and implemented summarization in KubeStellar; - that summarization is adequate for the needs of the C2P Controller; - that controller consumes summaries rather than the reports themselves.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mailbox-vs-edge","title":"Mailbox vs. Edge","text":"<p>The latest plan is to use full EMC.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#using-the-current-tmc-syncer","title":"Using the current TMC syncer","text":"<p>In this scenario the edge clusters are not self-sufficient; the workload containers in an edge cluster use kube api services from the corresponding mailbox workspace.  The key insight here is that from an outside perspective, a pair of (edge cluster, corresponding mailbox workspace) operates as a unit and the rest of the world does not care about internal details of that unit.  But that is only true if you do not require too much from the networking.  In this scenario, a workload container runs in the edge cluster and a workload Service object is about proxying/load-balancing in the edge cluster.  An admission control webhook normally directs the apiserver to call out to a virtual IP address associated with a Service; that is a problem in this scenario because the apiserver in question is the one holding the mailbox workspace but the Service that gets connections to the workload containers is in the edge cluster.  This scenario will work if the C2P workload does not include admission control webhooks.  Note that Kubernetes release 1.26 introduces CEL-based validating admission control policies, so using them would not involve webhooks.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#expand-tmc-to-support-webhooks","title":"Expand TMC to support webhooks","text":"<p>The problem with webhooks would go away if TMC were expanded to support them, perhaps through some sort of tunneling so that a client in the center can open connections to a Service at the edge.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#pre-deploy-controllers-and-resources-on-edge-clusters","title":"Pre-deploy controllers and resources on edge clusters","text":"<p>In this scenario, the PVP/PEP is pre-deployed on the edge clusters, and the policy and report resources (which are cluster-scoped) are predefined there too.  This scenario would continue to use the TMC syncer, but only need it to downsync the policies and upsync the reports.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#use-full-emc","title":"Use full EMC","text":"<p>No shortcuts here, no limitations.</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/","title":"KubeStellar Where Resolver","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre></p> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/where-resolver.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#how-to-install-pre-requisites-for-a-windows-subsystem-for-linux-wsl-envronment-using-an-ubuntu-220401-distribution","title":"How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution","text":"<p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#1-if-youre-using-a-vpn-turn-it-off","title":"1. If you're using a VPN, turn it off","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#2-install-ubuntu-into-wsl","title":"2. Install Ubuntu into WSL","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#20-if-wsl-is-not-yet-installed-open-a-powershell-administrator-window-and-run-the-following","title":"2.0 If wsl is not yet installed, open a powershell administrator window and run the following","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#21-reboot-your-system","title":"2.1 reboot your system","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#22-in-a-windows-command-terminal-run-the-following-to-list-all-the-linux-distributions-that-are-available-online","title":"2.2 In a Windows command terminal run the following to list all the linux distributions that are available online","text":"<pre><code>wsl -l -o\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#23-select-a-linux-distribution-and-install-it-into-wsl","title":"2.3 Select a linux distribution and install it into WSL","text":"<p><pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#24-enter-your-new-username-and-password-at-the-prompts-and-you-will-eventually-see-something-like","title":"2.4 Enter your new username and password at the prompts, and you will eventually see something like:","text":"<pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#25-click-on-the-windows-start-icon-and-type-in-the-name-of-your-distribution-into-the-search-box","title":"2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box.","text":"<p>Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#3-install-pre-requisites-into-your-new-vm","title":"3. Install pre-requisites into your new VM","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#31-update-and-apply-apt-get-packages","title":"3.1 update and apply apt-get packages","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#32-install-golang","title":"3.2 Install golang","text":"<pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#33-install-ko-but-dont-do-ko-set-action-step","title":"3.3 Install ko (but don't do ko set action step)","text":"<pre><code>go install github.com/google/ko@latest\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#34-install-gcc","title":"3.4 Install gcc","text":"<p>Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#35-install-make-if-you-installed-build-essential-this-may-already-be-installed","title":"3.5 Install make (if you installed build-essential this may already be installed)","text":"<pre><code>apt install make\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#36-install-jq","title":"3.6 Install jq","text":"<pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#37-install-kubectl","title":"3.7 install kubectl","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#38-install-helm-required-when-deploying-as-workload","title":"3.8 install helm (required when deploying as workload)","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#how-to-install-docker-and-kind-into-a-windows-subsystem-for-linux-wsl-environment-using-an-ubuntu-220401-distribution","title":"How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#10-start-a-vm-terminal-by-clicking-on-the-app-you-configured-using-the-instructions-in-the-general-pre-requisites-described-above","title":"1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#20-install-docker","title":"2.0 Install docker","text":"<p>The installation instructions from docker are not sufficient to get docker working with WSL</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#21-follow-instructions-here-to-install-docker-httpsdocsdockercomengineinstallubuntu","title":"2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/","text":"<p>Here some additonal steps you will need to take:</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#22-ensure-that-etcwslconf-is-configured-so-that-systemd-will-run-on-booting","title":"2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting.","text":"<p>If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#23-edit-etcsudoers-it-is-strongly-recommended-to-not-add-directives-directly-to-etcsudoers-but-instead-to-put-them-in-files-in-etcsudoersd-which-are-auto-included-so-makemodify-a-new-file-via","title":"2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via","text":"<p><pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#24-add-your-user-to-the-docker-group","title":"2.4 Add your user to the docker group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#25-if-dockerd-is-already-running-then-stop-it-and-restart-it-as-follows-note-the-new-dockerd-instance-will-be-running-in-the-foreground","title":"2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground):","text":"<pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#251-if-you-encounter-an-iptables-issue-which-is-described-here-httpsgithubcommicrosoftwslissues6655","title":"2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655","text":"<p>The following commands will fix the issue:  <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#3-you-will-now-need-to-open-new-terminals-to-access-the-vm-since-dockerd-is-running-in-the-foreground-of-this-terminal","title":"3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#31-in-your-new-terminal-install-kind","title":"3.1 In your new terminal, install kind","text":"<pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#usage-of-the-where-resolver","title":"Usage of the Where Resolver","text":"<p>The Where Resolver needs two Kubernetes client configurations.</p> <p>The first is needed to access the APIExport view of the <code>edge.kubestellar.io</code> API group. It must point to the edge service provider workspace that has this APIExport and is authorized to read its view for edge APIs.</p> <p>The second is needed to maintain <code>SinglePlacementSlice</code> objects in all workload management workspaces; this should be a client config that is able to read/write in all clusters. For example, there is a kubeconfig context named <code>base</code> in the kubeconfig created by <code>kcp start</code> which satisfies these requirements.</p> <p>The command line flags, beyond the basics, are as follows.</p> <pre><code>      --espw-cluster string                  The name of the kubeconfig cluster to use for access to the edge service provider workspace\n      --espw-context string                  The name of the kubeconfig context to use for access to the edge service provider workspace\n      --espw-kubeconfig string               Path to the kubeconfig file to use for access to the edge service provider workspace\n      --espw-user string                     The name of the kubeconfig user to use for access to the edge service provider workspace\n\n--base-cluster string                  The name of the kubeconfig cluster to use for access to all logical clusters as kcp-admin (default \"base\")\n--base-context string                  The name of the kubeconfig context to use for access to all logical clusters as kcp-admin\n      --base-kubeconfig string               Path to the kubeconfig file to use for access to all logical clusters as kcp-admin\n      --base-user string                     The name of the kubeconfig user to use for access to all logical clusters as kcp-admin (default \"kcp-admin\")\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#steps-to-try-the-where-resolver","title":"Steps to try the Where Resolver","text":""},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#pull-the-kcp-source-code-build-kcp-and-start-kcp","title":"Pull the kcp source code, build kcp, and start kcp","text":"<p>At this point you should have cloned the KubeStellar repo and <code>cd</code>ed into it as directed above.</p> <p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window). Set your <code>KUBECONFIG</code> environment variable to name the kubernetes client config file that <code>kcp</code> generates. <pre><code>kcp start &amp;&gt; /dev/null &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#build-and-initialize-kubestellar","title":"Build and initialize KubeStellar","text":"<p>First build KubeStellar and add the result to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>Next, use the command that makes sure the Edge Service Provider Workspace (ESPW), which is <code>root:espw</code>, and the TMC provider workspace (<code>root:compute</code>) are properly set up.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#create-the-workload-management-workspace-wmw-and-bind-it-to-the-espw-apis","title":"Create the Workload Management Workspace (WMW) and bind it to the ESPW APIs","text":"<p>Use the user home workspace (\\~) as the workload management workspace (WMW). <pre><code>kubectl ws \\~\n</code></pre></p> <p>Bind APIs. <pre><code>kubectl apply -f config/imports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#run-the-kubestellar-where-resolver-against-the-espw","title":"Run the KubeStellar Where Resolver against the ESPW","text":"<p>Go to the <code>root:espw</code> workspace and run the Where Resolver.</p> <pre><code>kubectl ws root:espw\nkubestellar-where-resolver -v 2 &amp;\nsleep 45\n</code></pre> <p>The outputs from the Where Resolver should be similar to: <pre><code>I0605 10:53:00.156100   29786 main.go:212] \"Found APIExport view\" exportName=\"edge.kubestellar.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/jxch2kyb3c1h6bac/edge.kubestellar.io\"\nI0605 10:53:00.157874   29786 main.go:212] \"Found APIExport view\" exportName=\"scheduling.kcp.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/root/scheduling.kcp.io\"\nI0605 10:53:00.159242   29786 main.go:212] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/root/workload.kcp.io\"\nI0605 10:53:00.261128   29786 controller.go:201] \"starting controller\" controller=\"where-resolver\"\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#create-the-inventory-management-workspace-imw-and-populate-it-with-locations-and-synctargets","title":"Create the Inventory Management Workspace (IMW) and populate it with locations and synctargets","text":"<p>Use workspace <code>root:compute</code> as the Inventory Management Workspace (IMW). <pre><code>kubectl ws root:compute\nkubectl kcp bind apiexport root:espw:edge.kubestellar.io\n</code></pre></p> <p>Create two Locations and two SyncTargets. <pre><code>kubectl create -f config/samples/location_prod.yaml\nkubectl create -f config/samples/location_dev.yaml\nkubectl create -f config/samples/synctarget_prod.yaml\nkubectl create -f config/samples/synctarget_dev.yaml\nsleep 5\n</code></pre></p> <p>Note that kcp automatically creates a Location <code>default</code>. So there are 3 Locations and 2 SyncTargets in <code>root:compute</code>. <pre><code>kubectl get locations,synctargets\n</code></pre> <pre><code>NAME                                 RESOURCE      AVAILABLE   INSTANCES   LABELS   AGE\nlocation.edge.kubestellar.io/default   synctargets   0           2                    2m12s\nlocation.edge.kubestellar.io/dev       synctargets   0           1                    2m39s\nlocation.edge.kubestellar.io/prod      synctargets   0           1                    3m13s\n\nNAME                              AGE\nsynctarget.edge.kubestellar.io/dev    110s\nsynctarget.edge.kubestellar.io/prod   2m12s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#create-some-edgeplacements-in-the-wmw","title":"Create some EdgePlacements in the WMW","text":"<p>Go to Workload Management Workspace (WMW) and create an EdgePlacement <code>all2all</code>. <pre><code>kubectl ws \\~\nkubectl create -f config/samples/edgeplacement_all2all.yaml\nsleep 3\n</code></pre></p> <p>The Where Resolver maintains a SinglePlacementSlice for an EdgePlacement in the same workspace. <pre><code>kubectl get sps all2all -oyaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\ndestinations:\n- cluster: 1yotsgod0d2p3xa5\n  locationName: prod\n  syncTargetName: prod\n  syncTargetUID: 13841ffd-33f2-4cf4-9114-6156f73aa5c8\n- cluster: 1yotsgod0d2p3xa5\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\n- cluster: 1yotsgod0d2p3xa5\n  locationName: default\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\n- cluster: 1yotsgod0d2p3xa5\n  locationName: default\n  syncTargetName: prod\n  syncTargetUID: 13841ffd-33f2-4cf4-9114-6156f73aa5c8\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-06-05T14:55:20Z\"\ngeneration: 1\nname: all2all\n  ownerReferences:\n  - apiVersion: edge.kubestellar.io/v2alpha1\n    kind: EdgePlacement\n    name: all2all\n    uid: 31915018-6a25-4f01-943e-b8a0a0ed35ba\n  resourceVersion: \"875\"\nuid: a2b8224d-5feb-40a1-adb2-67c07965f13b\n</code></pre> EdgePlacement <code>all2all</code> selects all the 3 Locations in <code>root:compute</code>.</p> <p>Create a more specific EdgePlacement which selects Locations labeled by <code>env: dev</code>. <pre><code>kubectl create -f config/samples/edgeplacement_dev.yaml\nsleep 3\n</code></pre></p> <p>The corresponding SinglePlacementSlice has a shorter list of <code>destinations</code>: <pre><code>kubectl get sps dev -oyaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\ndestinations:\n- cluster: 1yotsgod0d2p3xa5\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-06-05T14:57:00Z\"\ngeneration: 1\nname: dev\n  ownerReferences:\n  - apiVersion: edge.kubestellar.io/v2alpha1\n    kind: EdgePlacement\n    name: dev\n    uid: 1ac4b7f5-5521-4b5a-a0fa-cc2ec87b458b\n  resourceVersion: \"877\"\nuid: c9c0c2fc-d721-4c73-9788-e10711bad23a\n</code></pre></p> <p>Feel free to change the Locations, SyncTargets, and EdgePlacements and see how the Where Resolver reacts.</p> <p>Your next step is to deliver a workload to a mailbox (that represents an edge location).  Go here to take the next step... (TBD)</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl kubestellar remove wmw example-wmw\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/_index/","title":"Environments","text":"<p>There will be 2 environments designed, created, and maintained for KubeStellar: - A cloud environment (cloud-env) which will be used to deploy, scale, and measure metrics associated with small, medium, and large-scale KubeStellar experiments, - A development environment (dev-env) which will be used to deploy a small local KubeStellar installation that is sized for daily use and experimentation on a laptop.</p>"},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/","title":"Cloud-Environment (cloud-env)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#monitoring-tools-for-kubestellar-prometheus-grafana-and-node-exporter","title":"Monitoring Tools for KubeStellar (Prometheus, Grafana and Node Exporter)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#description","title":"Description","text":"<p>This example shows how to deploy monitoring tools (prometheus, grafana and node exporter) for KubeStellar components (core and edge regions) - see architecture image above. Prometheus server is deployed in the core region running the KCP server alongside the components for KubeStellar. A Prometheus agent is deployed in the edge regions running the workload execution clusters.</p> <ol> <li>Create your hosts file with the list of target hosts (KCP server &amp; workload execution cluster)</li> </ol> <pre><code>[kcp-server]\n192.168.56.2\n\n[workload-execution-clusters]\n192.160.56.10\n192.160.56.12\n</code></pre> <ol> <li>Configure the prometheus targets endpoints:</li> </ol> <p>a) Prometheus Server: edit the file roles/prometheus/templates/prometheus-config.yaml.j2</p> <pre><code>global:\n  evaluation_interval: 5s\n  external_labels:\n    env: dev\n  scrape_interval: 30s\nscrape_configs:\n- job_name: mailbox-controller\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:10203'\n\n- job_name: node-exporter\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:9100'\n\n- job_name: kcp\n  scrape_interval: 15s\n  scheme: https\n  metrics_path: /metrics\n  tls_config:\n    insecure_skip_verify: true\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:6443'\n</code></pre> <p>b) Prometheus Agent: edit the file roles/prometheus/templates/prometheus-agent-config.yaml.j2</p> <pre><code># my global config\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s \nscrape_configs:\n  - job_name: \"prometheus-agent\"\n\n    static_configs:\n      - targets: [\"{ ansible_default_ipv4.address }:&lt;port_number&gt;\"]\nremote_write:\n  - url: \"http://&lt;prometheus-server-ip-address&gt;:&lt;port_number&gt;/api/v1/write\"\n</code></pre> <ol> <li>Install prometheus server, grafana and node exporter to the KCP server host using the following playbook:</li> </ol> <pre><code>- hosts: kcp-server\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n    - grafana\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-kcpServer.yaml\n</code></pre> <ol> <li>Install prometheus agent and node exporter to a workload execution cluster using the following playbook:</li> </ol> <pre><code>- hosts: workload-execution-clusters\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  vars:\n   agent: 'yes'\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-workload-execution-cluster.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/","title":"Development-Environment (dev-env)","text":"<p>Mostly under construction - coming soon</p>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#hosting-kubestellar-in-a-kind-cluster","title":"Hosting KubeStellar in a Kind cluster","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#create-a-kind-cluster-with-a-port-mapping","title":"Create a Kind cluster with a port mapping","text":"<p>Create a Kind cluster with the <code>extraPortMappings</code> for the Ingress controller, which will listen at port 443 on the one kind node.  We pick a port number here that does not run afoul of the usual prohibition of ordinary user processes listening at low port numbers.</p> <pre><code>kind create cluster --name ks-host --config=- &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 1024\n    protocol: TCP\nEOF\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#create-an-nginx-ingress-controller-with-ssl-passthrough","title":"Create an nginx Ingress controller with SSL passthrough","text":"<p>Create an <code>nginx-ingress</code> with SSL passthrough. Following Kind NGINX ingress instructions, we have modified the YAML at https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml to include the <code>--enable-ssl-passthrough=true</code> argument. This is the link to our raw modified nginx controller deployment YAML.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/main/example/kind-nginx-ingress-with-SSL-passthrough.yaml\n</code></pre> <p>Wait for the ingress to be ready:</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=90s\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/#load-a-locally-built-container-image-into-the-kind-cluster","title":"Load a locally-built container image into the kind cluster","text":"<p>Remember that you can do this.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/","title":"Example1 post espw","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 10\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox workspaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox workspaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>kubectl ws root\nwhile [ $(kubectl ws tree | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into your shell if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI0721 17:37:10.186848  189094 main.go:206] \"Found APIExport view\" exportName=\"e\ndge.kubestellar.io\" serverURL=\"https://10.0.2.15:6443/services/apiexport/cseslli1ddit3s\na5/edge.kubestellar.io\"\n...\nI0721 19:17:21.906984  189094 controller.go:300] \"Created APIBinding\" worker=1\nmbwsName=\"1d55jhazpo3d3va6-mb-551bebfd-b75e-47b1-b2e0-ff0a4cb7e006\" mbwsCluster\n=\"32x6b03ixc49cj48\" bindingName=\"bind-edge\" resourceVersion=\"1247\"\n...\nI0721 19:18:56.203057  189094 controller.go:300] \"Created APIBinding\" worker=0\nmbwsName=\"1d55jhazpo3d3va6-mb-732cf72a-1ca9-4def-a5e7-78fd0e36e61c\" mbwsCluster\n=\"q31lsrpgur3eg9qk\" bindingName=\"bind-edge\" resourceVersion=\"1329\"\n^C\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>A mailbox workspace name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl ws root\nkubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE          REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal              Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal              Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\ncompute                                                    universal              Ready   https://172.20.144.39:6443/clusters/root:compute        6m8s\nespw                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:espw           2m4s\nimw1                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:imw1           1m9s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\ncompute                                                    &lt;none&gt;       mqnl7r5f56hswewy\nespw                                                       &lt;none&gt;       2n88ugkhysjbxqp5\nimw1                                                       &lt;none&gt;       4d2r9stcyy2qq5c1\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox workspace name is $GUILDER_WS\n</code></pre> <pre><code>The guilder mailbox workspace name is 1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox workspace name is $FLORIN_WS\n</code></pre> <pre><code>The florin mailbox workspace name is 1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kubestellar.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\", \"bind-apiregistration.k8s.io\" ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    namespaceSelectors: []\n    objectNames: [ \"v1090.example.my\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver; it requires the ESPW to be the current kcp workspace at start time.</p> <pre><code>kubectl ws root:espw\nkubestellar-where-resolver &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>kubectl ws root:wmw-c\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nkubectl ws root:wmw-s\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I0423 01:33:37.036752   11305 main.go:212] \"Found APIExport view\" exportName=\"edge.kubestellar.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kubestellar.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl ws root:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox workspaces and create <code>SyncerConfig</code> objects there.</p> <p>If you have deployed the KubeStellar core as workload in a Kubernetes cluster then the placement translator is running in a Pod there. If instead you are running the core controllers as bare processes then use the following commands to launch the placement translator; it requires the ESPW to be current at start time.</p> <pre><code>kubectl ws root:espw\nplacement-translator &amp;\nsleep 10\n</code></pre> <p>The following commands wait for the placement translator to get its job done for this example.</p> <pre><code># wait until SyncerConfig, ReplicaSets and Deployments are ready\nmbxws=($FLORIN_WS $GUILDER_WS)\nfor ii in \"${mbxws[@]}\"; do\nkubectl ws root:$ii\n# wait for SyncerConfig resource\nwhile ! kubectl get SyncerConfig the-one &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* SyncerConfig resource exists\"\n# wait for ReplicaSet resource\nwhile ! kubectl get rs &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* ReplicaSet resource exists\"\n# wait until ReplicaSet running\nwhile [ \"$(kubectl get rs -n commonstuff commond -o 'jsonpath={.status.readyReplicas}')\" != 1 ]; do\nsleep 10\ndone\necho \"* commonstuff ReplicaSet running\"\ndone\n# check for deployment in guilder\nwhile ! kubectl get deploy -A &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* Deployment resource exists\"\nwhile [ \"$(kubectl get deploy -n specialstuff speciald -o 'jsonpath={.status.availableReplicas}')\" != 1 ]; do\nsleep 10\ndone\necho \"* specialstuff Deployment running\"\n</code></pre> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize the name of the mailbox workspace for florin (which you stored in Stage 1) here.</p> <pre><code>kubectl ws root:$FLORIN_WS\n</code></pre> <pre><code>Current workspace is \"root:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\n</code></pre> <pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    resource: configmaps\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>You can check that the workload got there too.</p> <p><pre><code>kubectl get ns\n</code></pre> <pre><code>NAME          STATUS   AGE\ncommonstuff   Active   6m34s\ndefault       Active   32m\n</code></pre></p> <p><pre><code>kubectl get replicasets -A\n</code></pre> <pre><code>NAMESPACE     NAME      DESIRED   CURRENT   READY   AGE\ncommonstuff   commond   0         1         1       10m\n</code></pre></p> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the mailbox workspace name that you stored in Stage 1.</p> <p><pre><code>kubectl ws root:$GUILDER_WS\n</code></pre> <pre><code>Current workspace is \"root:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  clusterScope:\n  - apiVersion: v1\n    group: apiregistration.k8s.io\n    objects:\n    - v1090.example.my\n    resource: apiservices\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - speciald\n      namespace: specialstuff\n    resource: deployments\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    - names:\n      - httpd-htdocs\n      namespace: specialstuff\n    resource: configmaps\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p><pre><code>kubectl get deployments,replicasets -A\n</code></pre> <pre><code>NAMESPACE      NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff   deployment.apps/speciald   0/0     1            0           12m\n\nNAMESPACE     NAME                      DESIRED   CURRENT   READY   AGE\ncommonstuff   replicaset.apps/commond   0         1         1       7m4s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>( KUBECONFIG=~/.kube/config\n  let tries=1\nwhile ! kubectl --context kind-florin get ns commonstuff &amp;&gt; /dev/null; do\nif (( tries &gt;= 30)); then\necho 'The commonstuff namespace failed to appear in florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n    sleep 10\ndone\nkubectl --context kind-florin get ns\n)\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy,rs -A | egrep 'NAME|stuff'\n</code></pre> <pre><code>NAMESPACE                            NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\nNAMESPACE                            NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                          replicaset.apps/commond                                         1         1         1       13m\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces and both Deployments.</p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy,rs -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff                          deployment.apps/speciald                              1/1     1            1           23m\nNAMESPACE                             NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                           replicaset.apps/commond                                         1         1         1       23m\nspecialstuff                          replicaset.apps/speciald-76cdbb69b5                             1         1         1       14s\n</code></pre></p> <p>Examine the <code>APIService</code> objects in the guilder cluster, find the one named <code>v1090.example.my</code>. It is broken because it refers to a Service object that we have not bothered to create.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get apiservices | grep 1090\n</code></pre> <pre><code>v1090.example.my                       my-example/my-service   False (ServiceNotFound)   2m39s\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get rs -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8094 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>let tries=1\nwhile ! curl http://localhost:8097 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The special workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8096 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/","title":"Example1 post kcp","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#get-kubestellar","title":"Get KubeStellar","text":"<p>You will need a local copy of KubeStellar.  You can either use the pre-built archive (containing executables and config files) from a release or get any desired version from GitHub and build.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#use-pre-built-archive","title":"Use pre-built archive","text":"<p>Fetch the archive for your operating system and instruction set architecture as follows, in which <code>$kubestellar_version</code> is your chosen release of KubeStellar (see the releases on GitHub) and <code>$os_type</code> and <code>$arch_type</code> are chosen according to the list of \"assets\" for your chosen release.</p> <pre><code>curl -SL -o kubestellar.tar.gz \"https://github.com/kubestellar/kubestellar/releases/download/${kubestellar_version}/kubestellar_${kubestellar_version}_${os_type}_${arch_type}.tar.gz\ntar xzf kubestellar.tar.gz\nexport PATH=$PWD/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#get-from-github","title":"Get from GitHub","text":"<p>You can get the latest version from GitHub with the following command, which will get you the default branch (which is named \"main\"); add <code>-b $branch</code> to the <code>git</code> command in order to get a different branch.</p> <pre><code>git clone https://github.com/kubestellar/kubestellar\ncd kubestellar\n</code></pre> <p>Use the following commands to build and add the executables to your <code>$PATH</code>.</p> <pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames. The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#initialize-the-kubestellar-platform-as-bare-processes","title":"Initialize the KubeStellar platform as bare processes","text":"<p>In this step KubeStellar creates and populates the Edge Service Provider Workspace (ESPW), which exports the KubeStellar API, and also augments the <code>root:compute</code> workspace from kcp TMC as needed here. That augmentation consists of adding authorization to update the relevant <code>/status</code> and <code>/scale</code> subresources (missing in kcp TMC) and extending the supported subset of the Kubernetes API for managing containerized workloads from the four resources built into kcp TMC (<code>Deployment</code>, <code>Pod</code>, <code>Service</code>, and <code>Ingress</code>) to the other ones that are meaningful in KubeStellar.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#deploy-kcp-and-kubestellar-as-a-workload-in-a-kubernetes-cluster","title":"Deploy kcp and KubeStellar as a workload in a Kubernetes cluster","text":"<p>(This style of deployment requires release v0.6 or later of KubeStellar.)</p> <p>You need a Kubernetes cluster; see the documentation for <code>kubectl kubestellar deploy</code> for more information.</p> <p>You will need a domain name that, on each of your clients, resolves to an IP address that the client can use to open a TCP connection to the Ingress controller's listening socket.</p> <p>You will need the kcp <code>kubectl</code> plugins.  See the \"Start kcp\" section above for instructions on how to get all of the kcp executables.</p> <p>You will need to get a build of KubeStellar.  See above.</p> <p>To do the deployment and prepare to use it you will be using the commands defined for that.  These require your shell to be in a state where <code>kubectl</code> manipulates the hosting cluster (the Kubernetes cluster into which you want to deploy kcp and KubeStellar), either by virtue of having set your <code>KUBECONFIG</code> envar appropriately or putting the relevant contents in <code>~/.kube/config</code> or by passing <code>--kubeconfig</code> explicitly on the following command lines.</p> <p>Use the kubectl kubestellar deploy command to do the deployment.</p> <p>Then use the kubectl kubestellar get-external-kubeconfig command to put into a file the kubeconfig that you will use as a user of kcp and KubeStellar.  Do not overwrite the kubeconfig file for your hosting cluster.  But do update your <code>KUBECONFIG</code> envar setting or remember to pass the new file with <code>--kubeconfig</code> on the command lines when using kcp or KubeStellar. For example, you might use the following commands to fetch and start using that kubeconfig file; the first assumes that you deployed the core into a Kubernetes namespace named \"kubestellar\".</p> <pre><code>kubectl kubestellar get-external-kubeconfig -n kubestellar -o kcs.kubeconfig\nexport KUBECONFIG=$(pwd)/kcs.kubeconfig\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that you just fetched and started using for working with the KubeStellar interface. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands to put inventory objects in the IMW at <code>root:imw1</code> that was automatically created during deployment of KubeStellar. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=yes</code>.</p> <pre><code>kubectl ws root:imw1\nkubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=yes\necho \"decribe the florin location object\"\nkubectl describe location.edge.kubestellar.io florin\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects plus the kcp <code>APIBinding</code> objects that import the definition of the KubeStellar API.</p> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: yes\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: yes\nspec:\nresource: {group: edge.kubestellar.io, version: v2alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-pre-kcp/","title":"Example1 pre kcp","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-pre-kcp/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1a/","title":"Example1 stage 1a","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1a/#the-mailbox-controller","title":"The mailbox controller","text":"<p>The mailbox controller is one of the central controllers of KubeStellar.  If you have deployed the KubeStellar core as Kubernetes workload then this controller is already running in a pod in your hosting cluster. If instead you are running these controllers as bare processes then launch this controller as follows.</p> <pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 10\n</code></pre> <p>This controller is in charge of maintaining the collection of mailbox workspaces, which are an implementation detail not intended for user consumption. You can use the following command to wait for the appearance of the mailbox workspaces implied by the florin and guilder <code>SyncTarget</code> objects that you made earlier.</p> <pre><code>kubectl ws root\nwhile [ $(kubectl ws tree | grep \"\\-mb\\-\" | wc -l) -ne 2 ]; do\nsleep 10\ndone\n</code></pre> <p>If it is working correctly, lines like the following will appear in the controller's log (which is being written into your shell if you ran the controller as a bare process above, otherwise you can fetch as directed).</p> <pre><code>...\nI0721 17:37:10.186848  189094 main.go:206] \"Found APIExport view\" exportName=\"e\ndge.kubestellar.io\" serverURL=\"https://10.0.2.15:6443/services/apiexport/cseslli1ddit3s\na5/edge.kubestellar.io\"\n...\nI0721 19:17:21.906984  189094 controller.go:300] \"Created APIBinding\" worker=1\nmbwsName=\"1d55jhazpo3d3va6-mb-551bebfd-b75e-47b1-b2e0-ff0a4cb7e006\" mbwsCluster\n=\"32x6b03ixc49cj48\" bindingName=\"bind-edge\" resourceVersion=\"1247\"\n...\nI0721 19:18:56.203057  189094 controller.go:300] \"Created APIBinding\" worker=0\nmbwsName=\"1d55jhazpo3d3va6-mb-732cf72a-1ca9-4def-a5e7-78fd0e36e61c\" mbwsCluster\n=\"q31lsrpgur3eg9qk\" bindingName=\"bind-edge\" resourceVersion=\"1329\"\n^C\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>A mailbox workspace name is distinguished by <code>-mb-</code> separator. You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl ws root\nkubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE          REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal              Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal              Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\ncompute                                                    universal              Ready   https://172.20.144.39:6443/clusters/root:compute        6m8s\nespw                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:espw           2m4s\nimw1                                                       organization           Ready   https://172.20.144.39:6443/clusters/root:imw1           1m9s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kubestellar\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\ncompute                                                    &lt;none&gt;       mqnl7r5f56hswewy\nespw                                                       &lt;none&gt;       2n88ugkhysjbxqp5\nimw1                                                       &lt;none&gt;       4d2r9stcyy2qq5c1\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"guilder\") | .name')\necho The guilder mailbox workspace name is $GUILDER_WS\n</code></pre> <pre><code>The guilder mailbox workspace name is 1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kubestellar.io/sync-target-name\"] == \"florin\") | .name')\necho The florin mailbox workspace name is $FLORIN_WS\n</code></pre> <pre><code>The florin mailbox workspace name is 1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/","title":"Example1 stage 1b","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/","title":"Example1 stage 2","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the Where Resolver react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a WMW for the common workload, with the following commands.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kubestellar.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaces: [ commonstuff ]\n    objectNames: [ httpd-htdocs ]\n  - apiGroup: apps\n    resources: [ replicasets ]\n    namespaces: [ commonstuff ]\n    objectNames: [ \"*\" ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace. The <code>APIService</code> object included here does not contribute to the httpd workload but is here to demonstrate that <code>APIService</code> objects can be downsynced.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"yes\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kubestellar.io/v2alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kubestellar.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kubestellar.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\n---\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1090.example.my\nspec:\n  group: example.my\n  groupPriorityMinimum: 360\n  service:\n    name: my-service\n    namespace: my-example\n  version: v1090\n  versionPriority: 42\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"yes\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"special\":\"yes\"}\n    objectNames: [ speciald ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\", \"bind-apiregistration.k8s.io\" ]\n  - apiGroup: apiregistration.k8s.io\n    resources: [ apiservices ]\n    namespaceSelectors: []\n    objectNames: [ \"v1090.example.my\" ]\n  wantSingletonReportedState: true\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#where-resolver","title":"Where Resolver","text":"<p>In response to each EdgePlacement, the Where Resolver will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>If you have deployed the KubeStellar core in a Kubernetes cluster then the where resolver is running in a pod there. If instead you are running the core controllers are bare processes then you can use the following commands to launch the where-resolver; it requires the ESPW to be the current kcp workspace at start time.</p> <pre><code>kubectl ws root:espw\nkubestellar-where-resolver &amp;\nsleep 10\n</code></pre> <p>The following commands wait until the where-resolver has done its job for the common and special <code>EdgePlacement</code> objects.</p> <pre><code>kubectl ws root:wmw-c\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\nkubectl ws root:wmw-s\nwhile ! kubectl get SinglePlacementSlice &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>If things are working properly then you will see log lines like the following (among many others) in the where-resolver's log.</p> <pre><code>I0423 01:33:37.036752   11305 main.go:212] \"Found APIExport view\" exportName=\"edge.kubestellar.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kubestellar.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-where-resolver\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n</code></pre> <p>Check out a SinglePlacementSlice object as follows.</p> <p><pre><code>kubectl ws root:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kubestellar.io/v2alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kubestellar.io/v2alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-3/","title":"Example1 stage 3","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-3/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox workspaces and create <code>SyncerConfig</code> objects there.</p> <p>If you have deployed the KubeStellar core as workload in a Kubernetes cluster then the placement translator is running in a Pod there. If instead you are running the core controllers as bare processes then use the following commands to launch the placement translator; it requires the ESPW to be current at start time.</p> <pre><code>kubectl ws root:espw\nplacement-translator &amp;\nsleep 10\n</code></pre> <p>The following commands wait for the placement translator to get its job done for this example.</p> <pre><code># wait until SyncerConfig, ReplicaSets and Deployments are ready\nmbxws=($FLORIN_WS $GUILDER_WS)\nfor ii in \"${mbxws[@]}\"; do\nkubectl ws root:$ii\n# wait for SyncerConfig resource\nwhile ! kubectl get SyncerConfig the-one &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* SyncerConfig resource exists\"\n# wait for ReplicaSet resource\nwhile ! kubectl get rs &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* ReplicaSet resource exists\"\n# wait until ReplicaSet running\nwhile [ \"$(kubectl get rs -n commonstuff commond -o 'jsonpath={.status.readyReplicas}')\" != 1 ]; do\nsleep 10\ndone\necho \"* commonstuff ReplicaSet running\"\ndone\n# check for deployment in guilder\nwhile ! kubectl get deploy -A &amp;&gt; /dev/null; do\nsleep 10\ndone\necho \"* Deployment resource exists\"\nwhile [ \"$(kubectl get deploy -n specialstuff speciald -o 'jsonpath={.status.availableReplicas}')\" != 1 ]; do\nsleep 10\ndone\necho \"* specialstuff Deployment running\"\n</code></pre> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize the name of the mailbox workspace for florin (which you stored in Stage 1) here.</p> <pre><code>kubectl ws root:$FLORIN_WS\n</code></pre> <pre><code>Current workspace is \"root:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\n</code></pre> <pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    resource: configmaps\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>You can check that the workload got there too.</p> <p><pre><code>kubectl get ns\n</code></pre> <pre><code>NAME          STATUS   AGE\ncommonstuff   Active   6m34s\ndefault       Active   32m\n</code></pre></p> <p><pre><code>kubectl get replicasets -A\n</code></pre> <pre><code>NAMESPACE     NAME      DESIRED   CURRENT   READY   AGE\ncommonstuff   commond   0         1         1       10m\n</code></pre></p> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the mailbox workspace name that you stored in Stage 1.</p> <p><pre><code>kubectl ws root:$GUILDER_WS\n</code></pre> <pre><code>Current workspace is \"root:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kubestellar.io/v2alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  clusterScope:\n  - apiVersion: v1\n    group: apiregistration.k8s.io\n    objects:\n    - v1090.example.my\n    resource: apiservices\n  namespaceScope: {}\nnamespacedObjects:\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - commond\n      namespace: commonstuff\n    resource: replicasets\n  - apiVersion: v1\n    group: apps\n    objectsByNamespace:\n    - names:\n      - speciald\n      namespace: specialstuff\n    resource: deployments\n  - apiVersion: v1\n    group: \"\"\nobjectsByNamespace:\n    - names:\n      - httpd-htdocs\n      namespace: commonstuff\n    - names:\n      - httpd-htdocs\n      namespace: specialstuff\n    resource: configmaps\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p><pre><code>kubectl get deployments,replicasets -A\n</code></pre> <pre><code>NAMESPACE      NAME                       READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff   deployment.apps/speciald   0/0     1            0           12m\n\nNAMESPACE     NAME                      DESIRED   CURRENT   READY   AGE\ncommonstuff   replicaset.apps/commond   0         1         1       7m4s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-4/","title":"Example1 stage 4","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-4/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>( KUBECONFIG=~/.kube/config\n  let tries=1\nwhile ! kubectl --context kind-florin get ns commonstuff &amp;&gt; /dev/null; do\nif (( tries &gt;= 30)); then\necho 'The commonstuff namespace failed to appear in florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n    sleep 10\ndone\nkubectl --context kind-florin get ns\n)\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy,rs -A | egrep 'NAME|stuff'\n</code></pre> <pre><code>NAMESPACE                            NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\nNAMESPACE                            NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                          replicaset.apps/commond                                         1         1         1       13m\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces and both Deployments.</p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy,rs -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                             NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\nspecialstuff                          deployment.apps/speciald                              1/1     1            1           23m\nNAMESPACE                             NAME                                                            DESIRED   CURRENT   READY   AGE\ncommonstuff                           replicaset.apps/commond                                         1         1         1       23m\nspecialstuff                          replicaset.apps/speciald-76cdbb69b5                             1         1         1       14s\n</code></pre></p> <p>Examine the <code>APIService</code> objects in the guilder cluster, find the one named <code>v1090.example.my</code>. It is broken because it refers to a Service object that we have not bothered to create.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get apiservices | grep 1090\n</code></pre> <pre><code>v1090.example.my                       my-example/my-service   False (ServiceNotFound)   2m39s\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <pre><code>sleep 15\n</code></pre> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get rs -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8094 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on florin!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>let tries=1\nwhile ! curl http://localhost:8097 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The special workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>let tries=1\nwhile ! curl http://localhost:8096 &amp;&gt; /dev/null; do\nif (( tries &gt;= 30 )); then\necho 'The common workload failed to come up on guilder!' &gt;&amp;2\nexit 10\nfi\nlet tries=tries+1\n  sleep 10\ndone\ncurl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-start-kcp/","title":"Example1 start kcp","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-start-kcp/#deploy-kcp-and-kubestellar","title":"Deploy kcp and KubeStellar","text":"<p>You need kcp and KubeStellar and can deploy them in either of two ways: as bare processes on whatever host you are using to run this example, or as workload in a Kubernetes cluster (an OpenShift cluster qualifies).  Do one or the other, not both.</p> <p>KubeStellar only works with release <code>v0.11.0</code> of kcp.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-start-kcp/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-start-kcp/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.  See the start of the kcp quickstart for instructions on that, but get release v0.11.0 rather than the latest (the downloadable assets appear after the long list of changes/features/etc).</p> <p>Clone the v0.11.0 branch of the kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> and build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Running the kcp server creates a hidden subdirectory named <code>.kcp</code> to hold all sorts of state related to the server. If you have run it before and want to start over from scrath then you should <code>rm -rf .kcp</code> first.</p> <p>Use the following commands to: (a) run the kcp server in a forked command, (b) update your <code>KUBECONFIG</code> environment variable to configure <code>kubectl</code> to use the kubeconfig produced by the kcp server, and (c) wait for the kcp server to get through some initialization. The choice of <code>-v=3</code> for the kcp server makes it log a line for every HTTP request (among other things).</p> <pre><code>kcp start -v=3 &amp;&gt; /tmp/kcp.log &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\n# wait until KCP is ready checking availability of ws resource\nwhile ! kubectl ws tree &amp;&gt; /dev/null; do\nsleep 10\ndone\n</code></pre> <p>Note that you now care about two different kubeconfig files: the one that you were using earlier, which holds the contexts for your <code>kind</code> clusters, and the one that the kcp server creates. The remainder of this document assumes that your <code>kind</code> cluster contexts are in <code>~/.kube/config</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-teardown/","title":"Example1 teardown","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace imw1\nkubectl delete workspace $FLORIN_WS\nkubectl delete workspace $GUILDER_WS\nkubectl kubestellar remove wmw wmw-c\nkubectl kubestellar remove wmw wmw-s\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-teardown/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-teardown/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-0-deploy-florin/","title":"Kubestellar syncer 0 deploy florin","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw1\nmbws=`kubectl get synctargets.edge.kubestellar.io florin -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:$mbws\n./bin/kubectl-kubestellar-syncer_gen florin --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o florin-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\".\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-florin-32uaph9l    kubestellar-syncer-florin-32uaph9l    1/1     1            1           42s\nkube-system                           coredns                               2/2     2            2           41m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           41m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-0-deploy-guilder/","title":"Kubestellar syncer 0 deploy guilder","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw1\nkubectl get SyncTargets\nkubectl get synctargets.edge.kubestellar.io\nkubectl describe synctargets.edge.kubestellar.io guilder\n#kubectl describe Synctarget guilder\nmbws=`kubectl get synctargets.edge.kubestellar.io guilder -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:$mbws\n./bin/kubectl-kubestellar-syncer_gen guilder --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o guilder-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\".\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote WEC manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                           coredns                               2/2     2            2           35m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-1-syncer-gen-plugin/","title":"Kubestellar syncer 1 syncer gen plugin","text":"<p>Generate UUID for Syncer identification. <pre><code>syncer_id=\"syncer-\"`uuidgen | tr '[:upper:]' '[:lower:]'`\n</code></pre></p> <p>Go to a workspace. <pre><code>kubectl ws root\nkubectl ws create ws1 --enter\n</code></pre></p> <p>Create the following APIBinding in the workspace (Note that in the case of mailbox workspaces, it's done by mailbox controller at creating the mailbox workspace.) <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: bind-espw\nspec:\n  reference:\n    export:\n      path: root:espw\n      name: edge.kubestellar.io\nEOL\n</code></pre></p> <p>Create a serviceaccount in the workspace. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: $syncer_id\nEOL\n</code></pre></p> <p>Create clusterrole and clusterrolebinding to bind the serviceaccount to the role. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: $syncer_id\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- nonResourceURLs: [\"/\"]\n  verbs: [\"access\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: $syncer_id\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: $syncer_id\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: $syncer_id\n  namespace: default\nEOL\n</code></pre></p> <p>Get the serviceaccount token that will be set in the upstream kubeconfig manifest. <pre><code>secret_name=`kubectl get secret -o custom-columns=\":.metadata.name\"| grep $syncer_id`\ntoken=`kubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d`\n</code></pre></p> <p>Get the certificates that will be set in the upstream kubeconfig manifest. <pre><code>cacrt=`kubectl config view --minify --raw | yq \".clusters[0].cluster.certificate-authority-data\"`\n</code></pre></p> <p>Get server_url that will be set in the upstream kubeconfig manifest. <pre><code>server_url=`kubectl config view --minify --raw | yq \".clusters[0].cluster.server\" | sed -e 's|https://\\(.*\\):\\([^/]*\\)/.*|https://\\1:\\2|g'`\n</code></pre></p> <p>Set some other parameters. a. downstream_namespace where Syncer Pod runs <pre><code>downstream_namespace=\"kubestellar-$syncer_id\"\n</code></pre> b. Syncer image <pre><code>image=\"quay.io/kubestellar/syncer:v0.2.2\"\n</code></pre></p> <p>Download manifest template. <pre><code>curl -LO https://raw.githubusercontent.com/kubestellar/kubestellar/main/pkg/syncer/scripts/kubestellar-syncer-bootstrap.template.yaml\n</code></pre></p> <p>Generate manifests to bootstrap KubeStellar-Syncer. <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image envsubst &lt; kubestellar-syncer-bootstrap.template.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubestellar-syncer-9ee90de6-eb76-4ddb-9346-c4c8d92075e1\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n...\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller-subs/mailbox-controller-process-start-without-cd-kubestellar/","title":"Mailbox controller process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\ngo run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller-subs/mailbox-controller-process-start/","title":"Mailbox controller process start","text":"<pre><code>kubectl ws root:espw\nmailbox-controller -v=2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator-subs/placement-translator-process-start-without-cd-kubestellar/","title":"Placement translator process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\nplacement-translator &amp;\nsleep 120\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator-subs/placement-translator-process-start/","title":"Placement translator process start","text":"<pre><code>kubectl ws root:espw\ncd ../kubestellar\ngo run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-0-pull-kcp-and-kubestellar-source-and-start-kcp/","title":"Where resolver 0 pull kcp and kubestellar source and start kcp","text":"<p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>pushd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window). Set your <code>KUBECONFIG</code> environment variable to name the kubernetes client config file that <code>kcp</code> generates. <pre><code>kcp start &amp;&gt; /dev/null &amp;\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\npopd\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-1-build-kubestellar/","title":"Where resolver 1 build kubestellar","text":"<pre><code>make build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-2-ws-root-and-ws-create-edge/","title":"Where resolver 2 ws root and ws create edge","text":"<p>Next, use the command that makes sure the Edge Service Provider Workspace (ESPW), which is <code>root:espw</code>, and the TMC provider workspace (<code>root:compute</code>) are properly set up.</p> <pre><code>kubestellar init\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-exports/","title":"Where resolver exports","text":"<pre><code>kubectl ws root:espw\nkubectl apply -f ../kubestellar/config/exports/\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-imports/","title":"Where resolver imports","text":"<p>Use the user home workspace (\\~) as the workload management workspace (WMW). <pre><code>kubectl ws \\~\n</code></pre></p> <p>Bind APIs. <pre><code>kubectl apply -f config/imports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-process-start-without-cd-kubestellar/","title":"Where resolver process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\ngo run cmd/kubestellar-where-resolver/main.go -v 2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/where-resolver-subs/where-resolver-process-start/","title":"Where resolver process start","text":"<pre><code>kubectl ws root:espw\nkubestellar-where-resolver -v 2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q4/outline/","title":"Details","text":"<p>Want to get involved? Check out our good-first-issue list.</p> <p>TODO: draw new picture</p>"},{"location":"Coding%20Milestones/PoC2023q4/outline/#status-of-this-memo","title":"Status of this memo","text":"<p>This document outlines near-term plans for building on what was produced in the work on the PoC for 2023q1.</p>"},{"location":"Coding%20Milestones/PoC2023q4/outline/#introduction","title":"Introduction","text":"<p>PoC2023q1 was defined with the over-arching goal of supporting edge computing scenarios.  Since then we have realized that the technical problems that we took on are not that specific, they appear in other multi-cluster scenarios as well.</p> <p>The goals of this PoC are as follows.  Ones that are substantially different from what has been accomplished for PoC2023q1 are highlighted.</p> <ul> <li>Separation of infrastructure and workload management.</li> <li>The focus here is on workload management, and that strictly reads   an inventory of infrastructure.</li> <li>What passes from inventory to workload management is kcp TMC   Location and SyncTarget objects.</li> <li>Compared to PoC2023q1, decoupling from kcp TMC by making our own   copy of the definitions of SyncTarget and Location.</li> <li>Potentially: switch from using SyncTarget and Location to some   other representation of inventory.</li> <li>Compared to PoC2023q1, decoupling from kcp core by (1) introducing   an abstraction layer that delivers the essential functionality of   kcp's logical clusters based on a variety of implementations and (2)   using kube-bind instead of   kcp's APIExport/APIBinding.  Where PoC2023q1 used the concept of a   kcp workspace, PoC2023q4 uses the abstract concept that we call a   \"space\".</li> <li>Use of a space as the container for the central spec of a workload.</li> <li>Propagation of desired state from center outward, as directed by   EdgePlacement objects and the referenced inventory objects.</li> <li>Interfaces designed for a large number of workload execution clusters.</li> <li>Interfaces designed with the intention that workload execution   clusters operate independently of each other and the center (e.g.,   can tolerate only occasional connectivity) and thus any \"service   providers\" (in the technical sense from kcp) in the center or   elsewhere.</li> <li>Rule-based customization of desired state.</li> <li>Propagation of reported state from workload execution clusters to center.</li> <li>Summarization of reported state in the center.</li> <li>Exact, not summarized, reported state returned to workload   description space in the case of placement on exactly 1 workload   execution cluster.</li> <li>Return and/or summarization of state from associated objects (e.g.,   ReplicaSet or Pod objects associated with a given Deployment   object).</li> <li>The TCP connections are opened in the inward direction, not outward.</li> <li>A platform \"product\" that can be deployed (as opposed to a service   that is used).</li> <li>Codified support for scenarios where some KubeStellar clients and   the syncers in some of the workload execution clusters have to go   through load balancers and/or other proxies to reach the central   server(s).</li> <li>Compared to PoC2023q1, codification of closer to production grade   deployment technique(s).</li> <li>A hierarchy with more than two levels.</li> </ul> <p>Some important things that are not attempted in this PoC include the following.</p> <ul> <li>An implementation that supports a very large volume of reported   state (which could come from either a large number of workload   execution clusters and/or a large amount of reported state in each   one of those).</li> <li>User control over ordering of propagation from center outward,   either among destinations or kinds of objects.</li> <li>More than baseline security (baseline being, e.g., HTTPS, Secret   objects, non-rotating bearer token based service authentication).</li> <li>A good design for bootstrapping the workload management in the   workload execution clusters.</li> <li>Very strong isolation between tenants of this platform.</li> </ul>"},{"location":"Community/_index/","title":"Join the KubeStellar community","text":""},{"location":"Community/_index/#kubestellar-is-an-open-source-project-that-anyone-in-the-community-can-use-improve-and-enjoy-join-us-heres-a-few-ways-to-find-out-whats-happening-and-get-involved","title":"KubeStellar is an open source project that anyone in the community can use, improve, and enjoy. Join us! Here's a few ways to find out what's happening and get involved","text":""},{"location":"Community/_index/#learn-and-connect","title":"Learn and Connect","text":""},{"location":"Community/_index/#using-or-want-to-use-kubestellar-find-out-more-here","title":"Using or want to use KubeStellar? Find out more here:","text":"<ul> <li>User mailing list: Discussion and help from your fellow users</li> <li>YouTube Channel: Follow us on YouTube to view recordings of past KubeStellar community meetings and demo days</li> <li>LinkedIn: See what others are saying about the community</li> <li>Medium Blog Series: Follow us on Medium to read about community developments</li> </ul>"},{"location":"Community/_index/#develop-and-contribute","title":"Develop and Contribute","text":""},{"location":"Community/_index/#if-you-want-to-get-more-involved-by-contributing-to-kubestellar-join-us-here","title":"If you want to get more involved by contributing to KubeStellar, join us here:","text":"<ul> <li>GitHub: Development takes place here!</li> <li>#kubestellar-dev Slack channel in the Kubernetes slack workspace: Chat with other project developers</li> <li>Developer mailing list: Discuss development issues around the project</li> <li>You can find out how to contribute to KubeStellar in our Contribution Guidelines</li> </ul>"},{"location":"Community/_index/#community-meetings","title":"Community Meetings","text":"<ol> <li>Join our Developer mailing list to get your community meeting invitation.</li> <li>You can also directly subscribe to the community calendar, or view our calendar</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Sign up to discuss a topic in the KubeStellar Community Meeting Agenda</li> </ol>"},{"location":"Community/_index/#other-resources","title":"Other Resources","text":"<ul> <li>Google Drive</li> </ul>"},{"location":"Community/partners/argocd/","title":"ArgoCD","text":"<p>This document explains how to add KubeStellar's 'workspaces' as Argo CD's 'clusters'.</p>"},{"location":"Community/partners/argocd/#add-kubestellars-workspaces-to-argo-cd-as-clusters","title":"Add KubeStellar's workspaces to Argo CD as clusters","text":"<p>As of today, the 'workspaces', aka 'logical clusters' used by KubeStellar are not identical with ordinary Kubernetes clusters. Thus, in order to add them as Argo CD's 'clusters', there are a few more steps to take.</p> <p>For KubeStellar's Inventory Management Workspace (IMW) and Workload Management Workspace (WMW). The steps are similar. Let's take WMW as an example:</p> <ol> <li>Create `kube-system` namespace in the workspace.</li> <li>Make sure necessary apibindings exist in the workspace.  For WMW, we need one for Kubernetes and one for KubeStellar's edge API.</li> <li>Exclude `ClusterWorkspace` from discovery and sync.  <pre><code>kubectl -n argocd edit cm argocd-cm\n</code></pre>  Make sure `resource.exclusions` exists in the `data` field of the `argocd-cm` configmap as follows: <pre><code>data:\nresource.exclusions: |\n- apiGroups:\n- \"tenancy.kcp.io\"\nkinds:\n- \"ClusterWorkspace\"\nclusters:\n- \"*\"\n</code></pre>  Restart the Argo CD server. <pre><code>kubectl -n argocd rollout restart deployment argocd-server\n</code></pre>  Argo CD's documentation mentions this feature as [Resource Exclusion/Inclusion](https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion). </li> <li>Make sure the current context uses WMW, then identify the admin.kubeconfig. The command and output should be similar to <pre><code>$ argocd cluster add --name wmw --kubeconfig ./admin.kubeconfig workspace.kcp.io/current\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `workspace.kcp.io/current` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" updated\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" updated\nCluster 'https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo' added\n</code></pre>  ### Create Argo CD Applications Once KubeStellar's workspaces are added, Argo CD Applications can be created as normal. There are a few examples listed [here](https://github.com/edge-experiments/gitops-source/tree/main/kubestellar), and the commands to use the examples are listed as follows.  #### Create Argo CD Applications against KubeStellar's IMW Create two Locations. The command and output should be similar to <pre><code>$ argocd app create locations \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/locations/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'locations' created\n</code></pre>  Create two SyncTargets. The command and output should be similar to <pre><code>$ argocd app create synctargets \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/synctargets/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'synctargets' created\n</code></pre>  #### Create Argo CD Application against KubeStellar's WMW Create a Namespace. The command and output should be similar to <pre><code>$ argocd app create namespace \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/namespaces/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'namespace' created\n</code></pre>  Create a Deployment for 'cpumemload'. The command and output should be similar to <pre><code>$ argocd app create cpumemload \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/workloads/cpumemload/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'cpumemload' created\n</code></pre>  Create an EdgePlacement. The command and output should be similar to <pre><code>$ argocd app create edgeplacement \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/placements/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'edgeplacement' created\n</code></pre> </li> </ol>"},{"location":"Community/partners/argocd/#other-resources","title":"Other Resources","text":"<p>Medium - Sync 10,000 ArgoCD Applications in One Shot Medium - Sync 10,000 ArgoCD Applications in One Shot, by Yourself Medium - GitOpsCon - here we come</p>"},{"location":"Community/partners/argocd/#argocd-scale-experiment-kubestellar-community-demo-day","title":"ArgoCD Scale Experiment - KubeStellar Community Demo Day","text":""},{"location":"Community/partners/argocd/#gitopscon-2023-a-quantitative-study-on-argo-scalability-andrew-anderson-jun-duan-ibm","title":"GitOpsCon 2023 - A Quantitative Study on Argo Scalability - Andrew Anderson &amp; Jun Duan, IBM","text":""},{"location":"Community/partners/argocd/#argocd-and-kubestellar-in-the-news","title":"ArgoCD and KubeStellar in the news","text":""},{"location":"Community/partners/fluxcd/","title":"FluxCD","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/kyverno/","title":"Check out KubeStellar working with Kyverno:","text":"<p>Medium - Syncing Objects from one Kubernetes cluster to another Kubernetes cluster</p>"},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-demo-day","title":"Kyverno and KubeStellar Demo Day","text":""},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-in-the-news","title":"Kyverno and KubeStellar in the news","text":""},{"location":"Community/partners/kyverno/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/","title":"Check out KubeStellar working with IBM's Maximo Visual Inspection (MVI):","text":"<p>Medium - Deployment and configuration of MVI-Edge using KubeStellar</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-demo-day","title":"MVI and KubeStellar Demo Day","text":""},{"location":"Community/partners/mvi/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-in-the-news","title":"MVI and KubeStellar in the news","text":""},{"location":"Community/partners/openziti/","title":"OpenZiti","text":""},{"location":"Community/partners/turbonomic/","title":"Check out KubeStellar working with Turbonomic:","text":"<p>Medium - Make Multi-Cluster Scheduling a No-Brainer</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-demo-day","title":"Turbonomic and KubeStellar Demo Day","text":""},{"location":"Community/partners/turbonomic/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>As we can see from the blog and the demo, Turbonomic talks to KubeStellar via GitOps. The scheduling decisions are passed from Turbonomic to KubeStellar in two steps: 1. Turbo -&gt; GitHub repository. 2. GitHub repository -&gt; KubeStellar.</p> <p>For the 1st step (Turbonomic -&gt; GitHub repository), a controller named \"change reconciler\" creates PRs against the GitHub repository, where the PRs contains changes to scheduling decisions.</p> <p>There's also a piece of code which intercepts Turbonomic actions and creates CRs for the above change reconciler.</p> <p>For the 2nd step (GitHub repository-&gt; KubeStellar), we can use Argo CD. The detailed procedure to integrate Argo CD with KubeStellar is documented here.</p> <p>As we can see from the blog and the demo, Turbonomic collects data from edge clusters. This is made possible by installing kubeturbo into each of the edge clusters.</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-in-the-news","title":"Turbonomic and KubeStellar in the news","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through this document and familiarize yourself with our code of conduct. If you have any inquiries, please feel free to reach out to us on the KubeStellar-dev Slack channel.</p> <p>We can't wait to collaborate with you!</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#contributing-code","title":"Contributing Code","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#prerequisites","title":"Prerequisites","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#go","title":"Go","text":"<p>Install Go 1.19+.  See this gist for another way to install Go.   Please note that the go language version numbers in these files must exactly agree:</p> <pre><code>Your local go/go.mod file, kcp/.ci-operator.yaml, and in all the kcp/.github/workflows yaml files that specify go-version.\n\n- In ./ci-operator.yaml the go version is indicated by the \"tag\" attribute.\n- In go.mod it is indicated by the \"go\" directive.\n- In the .github/workflows yaml files it is indicated by \"go-version\"\n</code></pre> <p>Check out our QuickStart Guide</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#other-packages","title":"Other packages","text":"<ul> <li>GNU make</li> <li>ko (required for compiling KubeStellar Syncer)</li> <li>slsa-verifier needed in Ubuntu for ko signing</li> </ul>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#issues","title":"Issues","text":"<p>Prioritization for pull requests is given to those that address and resolve existing GitHub issues. Utilize the available issue labels to identify meaningful and relevant issues to work on.</p> <p>If you believe that there is a need for a fix and no existing issue covers it, feel free to create a new one.</p> <p>As a new contributor, we encourage you to start with issues labeled as good first issues.</p> <p>Your assistance in improving documentation is highly valued, regardless of your level of experience with the project.</p> <p>To claim an issue that you are interested in, kindly leave a comment on the issue and request the maintainers to assign it to you.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#committing","title":"Committing","text":"<p>We encourage all contributors to adopt best practices in git commit management to facilitate efficient reviews and retrospective analysis. Your git commits should provide ample context for reviewers and future codebase readers.</p> <p>A recommended format for final commit messages is as follows:</p> <pre><code>{Short Title}: {Problem this commit is solving and any important contextual information} {issue number if applicable}\n</code></pre>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>When submitting a pull request, clear communication is appreciated. This can be achieved by providing the following information:</p> <ul> <li>Detailed description of the problem you are trying to solve, along with links to related GitHub issues</li> <li>Explanation of your solution, including links to any design documentation and discussions</li> <li>Information on how you tested and validated your solution</li> <li>Updates to relevant documentation and examples, if applicable</li> </ul> <p>The pull request template has been designed to assist you in communicating this information effectively.</p> <p>Smaller pull requests are typically easier to review and merge than larger ones. If your pull request is big, it is always recommended to collaborate with the maintainers to find the best way to divide it.</p> <p>Approvers will review your PR within a business day. A PR requires both an /lgtm and then an /approve in order to get merged. You may /approve your own PR but you may not /lgtm it. Automation will add the PR it to the OpenShift PR merge queue. The OpenShift Tide bot will automatically merge your work when it is available.</p> <p>Congratulations! Your pull request has been successfully merged! \ud83d\udc4f</p> <p>If you have any questions about contributing, don't hesitate to reach out to us on the KubeStellar-dev Slack channel.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-locally","title":"Testing Locally","text":"<p>Our QuickStart  guide shows a user how to install a local KCP server and install the KubeStellar components and run an example.  As a contributor you will want a different setup flow, including <code>git clone</code> of this repo instead of fetching and unpacking a release archive.  The same example usage should work for you, and there is a larger example at this link.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-changes-to-the-kubestellar-central-container-image","title":"Testing changes to the KubeStellar central container image","text":"<p>If you make a change that affects the container image holding the central components then you will need to build a new image; perhaps surprisingly, this is not included in <code>make build</code>.  The regular way to build this image is with the following command.  It builds a multi-platform image, for all the platforms that KubeStellar can run its central components on, and pushes it to quay.io.  Read the remarks below before trying this.</p> <pre><code>make kubestellar-image\n</code></pre> <p>The set of target platforms can be specified by setting the <code>CORE_PLATFORMS</code> variable. The following command is equivalent to the default behavior.</p> <pre><code>make kubestellar-image CORE_PLATFORMS=linux/amd64,linux/arm64,linux/ppc64le\n</code></pre> <p>NOTE VERY SHARP AND BURIED EDGE: IF the target platforms include   <code>linux/amd64</code> --- either because you explicitly set that or you let   the default setting apply --- then you MUST issue this command on a   machine (real or virtual) with the x86-64-v2   instructions. \"x86-64-v2\" is a shorthand for a bundle of instruction   set features that have been appearing in x86 chips for many years   now (any real machine that you are likely to use today has them) but   still do not all appear by default in some common emulators. See   QEMU configuration   recommendations,   for example. If the machine lacks the v2 instructions then the build   will fail when it tries to use the glibc in the redhat/ubi9   image. Cross-platform building when the builder is NOT x86 and the   target IS x86 is beyond the ken of modern technology (see   here and   here). If   you somehow succeed to build for the target platform   <code>linux/amd64/v2</code> and successfully test on real x86 hardware you   still are not done: when you try to use this image in OpenShift on   x86 you may get inexplicable failures to pull the image.</p> <p>The command shown above will only succeed if you have done <code>docker login</code> to quay.io with credentials authorized to write to the <code>kubestellar/kubestellar</code> repository. Look on quay.io to find the image you just pushed, you will soon need to use one of its tags. This make target pushes the image with two tags, one based on build timestamp and one based on git metadata.</p> <p>If you are not authorized to write to <code>quay.io/kubestellar/kubestellar</code> then you can specify an alternate image repository: put it in the make variable named <code>CORE_IMAGE_REPO</code>.  For example, you might invoke <code>make kubestellar-image CORE_IMAGE_REPO=docker.io/myacct/ksctr</code>.</p> <p>Another variable that you might like to use is <code>EXTRA_CORE_TAG</code>. This causes the make command to push the image with a third tag that you supply in that variable.  For example, if you want to tag the image with a release tag you might invoke <code>make kubestellar-image EXTRA_CORE_TAG=v0.42.7</code>.</p> <p>For a less pushy alternative you can build a single-platform image and not push it, using the following command. It also supports the <code>CORE_IMAGE_REPO</code> and <code>EXTRA_CORE_TAG</code> variables.  But it only builds for your local \"platform\"; you can use this if you have podman pretending to be docker.</p> <pre><code>make kubestellar-image-local\n</code></pre> <p>Follow that with <code>docker images</code> to find the tag of the image you just built.  Get that image:tag known where you are going to run the central container; for example, if that will be in a local <code>kind</code> cluster then you can use kind load.</p> <p>To get the image you just built used in your testing, edit <code>outer-scripts/kubectl-kubestellar-deploy</code> and update the line that defines <code>$image_tag</code>; follow this with your <code>make build</code>.  For the sake of future users of a merged change, your last edit like this should refer to a tag that you pushed to quay.io/kubestellar/kubestellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-changes-to-the-bootstrap-script","title":"Testing changes to the bootstrap script","text":"<p>The quickstart says to fetch the bootstrap script from the main branch of the KubeStellar repo; if you want to contribute a change to that script then you will need to test your changed version.  Just run your local copy (perhaps in a special testing directory, just to be safe) and be sure to add the downloaded <code>bin</code> at the front of your <code>$PATH</code> (contrary to what the scripting currently tells you) so that your <code>git clone</code>'s <code>bin</code> does not shadow the one being tested.</p> <p>Note that changes to the bootstrap script start being used by users as soon as your PR merges.  Since this script can only fetch a released version of the executables, changes to this script can not rely on any behavior of those executables that is not in the currently latest release.  Also, a change that restricts the range of usable releases needs to add checking for use of incompatible releases.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-the-bootstrap-script-against-an-upcoming-release","title":"Testing the bootstrap script against an upcoming release","text":"<p>Prior to making a new release, there needs to be testing that the current bootstrap script works with the executable behavior that will appear in the new release.  To support this we will add an option to the bootstrap script that enables it to use a local release archive instead of fetching an archive of an actual release from github.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>KubeStellar is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p> <p>Please read the following guide if you're interested in contributing to KubeStellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p>"},{"location":"Contribution%20guidelines/LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following    boilerplate notice, with the fields enclosed by brackets \"[]\"    replaced with your own identifying information. (Don't include    the brackets!)  The text should be enclosed in the appropriate    comment syntax for the file format. We also recommend that a    file or class name and description of purpose be included on the    same \"printed page\" as the copyright notice for easier    identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"Contribution%20guidelines/coc/","title":"Code of Conduct","text":"<p>The Code of Conduct serves as a set of rules used by the KubeStellar community to establish a safe, respectful and inclusive environment.</p>"},{"location":"Contribution%20guidelines/coc/#kubestellar-community-code-of-conduct","title":"KubeStellar Community Code of Conduct","text":"<p>The KubeStellar Community abides by the CNCF Code of Conduct.</p> <p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the KubeStellar Code of Conduct Committee of Conduct Committee.</p>"},{"location":"Contribution%20guidelines/coc/#contributor-code-of-conduct","title":"Contributor Code of Conduct","text":"<p>As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.</p> <p>We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, or nationality.</p> <p>Examples of unacceptable behavior by participants include:</p> <p>The use of sexualized language or imagery Personal attacks Trolling or insulting/derogatory comments Public or private harassment Publishing others\u2019 private information, such as physical or electronic addresses, without explicit permission Other unethical or unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. By adopting this Code of Conduct, project maintainers commit themselves to fairly and consistently applying these principles to every aspect of managing this project. Project maintainers who do not follow or enforce the Code of Conduct may be permanently removed from the project team.</p> <p>This code of conduct applies both within project spaces and in public spaces when an individual is representing the project or its community.</p> <p>Instances of abusive, harassing, or otherwise unacceptable behavior in KubeStellar may be reported by contacting the KubeStellar Code of Conduct Committee of Conduct Committee. For other projects, please contact a CNCF project maintainer or our mediator, Mishi Choudhary mishi@linux.com.</p> <p>This Code of Conduct is adapted from the Contributor Covenant (http://contributor-covenant.org), version 1.2.0, available at http://contributor-covenant.org/version/1/2/0/</p> <p>CNCF Events Code of Conduct</p> <p>CNCF events are governed by the Linux Foundation Code of Conduct available on the event page. This is designed to be compatible with the above policy and also includes more details on responding to incidents.</p>"},{"location":"Contribution%20guidelines/governance/","title":"Governance","text":""},{"location":"Contribution%20guidelines/governance/#kubestellar-project-governance","title":"KubeStellar Project Governance","text":"<p>The KubeStellar project is dedicated to solving challenges stemming from multi-cluster configuration management for edge, multi-cloud, and hybrid cloud.  This governance explains how the project is run.</p> <ul> <li>Manifesto</li> <li>Values</li> <li>Maintainers</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifying this Charter</li> </ul>"},{"location":"Contribution%20guidelines/governance/#manifesto","title":"Manifesto","text":"<ul> <li>KubeStellar Maintainers strive to be good citizens in the Kubernetes project.</li> <li>KubeStellar Maintainers see KubeStellar always as part of the Kubernetes ecosystem and always     strive to keep that ecosystem united. In particular, this means:</li> <li>KubeStellar strives to not divert from Kubernetes, but strives to extend its       use-cases to non-container control planes while keeping the ecosystems of       libraries and tooling united.</li> <li>KubeStellar \u2013 as a consumer of Kubernetes API Machinery \u2013 will strive to stay 100%       compatible with the semantics of Kubernetes APIs, while removing container       orchestration specific functionality.</li> <li>KubeStellar strives to upstream changes to Kubernetes code as much as possible.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#values","title":"Values","text":"<p>The KubeStellar and its leadership embrace the following values:</p> <ul> <li>Openness: Communication and decision-making happens in the open and is     discoverable for future reference. As much as possible, all discussions and     work take place in public forums and open repositories.</li> <li>Fairness: All stakeholders have the opportunity to provide feedback and     submit contributions, which will be considered on their merits.</li> <li>Community over Product or Company: Sustaining and growing our community     takes priority over shipping code or sponsors' organizational goals. Each     contributor participates in the project as an individual.</li> <li>Inclusivity: We innovate through different perspectives and skill sets,     which can only be accomplished in a welcoming and respectful environment.</li> <li>Participation: Responsibilities within the project are earned through     participation, and there is a clear path up the contributor ladder into     leadership positions.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#maintainers","title":"Maintainers","text":"<p>KubeStellar Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found as top-level approvers in OWNERS.  Maintainers collectively  manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the KubeStellar project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which  is the governing body for the project.</p>"},{"location":"Contribution%20guidelines/governance/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>participate in discussions, contributions, code and documentation reviews   for 3 months or more,</li> <li>perform reviews for 5 non-trivial pull requests,</li> <li>contribute 5 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.    </li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority  vote of existing Maintainers approves the application.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights, and invited to the private maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#bootstrapping-maintainers","title":"Bootstrapping Maintainers","text":"<p>To bootstrap the process, 3 maintainers are defined (in the initial PR adding  this to the repository) that do not necessarily follow the above rules. When a  new maintainer is added following the above rules, the existing maintainers  define one not following the rules to step down, until all of them follow the  rules.</p>"},{"location":"Contribution%20guidelines/governance/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to  continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons.  Inactivity is defined as a period of very low or no activity in the project for  a year or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus  status. Emeritus Maintainers will still be consulted on some project matters,  and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"Contribution%20guidelines/governance/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public  community call meeting. Maintainers will also have closed meetings in order to  discuss security reports or Code of Conduct violations. Such meetings should be  scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any  Maintainer who is accused of a CoC violation.</p>"},{"location":"Contribution%20guidelines/governance/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this  responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it. The Maintainers will review who is assigned to this  at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security  holes and breaches according to the security policy.</p>"},{"location":"Contribution%20guidelines/governance/#voting","title":"Voting","text":"<p>While most business in KubeStellar is conducted by \"lazy consensus\", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer mailing list for security or conduct matters.  Votes may also be taken at the community call  meeting. Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed. Maintainers can be removed by a 2/3 majority vote of all Maintainers, and changes to this Governance require a 2/3 vote of all Maintainers.</p>"},{"location":"Contribution%20guidelines/governance/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a  2/3 vote of the Maintainers.</p>"},{"location":"Contribution%20guidelines/onboarding/","title":"Onboarding","text":"<p>KubeStellar GitHub Organization On-boarding and Off-boarding Policy</p> <p>Effective Date: June 1st, 2023</p> <p>At KubeStellar we love our contributors.  Our contributors can make various valuable contributions to our project. They can actively engage in code development by submitting pull requests, implementing new features, or fixing bugs. Additionally, contributors can assist with testing, CICD, documentation, providing clear and comprehensive guides, tutorials, and examples. Moreover, they can contribute to the project by participating in discussions, offering feedback, and helping to improve overall community engagement and collaboration.</p> <ol> <li> <p>Introduction: The purpose of this policy is to ensure a smooth on-boarding and off-boarding process for members of the KubeStellar GitHub organization. This policy applies to all individuals joining or leaving the organization, including community contributors.</p> </li> <li> <p>On-boarding Process: 2.1. Access Request:</p> </li> <li>New members shall submit an access request, via a blank GitHub issue from the KubeStellar repository, mentioning all members of the OWNERS file.</li> <li>The access request should include the member's GitHub username and a brief description of their role and contributions to the KubeStellar project.</li> </ol> <p>2.2. Review and Approval: - The organization's maintainers or designated personnel will review the access request issue. - The maintainers will evaluate the request based on the member's role, contributions, and adherence to the organization's code of conduct. - Upon approval, the member will receive an invitation to join the KubeStellar GitHub organization.</p> <p>2.3. Getting Help: - The organization's maintainers are here to help contributors be efficient and confident in their collaboration effort. If you need help you can reach out to the maintainers on slack at the KubeStellar-dev channel. - Be sure to join the KubeStellar-dev Google Group to get access to important artifacts like proposals, diagrams, and meeting invitations.</p> <p>2.4. Orientation: - Newly on-boarded members will be provided with contribution guidelines. - The guide will include instructions on how to access relevant repositories, participate in discussions, and contribute to ongoing projects.</p> <ol> <li>Off-boarding Process: 3.1. Departure Notification:</li> <li>Members leaving the organization shall notify the maintainers or their respective team lead in advance of their departure date.</li> <li>The notification should include the member's departure date and any necessary transition information.</li> </ol> <p>3.2. Access Termination: - Upon receiving the departure notification, the maintainers or designated personnel will initiate the off-boarding process. - The member's access to the KubeStellar GitHub organization will be revoked promptly to ensure data security and prevent unauthorized access.</p> <p>3.3. Knowledge Transfer: - Departing members should facilitate the transfer of their ongoing projects, tasks, and knowledge to their respective replacements or relevant team members. - Documentation or guidelines related to ongoing projects should be updated and made available to the team for seamless continuity.</p> <ol> <li>Code of Conduct:</li> <li>All members of the KubeStellar GitHub organization are expected to adhere to the organization's code of conduct, promoting a respectful and inclusive environment.</li> <li> <p>Violations of the code of conduct will be addressed following the organization's established procedures for handling such incidents.</p> </li> <li> <p>Policy Compliance:</p> </li> <li>It is the responsibility of all members to comply with the on-boarding and off-boarding policy.</li> <li> <p>The organization's maintainers or designated personnel will oversee the implementation and enforcement of this policy.</p> </li> <li> <p>Policy Review:</p> </li> <li>This policy will be reviewed periodically to ensure its effectiveness and relevance.</li> <li>Any updates or revisions to the policy will be communicated to the organization's members in a timely manner.</li> </ol> <p>Please note that this policy is subject to change, and any modifications will be communicated to all members of the KubeStellar GitHub organization.</p> <p>By joining the organization, all members agree to abide by the terms and guidelines outlined in this policy.</p> <p>Andy Anderson (clubanderson) KubeStellar Maintainer June 1, 2023</p>"},{"location":"Contribution%20guidelines/operations/all-macros/","title":"All macros","text":"<p>All variables supported by this documentation implementation:</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#macros-plugin-environment","title":"Macros Plugin Environment","text":""},{"location":"Contribution%20guidelines/operations/all-macros/#general-list","title":"General List","text":"<p>All available variables and filters within the macros plugin:</p> Variable Type Content extra dict version [dict], analytics [dict] config MkDocsConfig {'config_file_path': '/home/runner/work/kubestellar/kubestellar/docs/mike-mkdocsr8d0i9cm.yml', 'site_name': 'KubeStellar', 'nav': [{'Home': [{'Overview': 'index.md'}, {'Readme': 'readme.md'}]}, {'Getting Started': [{'QuickStart': 'Getting-Started/quickstart.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'User Guide': 'Getting-Started/user-guide.md'}, {'KubeStellar - The Infomercial': 'Getting-Started/infomercial.md'}]}, {'Contributing': [{'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}, {'Operations': [{'Code Management': 'Contribution guidelines/operations/code-management.md'}, {'Release Management': 'Contribution guidelines/operations/release-management.md'}, {'Document Management': 'Contribution guidelines/operations/document-management.md'}]}]}, {'Coding Milestones': [{'PoC2023q1': [{'Details': 'Coding Milestones/PoC2023q1/outline.md'}, {'Invitation to Contribute': 'Coding Milestones/PoC2023q1/coding-milestone-invite-q1.md'}, {'KubeStellar-Syncer': 'Coding Milestones/PoC2023q1/kubestellar-syncer.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'KubeStellar Processes': [{'KubeStellar Where Resolver': 'Coding Milestones/PoC2023q1/where-resolver.md'}, {'KubeStellar Mailbox Controller': 'Coding Milestones/PoC2023q1/mailbox-controller.md'}, {'KubeStellar Placement Translator': 'Coding Milestones/PoC2023q1/placement-translator.md'}]}, {'Use Cases Affecting the Roadmap for PoC2023q1': 'Coding Milestones/PoC2023q1/roadmap-uses.md'}, {'Environments': [{'Overview': 'Coding Milestones/PoC2023q1/environments/_index.md'}, {'Cloud Environment': 'Coding Milestones/PoC2023q1/environments/cloud-env.md'}, {'Developer Environment': 'Coding Milestones/PoC2023q1/environments/dev-env.md'}]}, {'Reference': [{'Commands': 'Coding Milestones/PoC2023q1/commands.md'}]}]}, {'PoC2023q4': [{'Details': 'Coding Milestones/PoC2023q4/outline.md'}]}]}, {'Community': [{'Get Involved': 'Community/_index.md'}, {'Contributor Guidelines': 'https://docs.kubestellar.io/main/Contribution%20guidelines/CONTRIBUTING/'}, {'Contact Us': [{'Mailing List': 'https://kubestellar.io/join_us'}, {'Community Meeting Agenda (join mailing list first)': 'https://kubestellar.io/agenda'}, {'Slack': 'https://kubestellar.io/slack'}, {'Medium Blog': 'https://kubestellar.io/blog'}, {'YouTube Channel': 'https://kubestellar.io/tv'}, {'LinkedIn': 'https://kubestellar.io/linkedin'}, {'Reddit': 'https://www.reddit.com/r/kubestellar/'}, {'Google Drive': 'https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS'}]}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}], 'pages': None, 'site_url': 'https://docs.kubestellar.io/main', 'site_description': None, 'site_author': None, 'theme': Theme(name='material', dirs=['/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/kubestellar/kubestellar/docs/overrides', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png'), 'docs_dir': '/home/runner/work/kubestellar/kubestellar/docs/content', 'site_dir': '/home/runner/work/kubestellar/kubestellar/docs/generated', 'copyright': None, 'google_analytics': None, 'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000), 'use_directory_urls': True, 'repo_url': 'https://github.com/kubestellar/kubestellar', 'repo_name': 'GitHub', 'edit_uri_template': None, 'edit_uri': 'edit/main/docs/content/', 'extra_css': ['stylesheets/kubestellar.css'], 'extra_javascript': ['js/open_in_new_tab.js'], 'extra_templates': [], 'markdown_extensions': ['toc', 'tables', 'fenced_code', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'], 'mdx_configs': {'toc': {'permalink': '#'}, 'pymdownx.highlight': {'anchor_linenums': True}, 'pymdownx.tabbed': {'alternate_style': True}}, 'strict': False, 'remote_branch': 'gh-pages', 'remote_name': 'origin', 'extra': {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}}, 'plugins': {'mike': , 'awesome-pages': , 'material/search': , 'open-in-new-tab': , 'include-markdown': , 'macros': , 'i18n': }, 'hooks': {}, 'watch': ['/home/runner/work/kubestellar/kubestellar/docs/mkdocs.yml', '/home/runner/work/kubestellar/kubestellar/docs/content', '/home/runner/work/kubestellar/kubestellar/docs/overrides'], 'repo_short_name': 'kubestellar/kubestellar', 'repo_default_file_path': 'kubestellar', 'docs_url': 'https://docs.kubestellar.io', 'repo_raw_url': 'https://raw.githubusercontent.com/kubestellar/kubestellar', 'ks_branch': 'main', 'ks_tag': 'v0.9.0', 'ks_next_branch': 'release-0.10', 'ks_next_tag': 'v0.10.0', 'ks_port_num': '1119'} environment dict system = 'Linux', system_version = '6.2.0-1014-azure', python_version = '3.10.13', mkdocs_version = '1.4.2', macros_plugin_version = '0.7.0', jinja2_version = '3.1.2' plugin LegacyConfig {'module_name': 'main', 'modules': [], 'include_dir': 'overrides', 'include_yaml': [], 'j2_block_start_string': '', 'j2_block_end_string': '', 'j2_variable_start_string': '', 'j2_variable_end_string': '', 'on_undefined': 'keep', 'on_error_fail': False, 'verbose': False} git dict status = True, date [datetime], short_commit = 'dacb098f', commit = 'dacb098f6cb7694932017b00dc4fef51ed7a1cb2', tag = '', author = 'Andy Anderson', author_email = 'andy@clubanderson.com', committer = 'GitHub', committer_email = 'noreply@github.com', date_ISO = 'Mon Oct 23 09:38:47 2023 -0400', message = 'Update quickstart-1-install-and-run-kubestellar.md', raw = 'commit dacb098f6cb7694932017b00dc4fef51ed7a1cb2\\nAuthor: Andy Anderson \\nDate:   Mon Oct 23 09:38:47 2023 -0400\\n\\n    Update quickstart-1-install-and-run-kubestellar.md', root_dir = '/home/runner/work/kubestellar/kubestellar' version dict default = 'stable', provider = 'mike' analytics dict provider = 'google', property = 'G-SR5TD1CXY7', feedback [dict] macros SuperDict context [function], macros_info [function], now [function], fix_url [function], section_items [function] filters dict pretty [function] filters_builtin dict abs [builtin_function_or_method], attr [function], batch [function], capitalize [function], center [function], count [builtin_function_or_method], d [function], default [function], dictsort [function], e [builtin_function_or_method], escape [builtin_function_or_method], filesizeformat [function], first [function], float [function], forceescape [function], format [function], groupby [function], indent [function], int [function], join [function], last [function], length [builtin_function_or_method], list [function], lower [function], items [function], map [function], min [function], max [function], pprint [function], random [function], reject [function], rejectattr [function], replace [function], reverse [function], round [function], safe [function], select [function], selectattr [function], slice [function], sort [function], string [builtin_function_or_method], striptags [function], sum [function], title [function], trim [function], truncate [function], unique [function], upper [function], urlencode [function], urlize [function], wordcount [function], wordwrap [function], xmlattr [function], tojson [function] navigation Navigation Section(title='Home')    Page(title='Overview', url='/main/')    Page(title='Readme', url='/main/readme/')Section(title='Getting Started')    Page(title='QuickStart', url='/main/Getting-Started/quickstart/')    Page(title='Extended Example', url='/main/Coding%20Milestones/PoC2023q1/example1/')    Page(title='User Guide', url='/main/Getting-Started/user-guide/')    Page(title='KubeStellar - The Infomercial', url='/main/Getting-Started/infomercial/')Section(title='Contributing')    Page(title='Guidelines', url='/main/Contribution%20guidelines/CONTRIBUTING/')    Page(title='Code of Conduct', url='/main/Contribution%20guidelines/coc/')    Page(title='License', url='/main/Contribution%20guidelines/LICENSE/')    Page(title='Governance', url='/main/Contribution%20guidelines/governance/')    Page(title='Onboarding', url='/main/Contribution%20guidelines/onboarding/')    Section(title='Security')        Page(title='Policy', url='/main/Contribution%20guidelines/security/security/')        Page(title='Contacts', url='/main/Contribution%20guidelines/security/security_contacts/')    Section(title='Operations')        Page(title='Code Management', url='/main/Contribution%20guidelines/operations/code-management/')        Page(title='Release Management', url='/main/Contribution%20guidelines/operations/release-management/')        Page(title='Document Management', url='/main/Contribution%20guidelines/operations/document-management/')Section(title='Coding Milestones')    Section(title='PoC2023q1')        Page(title='Details', url='/main/Coding%20Milestones/PoC2023q1/outline/')        Page(title='Invitation to Contribute', url='/main/Coding%20Milestones/PoC2023q1/coding-milestone-invite-q1/')        Page(title='KubeStellar-Syncer', url='/main/Coding%20Milestones/PoC2023q1/kubestellar-syncer/')        Page(title='Extended Example', url='/main/Coding%20Milestones/PoC2023q1/example1/')        Section(title='KubeStellar Processes')            Page(title='KubeStellar Where Resolver', url='/main/Coding%20Milestones/PoC2023q1/where-resolver/')            Page(title='KubeStellar Mailbox Controller', url='/main/Coding%20Milestones/PoC2023q1/mailbox-controller/')            Page(title='KubeStellar Placement Translator', url='/main/Coding%20Milestones/PoC2023q1/placement-translator/')        Page(title='Use Cases Affecting the Roadmap for PoC2023q1', url='/main/Coding%20Milestones/PoC2023q1/roadmap-uses/')        Section(title='Environments')            Page(title='Overview', url='/main/Coding%20Milestones/PoC2023q1/environments/_index/')            Page(title='Cloud Environment', url='/main/Coding%20Milestones/PoC2023q1/environments/cloud-env/')            Page(title='Developer Environment', url='/main/Coding%20Milestones/PoC2023q1/environments/dev-env/')        Section(title='Reference')            Page(title='Commands', url='/main/Coding%20Milestones/PoC2023q1/commands/')    Section(title='PoC2023q4')        Page(title='Details', url='/main/Coding%20Milestones/PoC2023q4/outline/')Section(title='Community')    Page(title='Get Involved', url='/main/Community/_index/')    Link(title='Contributor Guidelines', url='https://docs.kubestellar.io/main/Contribution%20guidelines/CONTRIBUTING/')    Section(title='Contact Us')        Link(title='Mailing List', url='https://kubestellar.io/join_us')        Link(title='Community Meeting Agenda (join mailing list first)', url='https://kubestellar.io/agenda')        Link(title='Slack', url='https://kubestellar.io/slack')        Link(title='Medium Blog', url='https://kubestellar.io/blog')        Link(title='YouTube Channel', url='https://kubestellar.io/tv')        Link(title='LinkedIn', url='https://kubestellar.io/linkedin')        Link(title='Reddit', url='https://www.reddit.com/r/kubestellar/')        Link(title='Google Drive', url='https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS')    Section(title='Partners')        Page(title='ArgoCD', url='/main/Community/partners/argocd/')        Page(title='Turbonomic', url='/main/Community/partners/turbonomic/')        Page(title='MVI', url='/main/Community/partners/mvi/')        Page(title='FluxCD', url='/main/Community/partners/fluxcd/')        Page(title='OpenZiti', url='/main/Community/partners/openziti/')        Page(title='Kyverno', url='/main/Community/partners/kyverno/')Link(title='Blog', url='https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank') files I18nFiles page Page Page(title='All macros', url='/main/Contribution%20guidelines/operations/all-macros/')"},{"location":"Contribution%20guidelines/operations/all-macros/#config-information","title":"Config Information","text":"<p>Standard MkDocs configuration information. Do not try to modify.</p> <p>e.g. <code>{{ config.docs_dir }}</code></p> <p>See also the MkDocs documentation on the config object.</p> Variable Type Content config_file_path str '/home/runner/work/kubestellar/kubestellar/docs/mike-mkdocsr8d0i9cm.yml' site_name str 'KubeStellar' nav list [{'Home': [{'Overview': 'index.md'}, {'Readme': 'readme.md'}]}, {'Getting Started': [{'QuickStart': 'Getting-Started/quickstart.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'User Guide': 'Getting-Started/user-guide.md'}, {'KubeStellar - The Infomercial': 'Getting-Started/infomercial.md'}]}, {'Contributing': [{'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}, {'Operations': [{'Code Management': 'Contribution guidelines/operations/code-management.md'}, {'Release Management': 'Contribution guidelines/operations/release-management.md'}, {'Document Management': 'Contribution guidelines/operations/document-management.md'}]}]}, {'Coding Milestones': [{'PoC2023q1': [{'Details': 'Coding Milestones/PoC2023q1/outline.md'}, {'Invitation to Contribute': 'Coding Milestones/PoC2023q1/coding-milestone-invite-q1.md'}, {'KubeStellar-Syncer': 'Coding Milestones/PoC2023q1/kubestellar-syncer.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'KubeStellar Processes': [{'KubeStellar Where Resolver': 'Coding Milestones/PoC2023q1/where-resolver.md'}, {'KubeStellar Mailbox Controller': 'Coding Milestones/PoC2023q1/mailbox-controller.md'}, {'KubeStellar Placement Translator': 'Coding Milestones/PoC2023q1/placement-translator.md'}]}, {'Use Cases Affecting the Roadmap for PoC2023q1': 'Coding Milestones/PoC2023q1/roadmap-uses.md'}, {'Environments': [{'Overview': 'Coding Milestones/PoC2023q1/environments/_index.md'}, {'Cloud Environment': 'Coding Milestones/PoC2023q1/environments/cloud-env.md'}, {'Developer Environment': 'Coding Milestones/PoC2023q1/environments/dev-env.md'}]}, {'Reference': [{'Commands': 'Coding Milestones/PoC2023q1/commands.md'}]}]}, {'PoC2023q4': [{'Details': 'Coding Milestones/PoC2023q4/outline.md'}]}]}, {'Community': [{'Get Involved': 'Community/_index.md'}, {'Contributor Guidelines': 'https://docs.kubestellar.io/main/Contribution%20guidelines/CONTRIBUTING/'}, {'Contact Us': [{'Mailing List': 'https://kubestellar.io/join_us'}, {'Community Meeting Agenda (join mailing list first)': 'https://kubestellar.io/agenda'}, {'Slack': 'https://kubestellar.io/slack'}, {'Medium Blog': 'https://kubestellar.io/blog'}, {'YouTube Channel': 'https://kubestellar.io/tv'}, {'LinkedIn': 'https://kubestellar.io/linkedin'}, {'Reddit': 'https://www.reddit.com/r/kubestellar/'}, {'Google Drive': 'https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS'}]}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}] pages NoneType None site_url str 'https://docs.kubestellar.io/main' site_description NoneType None site_author NoneType None theme Theme Theme(name='material', dirs=['/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/kubestellar/kubestellar/docs/overrides', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png') docs_dir str '/home/runner/work/kubestellar/kubestellar/docs/content' site_dir str '/home/runner/work/kubestellar/kubestellar/docs/generated' copyright NoneType None google_analytics NoneType None dev_addr _IpAddressValue _IpAddressValue(host='127.0.0.1', port=8000) use_directory_urls bool True repo_url str 'https://github.com/kubestellar/kubestellar' repo_name str 'GitHub' edit_uri_template NoneType None edit_uri str 'edit/main/docs/content/' extra_css list ['stylesheets/kubestellar.css'] extra_javascript list ['js/open_in_new_tab.js'] extra_templates list [] markdown_extensions list ['toc', 'tables', 'fenced_code', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'] mdx_configs dict toc [dict], pymdownx.highlight [dict], pymdownx.tabbed [dict] strict bool False remote_branch str 'gh-pages' remote_name str 'origin' extra LegacyConfig {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}} plugins PluginCollection mike [MikePlugin], awesome-pages [AwesomePagesPlugin], material/search [SearchPlugin], open-in-new-tab [OpenInNewTabPlugin], include-markdown [IncludeMarkdownPlugin], macros [MacrosPlugin], i18n [I18n] hooks dict watch list ['/home/runner/work/kubestellar/kubestellar/docs/mkdocs.yml', '/home/runner/work/kubestellar/kubestellar/docs/content', '/home/runner/work/kubestellar/kubestellar/docs/overrides'] repo_short_name str 'kubestellar/kubestellar' repo_default_file_path str 'kubestellar' docs_url str 'https://docs.kubestellar.io' repo_raw_url str 'https://raw.githubusercontent.com/kubestellar/kubestellar' ks_branch str 'main' ks_tag str 'v0.9.0' ks_next_branch str 'release-0.10' ks_next_tag str 'v0.10.0' ks_port_num str '1119'"},{"location":"Contribution%20guidelines/operations/all-macros/#macros","title":"Macros","text":"<p>These macros have been defined programmatically for this environment (module or pluglets). </p> Variable Type Content context function (obj, e) <p>Default mkdocs_macro List the defined variables</p> macros_info function () <p>Test/debug function:         list useful documentation on the mkdocs_macro environment.</p> now function () <p>Get the current time (returns a datetime object).          Used alone, it provides a timestamp.         To get the year use <code>now().year</code>, for the month number          <code>now().month</code>, etc.</p> fix_url function (url, r) <p>If url is relative, fix it so that it points to the docs diretory.     This is necessary because relative links in markdown must be adapted     in html ('img/foo.png' =&gt; '../img/img.png').</p> section_items function (page, nav, config, children, siblings, child) <p>Returns a list of all pages that are siblings to page.</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#git-information","title":"Git Information","text":"<p>Information available on the last commit and the git repository containing the documentation project:</p> <p>e.g. <code>{{ git.message }}</code></p> Variable Type Content status bool True date datetime datetime.datetime(2023, 10, 23, 9, 38, 47, tzinfo=tzoffset(None, -14400)) short_commit str 'dacb098f' commit str 'dacb098f6cb7694932017b00dc4fef51ed7a1cb2' tag str '' author str 'Andy Anderson' author_email str 'andy@clubanderson.com' committer str 'GitHub' committer_email str 'noreply@github.com' date_ISO str 'Mon Oct 23 09:38:47 2023 -0400' message str 'Update quickstart-1-install-and-run-kubestellar.md' raw str 'commit dacb098f6cb7694932017b00dc4fef51ed7a1cb2\\nAuthor: Andy Anderson \\nDate:   Mon Oct 23 09:38:47 2023 -0400\\n\\n    Update quickstart-1-install-and-run-kubestellar.md' root_dir str '/home/runner/work/kubestellar/kubestellar'"},{"location":"Contribution%20guidelines/operations/all-macros/#page-attributes","title":"Page Attributes","text":"<p>Provided by MkDocs. These attributes change for every page (the attributes shown are for this page).</p> <p>e.g. <code>{{ page.title }}</code></p> <p>See also the MkDocs documentation on the page object.</p> Variable Type Content file I18nFile I18nFile(src_path='Contribution guidelines/operations/all-macros.md', abs_src_path='/home/runner/work/kubestellar/kubestellar/docs/content/Contribution guidelines/operations/all-macros.md', dest_path='Contribution guidelines/operations/all-macros/index.html', abs_dest_path='/home/runner/work/kubestellar/kubestellar/docs/generated/Contribution guidelines/operations/all-macros/index.html', name='all-macros', locale_suffix='None', dest_language='', dest_name='all-macros.md', url='Contribution%20guidelines/operations/all-macros/') title str 'All macros' parent NoneType None children NoneType None previous_page NoneType None next_page NoneType None _Page__active bool False update_date str '2023-10-23' canonical_url str 'https://docs.kubestellar.io/main/Contribution%20guidelines/operations/all-macros/' abs_url str '/main/Contribution%20guidelines/operations/all-macros/' edit_url str 'https://github.com/kubestellar/kubestellar/edit/main/docs/content/Contribution guidelines/operations/all-macros.md' markdown str 'All variables supported by this documentation implementation:\\n\\n{{ macros_info() }}' content NoneType None toc list [] meta dict <p>To have all titles of all pages, use:</p> <pre><code>{% for page in navigation.pages %}\n- {{ page.title }}\n{% endfor %}\n</code></pre>"},{"location":"Contribution%20guidelines/operations/all-macros/#plugin-filters","title":"Plugin Filters","text":"<p>These filters are provided as a standard by the macros plugin.</p> Variable Type Content pretty function (var_list, rows, header, e) <p>Default mkdocs_macro Prettify a dictionary or object          (used for environment documentation, or debugging).</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#builtin-jinja2-filters","title":"Builtin Jinja2 Filters","text":"<p>These filters are provided by Jinja2 as a standard.</p> <p>See also the Jinja2 documentation on builtin filters).</p> Variable Type Content abs builtin_function_or_method <p>Return the absolute value of the argument.</p> attr function (environment, obj, name, value) <p>Get an attribute of an object.  <code>foo|attr(\"bar\")</code> works like     <code>foo.bar</code> just that always an attribute is returned and items are not     looked up.</p> batch function (value, linecount, fill_with, tmp, item) <p>A filter that batches items. It works pretty much like <code>slice</code>     just the other way round. It returns a list of lists with the     given number of items. If you provide a second parameter this     is used to fill up missing items. See this example.</p> capitalize function (s) <p>Capitalize a value. The first character will be uppercase, all others     lowercase.</p> center function (value, width) <p>Centers the value in a field of a given width.</p> count builtin_function_or_method <p>Return the number of items in a container.</p> d function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> default function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> dictsort function (value, case_sensitive, by, reverse, sort_func) <p>Sort a dict and yield (key, value) pairs. Python dicts may not     be in the order you want to display them in, so sort them first.</p> e builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> escape builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> filesizeformat function (value, binary, bytes, base, prefixes, i, prefix, unit) <p>Format the value like a 'human-readable' file size (i.e. 13 kB,     4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,     Giga, etc.), if the second parameter is set to <code>True</code> the binary     prefixes are used (Mebi, Gibi).</p> first function (args, kwargs, b) <p>Return the first item of a sequence.</p> float function (value, default) <p>Convert the value into a floating point number. If the     conversion doesn't work it will return <code>0.0</code>. You can     override this default using the first parameter.</p> forceescape function (value) <p>Enforce HTML escaping.  This will probably double escape variables.</p> format function (value, args, kwargs) <p>Apply the given values to a <code>printf-style</code>_ format string, like     <code>string % values</code>.</p> groupby function (args, kwargs, b) <p>Group a sequence of objects by an attribute using Python's     :func:<code>itertools.groupby</code>. The attribute can use dot notation for     nested access, like <code>\"address.city\"</code>. Unlike Python's <code>groupby</code>,     the values are sorted first so only one group is returned for each     unique value.</p> indent function (s, width, first, blank, newline, rv, lines) <p>Return a copy of the string with each line indented by 4 spaces. The     first line and blank lines are not indented by default.</p> int function (value, default, base) <p>Convert the value into an integer. If the     conversion doesn't work it will return <code>0</code>. You can     override this default using the first parameter. You     can also override the default base (10) in the second     parameter, which handles input with prefixes such as     0b, 0o and 0x for bases 2, 8 and 16 respectively.     The base is ignored for decimal numbers and non-string values.</p> join function (args, kwargs, b) <p>Return a string which is the concatenation of the strings in the     sequence. The separator between elements is an empty string per     default, you can define it with the optional parameter.</p> last function (environment, seq) <p>Return the last item of a sequence.</p> length builtin_function_or_method <p>Return the number of items in a container.</p> list function (args, kwargs, b) <p>Convert the value into a list.  If it was a string the returned list     will be a list of characters.</p> lower function (s) <p>Convert a value to lowercase.</p> items function (value) <p>Return an iterator over the <code>(key, value)</code> items of a mapping.</p> map function (args, kwargs, b) <p>Applies a filter on a sequence of objects or looks up an attribute.     This is useful when dealing with lists of objects but you are really     only interested in a certain value of it.</p> min function (environment, value, case_sensitive, attribute) <p>Return the smallest item from the sequence.</p> max function (environment, value, case_sensitive, attribute) <p>Return the largest item from the sequence.</p> pprint function (value) <p>Pretty print a variable. Useful for debugging.</p> random function (context, seq) <p>Return a random item from the sequence.</p> reject function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and rejecting the objects with the test succeeding.</p> rejectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and rejecting the objects with the test     succeeding.</p> replace function (eval_ctx, s, old, new, count) <p>Return a copy of the value with all occurrences of a substring     replaced with a new one. The first argument is the substring     that should be replaced, the second is the replacement string.     If the optional third argument <code>count</code> is given, only the first     <code>count</code> occurrences are replaced.</p> reverse function (value, rv, e) <p>Reverse the object or return an iterator that iterates over it the other     way round.</p> round function (value, precision, method, func) <p>Round the number to a given precision. The first     parameter specifies the precision (default is <code>0</code>), the     second the rounding method.</p> safe function (value) <p>Mark the value as safe which means that in an environment with automatic     escaping enabled this variable will not be escaped.</p> select function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and only selecting the objects with the test succeeding.</p> selectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and only selecting the objects with the     test succeeding.</p> slice function (args, kwargs, b) <p>Slice an iterator and return a list of lists containing     those items. Useful if you want to create a div containing     three ul tags that represent columns.</p> sort function (environment, value, reverse, case_sensitive, attribute, key_func) <p>Sort an iterable using Python's :func:<code>sorted</code>.</p> string builtin_function_or_method <p>Convert an object to a string if it isn't already. This preserves a :class:<code>Markup</code> string rather than converting it back to a basic string, so it will still be marked as safe and won't be escaped again.</p> striptags function (value) <p>Strip SGML/XML tags and replace adjacent whitespace by one space.</p> sum function (args, kwargs, b) <p>Returns the sum of a sequence of numbers plus the value of parameter     'start' (which defaults to 0).  When the sequence is empty it returns     start.</p> title function (s) <p>Return a titlecased version of the value. I.e. words will start with     uppercase letters, all remaining characters are lowercase.</p> trim function (value, chars) <p>Strip leading and trailing characters, by default whitespace.</p> truncate function (env, s, length, killwords, end, leeway, result) <p>Return a truncated copy of the string. The length is specified     with the first parameter which defaults to <code>255</code>. If the second     parameter is <code>true</code> the filter will cut the text at length. Otherwise     it will discard the last word. If the text was in fact     truncated it will append an ellipsis sign (<code>\"...\"</code>). If you want a     different ellipsis sign than <code>\"...\"</code> you can specify it using the     third parameter. Strings that only exceed the length by the tolerance     margin given in the fourth parameter will not be truncated.</p> unique function (environment, value, case_sensitive, attribute, getter, seen, item, key) <p>Returns a list of unique items from the given iterable.</p> upper function (s) <p>Convert a value to uppercase.</p> urlencode function (value, items) <p>Quote data for use in a URL path or query using UTF-8.</p> urlize function (eval_ctx, value, trim_url_limit, nofollow, target, rel, extra_schemes, policies, rel_parts, scheme, rv) <p>Convert URLs in text into clickable links.</p> wordcount function (s) <p>Count the words in that string.</p> wordwrap function (environment, s, width, break_long_words, wrapstring, break_on_hyphens) <p>Wrap a string to the given width. Existing newlines are treated     as paragraphs to be wrapped separately.</p> xmlattr function (eval_ctx, d, autospace, rv) <p>Create an SGML/XML attribute string based on the items in a dict.     All values that are neither <code>none</code> nor <code>undefined</code> are automatically     escaped.</p> tojson function (eval_ctx, value, indent, policies, dumps, kwargs) <p>Serialize an object to a string of JSON, and mark it safe to     render in HTML. This filter is only for use in HTML documents.</p>"},{"location":"Contribution%20guidelines/operations/code-management/","title":"Code Management","text":""},{"location":"Contribution%20guidelines/operations/document-management/","title":"Document Management","text":""},{"location":"Contribution%20guidelines/operations/document-management/#overview","title":"Overview","text":"<p>Our documentation is powered by Material for MkDocs with some  additional plugins and tools:</p> <ul> <li>awesome-pages plugin</li> <li>macros plugin</li> <li>mike for multiple version support</li> </ul> <p>We have support in place for multiple languages (i18n), although we currently only have documentation in English. If  you're interested in contributing translations, please let us know!</p>"},{"location":"Contribution%20guidelines/operations/document-management/#serving-up-documents-locally","title":"Serving up documents locally","text":"<p>You can view and modify our documentation in your local development environment.  Simply checkout one of our branches.</p> <pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar/docs\ngit checkout main\n</code></pre> <p>You can view and modify our documentation in the branch you have checked out by using <code>mkdocs serve</code> from mkdocs.  We have a Python requirements file in <code>requirements.txt</code>, and a Makefile target that builds a Python virtual environment and installs the requirements there.  You can either install those requirements into your global Python environment or use the Makefile target.  To install those requirements into your global Python environment, do the following usual thing.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Alternatively, use the following commands to use the Makefile target to construct an adequate virtual environment and enter it.</p> <pre><code>( cd ..; make venv )\n. venv/bin/activate\n</code></pre> <p>Then, using your chosen environment with the requirements installed, build and serve the documents with the following command.</p> <p><pre><code>mkdocs serve\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p> <p>Another way to view (not modify - this method reflects what has been deployed to the <code>gh-pages</code> branch of our repo) all branches/versions of our documentation locally using 'mike' mike for mkdocs:</p> <p><pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\ncd docs\nmike set-default main\ncd ..\nmake serve-docs\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p>"},{"location":"Contribution%20guidelines/operations/document-management/#file-structure","title":"File structure","text":"<p>All documentation-related items live in <code>docs</code> (with the small exception of various <code>make</code> targets and some helper  scripts in <code>hack</code>).</p> <p>The structure of <code>docs</code> is as follows:</p> Path Description config/$language/mkdocs.yml Language-specific <code>mkdocs</code> configuration. content/$language Language-specific website content. generated/branch All generated content for all languages for the current version. generated/branch/$language Generated content for a single language. Never added to git. generated/branch/index.html Minimal index for the current version that redirects to the default language (en) overrides Global (not language-specific) content. Dockerfile Builds the kubestellar-docs image containing mkdocs + associated tooling. mkdocs.yml Minimal <code>mkdocs</code> configuration for <code>mike</code> for multi-version support. requirements.txt List of Python modules used to build the site."},{"location":"Contribution%20guidelines/operations/document-management/#global-variables","title":"Global Variables","text":"<p>There are many global variables defined in the docs/mkdocs.yml.  The following are some very common variables you are encouraged to use in our documentation.  Use of these variables/macros allows our documentation to have github branch context and take advantage of our evolution without breaking</p> <pre><code>- site_name: KubeStellar\n- repo_url: https://github.com/kubestellar/kubestellar\n- site_url: https://docs.kubestellar.io/main\n- repo_default_file_path: kubestellar\n- repo_short_name: kubestellar/kubestellar\n- docs_url: https://docs.kubestellar.io\n- repo_raw_url: https://raw.githubusercontent.com/kubestellar/kubestellar\n- edit_uri: edit/main/docs/content/\n- ks_branch: main\n- ks_tag: v0.9.0\n</code></pre> <p>to use a variables/macro in your documentation reference like this:</p> <p>{{ config.&lt;var_name&gt; }}</p> <p>and in context that can look something like this:</p> <p>bash &lt;(curl -s {{ config.repo_raw_url }}/{{ config.ks_branch }}/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version {{ config.ks_tag }}</p> <p>note:  \u00a0\u00a0\u00a0\u00a0- A more extensive and detailed list is located at mkdocs information  \u00a0\u00a0\u00a0\u00a0- We also check for broken links as part of our PR pipeline.  For more information check out our Broken Links Crawler</p>"},{"location":"Contribution%20guidelines/operations/document-management/#including-external-markdown","title":"Including external markdown","text":"<p>We make extensive use of 'include-markdown' to help us keep our documentation modular and up-to-date.  To use 'include-markdown' you must add a block in your document that refers to a block in your external document content:</p> <p>In your original markdown document, add a block that refers to the external markdown you want to include: </p> <p>In the document you want to include, add the start and end tags you configured in the include-markdown block in your original document: </p> <p>for more information on the 'include-markdown' plugin for mkdocs look here</p>"},{"location":"Contribution%20guidelines/operations/document-management/#supported-aliases-for-our-documentation","title":"Supported aliases for our documentation","text":"<p>We currently support 3 aliases for our documentation:</p> <pre><code>- from the release major.minor branch:\n    - [https://docs.kubestellar.io/stable](../../../https://docs.kubestellar.io/stable)\n- from the main branch:\n    - [https://docs.kubestellar.io/unstable](../../../https://docs.kubestellar.io/unstable)\n    - [https://docs.kubestellar.io/latest](../../../https://docs.kubestellar.io/latest)\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#shortcut-urls","title":"Shortcut URLs","text":"<p>We have a few shortcut urls that come in handy when referring others to our project:</p> <p>note: You need to join our mailing list first to get access to some of the links that follow (https://docs.kubestellar.io/joinus)</p> <ul> <li>https://kubestellar.io/agenda - our community meeting agenda google doc</li> <li>https://kubestellar.io/blog - our medium reading list</li> <li>https://kubestellar.io/code - our current GitHub repo (wherever that is)</li> <li>https://kubestellar.io/community - our stable docs community page</li> <li>https://kubestellar.io/drive - our google drive</li> <li>https://kubestellar.io/joinus - our dev mailing list where you join and get our invites</li> <li>https://kubestellar.io/join_us - also, our dev mailing list</li> <li>https://kubestellar.io/linkedin - our linkedin filter (soon, our page)</li> <li>https://kubestellar.io/tv - our youtube channel</li> <li>https://kubestellar.io/youtube - also, our youtube channel</li> <li>https://kubestellar.io/infomercial - our infomercial that premieres on June 12th at 9am</li> </ul> <p>and.. the very important\u2026 - https://kubestellar.io/quickstart - our 'stable' quickstart</p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblocks","title":"Codeblocks","text":"<p>mkdocs has some very helpful ways to include blocks of code in a style that makes it clear to our readers that console interaction is necessary in the documentation.  There are options to include a plain codeblock (```), shell (shell), console (console - no used in our documentation), language or format-specific (yaml, etc.), and others.  For more detailed information, checkout the mkdocs information on codeblocks.</p> <p>Here are some examples of how we use codeblocks:</p> <ul> <li> <p>For a codeblock that can be 'tested' (and seen by the reader) as part of our CI, use the <code>shell</code> block: codeblock: <pre><code>```shell\nmkdocs serve\n```\n</code></pre> as seen by reader: <pre><code>mkdocs serve\n</code></pre> </p> </li> <li> <p>For a codeblock that should be 'tested', BUT not seen by the reader, use the <code>.bash</code> with the plain codeblock, and the '.hide-me' style (great for hiding a sleep command that user does not need to run, but CI does): codeblock: <pre><code>``` {.bash .hide-me}\nsleep 10\n```\n</code></pre> as seen by reader: <pre><code>\n</code></pre> </p> </li> <li> <p>For a codeblock that should not be 'tested' as part of our CI, use the <code>.bash</code> with the plain codeblock, and without the '.hide-me' style: codeblock: <pre><code>``` {.bash}\nmkdocs server\n```\n</code></pre> as seen by reader: <pre><code>mkdocs server\n</code></pre> <li> <p>For a codeblock that should not be 'tested', be seen by the reader, and not include a 'copy' icon (great for output-only instances), use the <code>.bash</code> codeblock without the '.no-copy' style: codeblock: <pre><code>``` {.bash .no-copy}\nI0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n```\n</code></pre> as seen by reader: <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n</code></pre> <li> <p>For language-specific highlighting (yaml, etc.), use the yaml codeblock codeblock: <pre><code>```yaml\nnav:\n  - Home: index.md\n  - QuickStart: Getting-Started/quickstart.md\n  - Contributing: \n      - Guidelines: Contribution guidelines/CONTRIBUTING.md\n```\n</code></pre> as seen by reader: <pre><code>nav:\n- Home: index.md\n- QuickStart: Getting-Started/quickstart.md\n- Contributing: - Guidelines: Contribution guidelines/CONTRIBUTING.md\n</code></pre> </p> </li> <li> <p>For a codeblock that has a title, and will not be tested, use the 'title' parameter in conjunction with the plain codeblock (greater for showing or prescribing contents of files): codeblock: <pre><code>``` title=\"testing.sh\"\n#!/bin/sh\necho hello KubeStellar\n```\n</code></pre> as seen by reader: testing.sh<pre><code>#!/bin/sh\necho hello KubeStellar\n</code></pre> </p> </li> <p>(other variations are possible, PR an update to the kubestellar.css file and, once approved, use the style on the plain codeblock in your documentation.)</p>"},{"location":"Contribution%20guidelines/operations/document-management/#testingrunning-docs","title":"Testing/Running Docs","text":"<p>How do we ensure that our documented examples work?  Simple, we 'execute' our documentation in our CI.  We built automation called 'docs-ecutable' which can be invoked to test any markdown (.md) file in our repository. You could use it in your project as well - afterall it is opensource.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-way-it-works","title":"The way it works:","text":"<ul> <li>create your .md file as you normally would</li> <li>add codeblocks that can be tested, tested but hidden, or not tested at all:<ul> <li>use 'shell' to indicate code you want to be tested</li> <li>use '.bash' with the plain codeblock, and the '.hide-md' style for code you want to be tested, but hidden from the reader (some like this, but its not cool if you want others to run your instructions without hiccups)</li> <li>use plain codeblock (```) if you want to show sample output that is not to be tested</li> </ul> </li> <li>you can use 'include-markdown' blocks, and they will also be executed (or not), depending on the codeblock style you use in the included markdown files.</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-github-workflow","title":"The GitHub Workflow:","text":"<ul> <li>One example of the GitHub Workflow is located in our kubestellar/kubestellar at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-where-resolver.yml</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-secret-sauce","title":"The secret sauce:","text":"<ul> <li>The code that makes all this possible is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/docs-ecutable.sh<ul> <li>This code parses the .md file you give it to pull out all the 'shell' and '.bash .hide-me' blocks</li> <li>The code is smart enough to traverse the include-markdown blocks and include the 'shell' and '.bash .hide-me' blocks in them</li> <li>It then creates a file called 'generate_script.sh' which is then run at the end of the docs-ecutable execution.</li> </ul> </li> </ul> <p>All of this is invoke in a target in our Makefile <pre><code>.PHONY: docs-ecutable\ndocs-ecutable: MANIFEST=$(MANIFEST) docs/scripts/docs-ecutable.sh\n</code></pre></p> <p>You give the path from that follows the 'https://github.com/kubestellar/kubestellar/docs' path, and name of the .md file you want to 'execute'/'test' as the value for the MANIFEST variable:</p> How to 'make' our docs-ecutable target<pre><code>make MANIFEST=\"'docs/content/Getting-Started/quickstart.md'\" docs-ecutable\n</code></pre> <p>note: there are single and double-quotes used here to avoid issues with 'spaces' used in files names or directories.  Use the single and double-quotes as specified in the quickstart example here.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#important-files-in-our-gh-pages-branch","title":"Important files in our gh-pages branch","text":""},{"location":"Contribution%20guidelines/operations/document-management/#indexhtml-and-homehtml","title":"index.html and home.html","text":"<p>In the 'gh-pages' branch there are two(2) important files that redirect the github docs url to our KubeStellar doc site hosted with GoDaddy.com.</p> <p>https://github.com/kubestellar/kubestellar/blob/gh-pages/home.html https://github.com/kubestellar/kubestellar/blob/gh-pages/index.html</p> <p>both files have content similar to: index.html and home.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;KubeStellar&lt;/title&gt;\n&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" &gt;\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://docs.kubestellar.io/stable\" /&gt;\n&lt;/head&gt;\n</code></pre></p> <p>Do not remove these files!</p>"},{"location":"Contribution%20guidelines/operations/document-management/#cname","title":"CNAME","text":"<p>The CNAME file has to be in the gh-pages root to allow github to recognize the url tls cert served by our hosting provider.  Do not remove this file!</p> <p>the CNAME file must have the following content in it: CNAME<pre><code>docs.kubestellar.io\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/document-management/#versionsjson","title":"versions.json","text":"<p>The versions.json file contains the version and alias information required by 'mike' to properly serve our doc site.  This file is maintained by the 'mike' environment and should not be edited by hand.</p> <pre><code>[{\"version\": \"release-0.2\", \"title\": \"release-0.2\", \"aliases\": [\"stable\"]}, {\"version\": \"main\", \"title\": \"main\", \"aliases\": [\"latest\", \"unstable\"]}]\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#in-case-of-emergency","title":"In case of emergency","text":"<p>If you find yourself in a jam and the pages are not showing up at kubestellar.io or docs.kubestellar.io, check the following 1) Is the index.html, home.html, CNAME, and versions.json file in the gh-pages branch alongside the folders for the compiled documents?  If not, then recreate those files as indicated above (except for versions.json which is programmatically created by 'mike'). 2) Is GitHub settings for 'Pages' for the domain pointing at the https://docs.kubestellar.io url?  If not, paste it in and check off 'enforce https'.  This can happen if the CNAME file goes missing from the gh-pages branch.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#how-to-recreate-the-gh-pages-branch","title":"How to recreate the gh-pages branch","text":"<p>To recreate the gh-pages branch, do the following: - checkout the gh-pages branch to your local system <pre><code>git clone -b gh-pages https://github.com/kubestellar/kubestellar KubeStellar\ncd KubeStellar\n</code></pre> - delete all files in the branch and push it to GitHub <pre><code>rm -rf *\ngit add; git commit -m \"removing all gh-pages files\"; git push -u origin gh-pages\n</code></pre> -- switch to the 'main' branch <pre><code>git checkout main\ngit pull\n</code></pre> - switch to /docs and run 'mike deploy' for the main branch for alias 'unstable' and 'latest' <pre><code>cd docs\nmike deploy --push --rebase --update-aliases main unstable\nmike deploy --push --rebase --update-aliases main latest\n</code></pre> - switch to the 'release' branch and 'mike deploy' for the release branch for alias 'stable' (your release name will vary) <pre><code>git checkout release-0.2\ngit pull\nmike deploy --push --rebase --update-aliases release-0.2 stable\n</code></pre> - switch back to the gh-pages branch and recreate the home.html, index.html, and CNAME files as needed (make sure you back out of the docs path first before switching to gh-pages because that path does not exist in that branch) <pre><code>cd ..\ngit checkout gh-pages\ngit pull\nvi index.html\nvi home.html\nvi CNAME\n</code></pre> - push the new files into gh-pages <pre><code>git add .;git commit -m \"add index, home, and CNAME files\";git push -u origin gh-pages\n</code></pre> - go into the GitHub UI and go to the settings for the project and click on 'Pages' to add https://docs.kubestellar.io as the domain and check the box to enforce https.</p> <ul> <li>if the above did not work, then you might have an issue with the GoDaddy domain (expired, files missing, etc.)</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#publishing-workflow","title":"Publishing Workflow","text":"<p>All documentation building and publishing is done using GitHub Actions in docs-gen-and-push.yaml. The overall sequence is:</p>"},{"location":"Contribution%20guidelines/operations/release-management/","title":"Release Management","text":""},{"location":"Contribution%20guidelines/operations/release-management/#publishing-a-new-kubestellar-release","title":"Publishing a new KubeStellar release","text":""},{"location":"Contribution%20guidelines/operations/release-management/#prerequisite-make-sure-you-have-a-gpg-signing-key","title":"Prerequisite - make sure you have a GPG signing key","text":"<ol> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key</li> </ol>"},{"location":"Contribution%20guidelines/operations/release-management/#create-the-tags","title":"Create the tags","text":""},{"location":"Contribution%20guidelines/operations/release-management/#note","title":"Note:","text":"<p>You currently need write access to the https://github.com/kubestellar/kubestellar repository to perform these tasks.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#checkout-the-main-branch","title":"Checkout the main branch","text":"<pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-kubectl-kubestellar-prep_for_syncer-file-with-a-reference-to-the-new-version-of-the-kubestellar-syncer-version","title":"Update the 'kubectl-kubestellar-prep_for_syncer' file with a reference to the new version of the kubestellar syncer version","text":"<pre><code>vi outer-scripts/kubectl-kubestellar-prep_for_syncer\n</code></pre> <p>change the version in the following line: <pre><code>syncer_image=\"quay.io/kubestellar/syncer:v0.10.0\"\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-version-file","title":"Update the VERSION file","text":"<p>The VERSION file points to the 'latest' and 'stable' release tags associated with the most recent release (latest) and the most stable release (stable).  Update the 'stable' and 'latest tags accordingly</p> <pre><code>vi VERSION\n</code></pre> <p>before: VERSION<pre><code>...\nstable=v0.9.0\nlatest=v0.9.0\n...\n</code></pre></p> <p>after: VERSION<pre><code>...\nstable=v0.9.0\nlatest=v0.10.0\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#push-the-main-branch","title":"Push the main branch","text":"<pre><code>git add .\ngit commit -m \"updates to main to support new release\"\ngit push -u origin main\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-majorminor-branch","title":"Create a release-major.minor branch","text":"<p>To create a release branch, identify the current 'release' branches' name (e.g. release-0.10).  Increment the  or  segment as part of the 'release' branches' name.  For instance, the 'release' branch is 'main', you might name the new release branch 'release-0.10'. <pre><code>git checkout -b release-0.10\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-mkdocsyml-file","title":"Update the mkdocs.yml file","text":"<p>The mkdocs.yml file points to the branch and tag associated with the branch you have checked out.  Update the ks_branch and ks_tag key/value pairs at the top of the file</p> <pre><code>vi docs/mkdocs.yml\n</code></pre> <p>before: mkdocs.yml<pre><code>...\nedit_uri: edit/main/docs/content\nks_branch: 'main'\nks_tag: 'v0.9.0'\nks_next_branch: 'release-0.10'\nks_next_tag: 'v0.10.0'\n...\n</code></pre></p> <p>after: mkdocs.yml<pre><code>...\nedit_uri: edit/release-0.10/docs/content\nks_branch: 'release-0.10'\nks_tag: 'v0.10.0'\nks_next_branch:    # put the branch name of the next numerical branch that will come in the future\nks_next_tag:       # put the tag name of the next numerical tag that will come in the future\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#remove-the-current-stable-alias-using-mike-danger","title":"Remove the current 'stable' alias using 'mike' (DANGER!)","text":"<p>Be careful, this will cause links to the 'stable' docs, which is the default for our community, to become unavailable.  For now, point 'stable' at 'main' <pre><code>cd docs\nmike delete stable # remove the 'stable' alias from the current 'main' branches' doc set\nmike deploy --push --rebase --update-aliases main stable # this generates the 'main' branches' docs set and points 'stable' at it temporarily\ncd ..\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#push-the-new-release-branch","title":"Push the new release branch","text":"<pre><code>git add .\ngit commit -m \"new release version release-0.10\"\ngit push -u origin release-0.10 # replace &lt;major&gt;.&lt;minor&gt; with your incremented &lt;major&gt;.&lt;minor&gt; pair\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-stable-alias-using-mike","title":"Update the 'stable' alias using 'mike'","text":"<pre><code>cd docs\nmike delete stable # remove the 'stable' alias from the 'main' branches' doc set\ngit pull\nmike deploy --push --rebase --update-aliases release-0.10 stable  # this generates the new 'release-0.10' branches' doc set and points 'stable' at it\ncd ..\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#test-your-doc-site","title":"Test your doc site","text":"<p>Open a Chrome Incognito browser to https://docs.kubestellar.io and look for the version drop down to be updated to the new release you just pushed with 'git' and deployed with 'mike'</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-tagged-release","title":"Create a tagged release","text":"<p>View the existing tags you have for the repo</p> <pre><code>git fetch --tags\ngit tag\n</code></pre> <p>create a tag that follows ...  For this example we will increment tag 'v0.9.0' to 'v0.10.0' <pre><code>TAG=v0.10.0\nREF=release-0.10\ngit tag --sign --message \"$TAG\" \"$TAG\" \"$REF\"\ngit push origin --tags\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-build","title":"Create a build","text":"<pre><code>./hack/make-release-full.sh v0.10.0\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-kubestellar-container-image-just-build-and-uploaded-to-quayio","title":"Update the kubestellar container image just build and uploaded to quay.io","text":"<p>Head up to quay.io and look for the image of KubeStellar container just uploaded. Make this image 'stable' so that helm and other install methods pickup this image.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-in-gh-ui","title":"Create a release in GH UI","text":"<ul> <li>Navigate to the KubeStellar GitHub Source Repository Releases section at https://github.com/kubestellar/kubestellar/releases</li> <li> <p>Click 'Draft a new release' and select the tag ('v0.10.0')</p> <ul> <li>Select the release branch you created above (release-0.10)</li> <li>Add a release title (v0.10.0)</li> <li>Add some release notes ('generate release notes' if you like)</li> <li>select 'pre-release' as a the first step.  Once validated the release is working properly, come back and mark as 'release'</li> <li>Attach the binaries that were created in the 'make-release-full' process above<ul> <li>You add the KubeStellar-specific '*.tar.gz' and the 'checksum256.txt' files</li> <li>GitHub will automatically add the 'Source Code (zip)' and 'Source Code (tar.gz)'</li> </ul> </li> </ul> <p></p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/release-management/#check-that-gh-workflows-for-docs-are-working","title":"Check that GH Workflows for docs are working","text":"<p>Check to make sure the GitHub workflows for doc generation, doc push, and broken links is working and passing https://github.com/kubestellar/kubestellar/actions/workflows/docs-gen-and-push.yml https://github.com/kubestellar/kubestellar/actions/workflows/broken-links-crawler.yml</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-an-email-addressed-to-kubestellar-devgooglegroupscom-and-kubestellar-usersgooglegroupscom","title":"Create an email addressed to kubestellar-dev@googlegroups.com and kubestellar-users@googlegroups.com","text":"<pre><code>Subject: KubeStellar release v0.10.0\n\nBody:\n\nDear KubeStellar Community,\n    Release v0.10.0 is now available at https://github.com/kubestellar/kubestellar/releases/tag/v0.10.0\n\nWhat's Changed\n\n\ud83d\udc1b Fix display of initial spaces after deploy in kube by @MikeSpreitzer in #1143\n\u2728 Generalize bootstrap wrt namespace in hosting cluster by @MikeSpreitzer in #1144\n\u2728 Generalize bootstrap wrt namespace in hosting cluster by @MikeSpreitzer in #1145\n\u2728 Switch to use k8s code generators by @ezrasilvera in #1139\n\u2728 Bump actions/checkout from 4.1.0 to 4.1.1 by @dependabot in #1151\n\ud83c\udf31 Align default core image ref in chart with coming release by @MikeSpreitzer in #1146\n\ud83d\udcd6Update dev-env.md by @francostellari in #1157\n\ud83d\udcd6Update Chart.yaml appVersion by @francostellari in #1158\n\ud83d\udc1b Use realpath to see through symlinks by @MikeSpreitzer in #1156\n\u2728 Increase kind version to v0.20 for ubuntu by @fab7 in #1155\n\ud83d\udcd6 Document syncer removal by @MikeSpreitzer in #1164\n\ud83c\udf31 Rename urmeta to ksmeta by @MikeSpreitzer in #1166\n\u2728 Make get-internal-kubeconfig fetch mid-level kubeconfig by @MikeSpreitzer in #1161\n\u2728 Make ensure/remove wmw insensitive to current workspace by @MikeSpreitzer in #1160\nNew Contributors\n\n@fab7 made their first contribution in #1155\nFull Changelog: v0.8.0\u2026v0.9.0\n\nThank you for your continued support,\n\nAndy\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#post-the-same-message-in-the-kubestellar-slack-channel","title":"Post the same message in the #kubestellar Slack channel","text":""},{"location":"Contribution%20guidelines/security/security/","title":"Policy","text":""},{"location":"Contribution%20guidelines/security/security/#security-announcements","title":"Security Announcements","text":"<p>Join the kubestellar-security-announce group for emails about security and major API announcements.</p>"},{"location":"Contribution%20guidelines/security/security/#report-a-vulnerability","title":"Report a Vulnerability","text":"<p>We're extremely grateful for security researchers and users that report vulnerabilities to the KubeStellar Open Source Community. All reports are thoroughly investigated by a set of community volunteers.</p> <p>You can also email the private kubestellar-security-announce@googlegroups.com list with the security details and the details expected for all KubeStellar bug reports.</p>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-report-a-vulnerability","title":"When Should I Report a Vulnerability?","text":"<ul> <li>You think you discovered a potential security vulnerability in KubeStellar</li> <li>You are unsure how a vulnerability affects KubeStellar</li> <li>You think you discovered a vulnerability in another project that KubeStellar depends on</li> <li>For projects with their own vulnerability reporting and disclosure process, please report it directly there</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-not-report-a-vulnerability","title":"When Should I NOT Report a Vulnerability?","text":"<ul> <li>You need help tuning KubeStellar components for security</li> <li>You need help applying security related updates</li> <li>Your issue is not security related</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#security-vulnerability-response","title":"Security Vulnerability Response","text":"<p>Each report is acknowledged and analyzed by the maintainers of KubeStellar within 3 working days.</p> <p>Any vulnerability information shared with Security Response Committee stays within KubeStellar project and will not be disseminated to other projects unless it is necessary to get the issue fixed.</p> <p>As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated.</p>"},{"location":"Contribution%20guidelines/security/security/#public-disclosure-timing","title":"Public Disclosure Timing","text":"<p>A public disclosure date is negotiated by the KubeStellar Security Response Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible once a user mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure is from immediate (especially if it's already publicly known) to a few weeks. For a vulnerability with a straightforward mitigation, we expect report date to disclosure date to be on the order of 7 days. The KubeStellar maintainers hold the final say when setting a disclosure date.</p>"},{"location":"Contribution%20guidelines/security/security_contacts/","title":"Contacts","text":"<p>Defined below are the security contacts for this repo.</p> <p>They are the contact point for the Product Security Committee to reach out to for triaging and handling of incoming issues.</p> <p>The below names agree to address security concerns if and when they arise.</p> <p>DO NOT REPORT SECURITY VULNERABILITIES DIRECTLY TO THESE NAMES, SEND INFORMATION TO kubestellar-security-announce@googlegroups.com</p> <p>clubanderson MikeSpreitzer ezrasilvera pdettori</p>"},{"location":"Getting-Started/infomercial/","title":"KubeStellar - The Infomercial","text":""},{"location":"Getting-Started/quickstart/","title":"QuickStart","text":""},{"location":"Getting-Started/quickstart/#demo-video","title":"Demo Video","text":"<p>Watch this video to see a step-by-step demo of KubeStellar running and then follow the instructions below to get your own KubeStellar started quickly.</p> <p> </p> <p>Estimated time to complete this example:</p> <p>~4 minutes (after installing prerequisites)</p>"},{"location":"Getting-Started/quickstart/#setup-instructions","title":"Setup Instructions","text":"<p>Table of contents:</p> <ol> <li>Check Required Packages</li> <li>Install and run kcp and KubeStellar</li> <li>Example deployment of Apache HTTP Server workload into two local kind clusters<ol> <li>Stand up two kind clusters: florin and guilder</li> <li>Onboarding the clusters</li> <li>Create and deploy the Apache Server workload into florin and guilder clusters</li> </ol> </li> <li>Teardown the environment</li> <li>Next Steps</li> </ol> <p>This guide is intended to show how to (1) quickly bring up a KubeStellar environment with its dependencies from a binary release and then (2) run through a simple example usage.</p>"},{"location":"Getting-Started/quickstart/#1-check-required-packages","title":"1. Check Required Packages","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre></p> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p>"},{"location":"Getting-Started/quickstart/#how-to-install-pre-requisites-for-a-windows-subsystem-for-linux-wsl-envronment-using-an-ubuntu-220401-distribution","title":"How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution","text":"<p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p>"},{"location":"Getting-Started/quickstart/#1-if-youre-using-a-vpn-turn-it-off","title":"1. If you're using a VPN, turn it off","text":""},{"location":"Getting-Started/quickstart/#2-install-ubuntu-into-wsl","title":"2. Install Ubuntu into WSL","text":""},{"location":"Getting-Started/quickstart/#20-if-wsl-is-not-yet-installed-open-a-powershell-administrator-window-and-run-the-following","title":"2.0 If wsl is not yet installed, open a powershell administrator window and run the following","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"Getting-Started/quickstart/#21-reboot-your-system","title":"2.1 reboot your system","text":""},{"location":"Getting-Started/quickstart/#22-in-a-windows-command-terminal-run-the-following-to-list-all-the-linux-distributions-that-are-available-online","title":"2.2 In a Windows command terminal run the following to list all the linux distributions that are available online","text":"<pre><code>wsl -l -o\n</code></pre>"},{"location":"Getting-Started/quickstart/#23-select-a-linux-distribution-and-install-it-into-wsl","title":"2.3 Select a linux distribution and install it into WSL","text":"<p><pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p>"},{"location":"Getting-Started/quickstart/#24-enter-your-new-username-and-password-at-the-prompts-and-you-will-eventually-see-something-like","title":"2.4 Enter your new username and password at the prompts, and you will eventually see something like:","text":"<pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre>"},{"location":"Getting-Started/quickstart/#25-click-on-the-windows-start-icon-and-type-in-the-name-of-your-distribution-into-the-search-box","title":"2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box.","text":"<p>Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p>"},{"location":"Getting-Started/quickstart/#3-install-pre-requisites-into-your-new-vm","title":"3. Install pre-requisites into your new VM","text":""},{"location":"Getting-Started/quickstart/#31-update-and-apply-apt-get-packages","title":"3.1 update and apply apt-get packages","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"Getting-Started/quickstart/#32-install-golang","title":"3.2 Install golang","text":"<pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre>"},{"location":"Getting-Started/quickstart/#33-install-ko-but-dont-do-ko-set-action-step","title":"3.3 Install ko (but don't do ko set action step)","text":"<pre><code>go install github.com/google/ko@latest\n</code></pre>"},{"location":"Getting-Started/quickstart/#34-install-gcc","title":"3.4 Install gcc","text":"<p>Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p>"},{"location":"Getting-Started/quickstart/#35-install-make-if-you-installed-build-essential-this-may-already-be-installed","title":"3.5 Install make (if you installed build-essential this may already be installed)","text":"<pre><code>apt install make\n</code></pre>"},{"location":"Getting-Started/quickstart/#36-install-jq","title":"3.6 Install jq","text":"<pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre>"},{"location":"Getting-Started/quickstart/#37-install-kubectl","title":"3.7 install kubectl","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"Getting-Started/quickstart/#38-install-helm-required-when-deploying-as-workload","title":"3.8 install helm (required when deploying as workload)","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"Getting-Started/quickstart/#how-to-install-docker-and-kind-into-a-windows-subsystem-for-linux-wsl-environment-using-an-ubuntu-220401-distribution","title":"How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution","text":""},{"location":"Getting-Started/quickstart/#10-start-a-vm-terminal-by-clicking-on-the-app-you-configured-using-the-instructions-in-the-general-pre-requisites-described-above","title":"1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.","text":""},{"location":"Getting-Started/quickstart/#20-install-docker","title":"2.0 Install docker","text":"<p>The installation instructions from docker are not sufficient to get docker working with WSL</p>"},{"location":"Getting-Started/quickstart/#21-follow-instructions-here-to-install-docker-httpsdocsdockercomengineinstallubuntu","title":"2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/","text":"<p>Here some additonal steps you will need to take:</p>"},{"location":"Getting-Started/quickstart/#22-ensure-that-etcwslconf-is-configured-so-that-systemd-will-run-on-booting","title":"2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting.","text":"<p>If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"Getting-Started/quickstart/#23-edit-etcsudoers-it-is-strongly-recommended-to-not-add-directives-directly-to-etcsudoers-but-instead-to-put-them-in-files-in-etcsudoersd-which-are-auto-included-so-makemodify-a-new-file-via","title":"2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via","text":"<p><pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p>"},{"location":"Getting-Started/quickstart/#24-add-your-user-to-the-docker-group","title":"2.4 Add your user to the docker group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"Getting-Started/quickstart/#25-if-dockerd-is-already-running-then-stop-it-and-restart-it-as-follows-note-the-new-dockerd-instance-will-be-running-in-the-foreground","title":"2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground):","text":"<pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre>"},{"location":"Getting-Started/quickstart/#251-if-you-encounter-an-iptables-issue-which-is-described-here-httpsgithubcommicrosoftwslissues6655","title":"2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655","text":"<p>The following commands will fix the issue:  <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p>"},{"location":"Getting-Started/quickstart/#3-you-will-now-need-to-open-new-terminals-to-access-the-vm-since-dockerd-is-running-in-the-foreground-of-this-terminal","title":"3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal","text":""},{"location":"Getting-Started/quickstart/#31-in-your-new-terminal-install-kind","title":"3.1 In your new terminal, install kind","text":"<pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre>"},{"location":"Getting-Started/quickstart/#2-install-and-run-kcp-and-kubestellar","title":"2. Install and run kcp and KubeStellar","text":"<p>KubeStellar works in the context of kcp, so to use KubeStellar you also need kcp.</p> <p>KubeStellar works with release <code>v0.11.0</code> of kcp.</p> <p>We support two ways to deploy kcp and KubeStellar. The older way is to run them as bare processes. The newer way is to deploy them as workload in a Kubernetes (possibly OpenShift) cluster.</p>"},{"location":"Getting-Started/quickstart/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":"<p>The following commands will download the kcp and KubeStellar executables into subdirectories of your current working directory, deploy (i.e., start and configure) kcp and KubeStellar as bare processes, and configure your shell to use kcp and KubeStellar.  If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation about the commands for bare process deployment.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.9.0\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport KUBECONFIG=\"$(pwd)/.kcp/admin.kubeconfig\"\n</code></pre> <p>Check that <code>KubeStellar</code> is running.</p> <p>First, check that controllers are running with the following command:</p> <pre><code>ps aux | grep -e mailbox-controller -e placement-translator -e kubestellar-where-resolver\n</code></pre> <p>which should yield something like:</p> <pre><code>user     1892  0.0  0.3 747644 29628 pts/1    Sl   10:51   0:00 mailbox-controller -v=2\nuser     1902  0.3  0.3 743652 27504 pts/1    Sl   10:51   0:02 kubestellar-where-resolver -v 2\nuser     1912  0.3  0.5 760428 41660 pts/1    Sl   10:51   0:02 placement-translator -v=2\n</code></pre> <p>Second, check that TMC compute service provider workspace and the KubeStellar Edge Service Provider Workspace (<code>espw</code>) have been created with the following command:</p> <pre><code>kubectl ws tree\n</code></pre> <p>which should yield:</p> <pre><code>.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 compute\n    \u251c\u2500\u2500 espw\n    \u251c\u2500\u2500 imw1\n    \u2514\u2500\u2500 wmw1\n</code></pre>"},{"location":"Getting-Started/quickstart/#deploy-kcp-and-kubestellar-as-kubernetes-workload","title":"Deploy kcp and KubeStellar as Kubernetes workload","text":"<p>This requires a KubeStellar release GREATER THAN v0.5.0.</p> <p>This example uses a total of three <code>kind</code> clusters, which tends to run into a known issue with a known work-around, so take care of that.</p> <p>Before you can deploy kcp and KubeStellar as workload in a Kubernetes cluster, you need a Kubernetes cluster and it needs to have an Ingress controller installed.  We use the term \"hosting cluster\" for the cluster that plays this role.  In this quickstart, we make such a cluster with kind.  Follow the developer directions for making a hosting cluster with kind; you need not worry about loading a locally built container image into that cluster.</p> <p>This example uses the domain name \"hostname.favorite.my\" for the machine where you invoked <code>kind create cluster</code>. If you have not already done so then issue the following command, replacing <code>a_good_IP_address_for_this_machine</code> with an IPv4 address for your machine that can be reached from inside a container or VM (i.e., not 127.0.0.1).</p> <pre><code>sudo sh -c \"echo a_good_IP_address_for_this_machine hostname.favorite.my &gt;&gt; /etc/hosts\"\n</code></pre> <p>The next command relies on <code>kubectl</code> already being configured to manipulate the hosting cluster, which is indeed the state that <code>kind create cluster</code> leaves it in.</p> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory and (b) deploy (i.e., start and configure) kcp and KubeStellar as workload in the hosting cluster. If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation for the commands about deployment into a Kubernetes cluster.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.9.0 --external-endpoint hostname.favorite.my:{{ no such element: mkdocs.config.defaults.MkDocsConfig object['ks_kind_port_num'] }}\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre> <p>Using your original <code>kubectl</code> configuration that manipulates the hosting cluster, check that the KubeStellar Deployment has its intended one running Pod.</p> <pre><code>kubectl get deployments -n kubestellar\n</code></pre> <p>which should yield something like:</p> <pre><code>NAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-server   1/1     1            1           2m42s\n</code></pre> <p>It may take some time for that Pod to reach Running state.</p> <p>The bootstrap command above will print out instructions to set your KUBECONFIG environment variable to the pathname of a kubeconfig file that you can use as a user of kcp and KubeStellar.  Do that now, for the benefit of the remaining commands in this example.  It will look something like the following command.</p> <pre><code>export KUBECONFIG=\"$(pwd)/kubestellar.kubeconfig\"\n</code></pre> <p>Check that the TMC compute service provider workspace and the KubeStellar Edge Service Provider Workspace (<code>espw</code>) have been created with the following command:</p> <pre><code>kubectl ws tree\n</code></pre> <p>which should yield:</p> <pre><code>.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 compute\n    \u251c\u2500\u2500 espw\n    \u251c\u2500\u2500 imw1\n    \u2514\u2500\u2500 wmw1\n</code></pre>"},{"location":"Getting-Started/quickstart/#3-example-deployment-of-apache-http-server-workload-into-two-local-kind-clusters","title":"3. Example deployment of Apache HTTP Server workload into two local kind clusters","text":"<p>In this example you will create two edge clusters and define one workload that will be distributed from the center to those edge clusters.  This example is similar to the one described more expansively on the website, but with the some steps reorganized and combined and the special workload and summarization aspirations removed.</p>"},{"location":"Getting-Started/quickstart/#a-stand-up-two-kind-clusters-florin-and-guilder","title":"a. Stand up two kind clusters: florin and guilder","text":"<p>Create the first edge cluster:</p> <pre><code>kind create cluster --name florin --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'florin' from a previous exercise of KubeStellar, please delete the florin cluster ('kind delete cluster --name florin') and create it using the instruction above.</p> <p>Create the second edge cluster:</p> <pre><code>kind create cluster --name guilder --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'guilder' from a previous exercise of KubeStellar, please delete the guilder cluster ('kind delete cluster --name guilder') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart/#b-onboarding-the-clusters","title":"b. Onboarding the clusters","text":"<p>The above use of <code>kind</code> has knocked kcp's <code>kubectl ws</code> plugin off kilter, as the latter uses the local kubeconfig to store its state about the \"current\" and \"previous\" workspaces.  Get it back on track with the following command.</p> <pre><code>kubectl config use-context root\n</code></pre> <p>KubeStellar will have created an Inventory Management Workspace (IMW) for the user to put inventory objects in, describing the user's clusters. The IMW that is automatically created for the user is at <code>root:imw1</code>.</p> <p>Let's begin by onboarding the <code>florin</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:imw1 florin env=prod\n</code></pre> <p>which should yield something like:</p> <pre><code>Current workspace is \"root:imw1\".\nsynctarget.workload.kcp.io/florin created\nlocation.scheduling.kcp.io/florin created\nsynctarget.workload.kcp.io/florin labeled\nlocation.scheduling.kcp.io/florin labeled\nCurrent workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:9nemli4rpx83ahnz-mb-c44d04db-ae85-422c-9e12-c5e7865bf37a\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-1yi5q9c4\"\nCreating cluster role \"kubestellar-syncer-florin-1yi5q9c4\" to give service account \"kubestellar-syncer-florin-1yi5q9c4\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-1yi5q9c4\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-1yi5q9c4\" to bind service account \"kubestellar-syncer-florin-1yi5q9c4\" to cluster role \"kubestellar-syncer-florin-1yi5q9c4\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-1yi5q9c4\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-1yi5q9c4\" kubestellar-syncer-florin-1yi5q9c4\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:imw1\".\nCurrent workspace is \"root\".\n</code></pre> <p>An edge syncer manifest yaml file was created in your current directory: <code>florin-syncer.yaml</code>. The default for the output file is the name of the SyncTarget object with \u201c-syncer.yaml\u201d appended.</p> <p>Now let's deploy the edge syncer to the <code>florin</code> edge cluster:</p> <pre><code>kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <p>which should yield something like:</p> <pre><code>namespace/kubestellar-syncer-florin-1yi5q9c4 created\nserviceaccount/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4 created\ndeployment.apps/kubestellar-syncer-florin-1yi5q9c4 created\n</code></pre> <p>Optionally, check that the edge syncer pod is running:</p> <pre><code>kubectl --context kind-florin get pods -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                                  READY   STATUS    RESTARTS   AGE\nkubestellar-syncer-florin-1yi5q9c4   kubestellar-syncer-florin-1yi5q9c4-77cb588c89-xc5qr   1/1     Running   0          12m\nkube-system                          coredns-565d847f94-92f4k                              1/1     Running   0          58m\nkube-system                          coredns-565d847f94-kgddm                              1/1     Running   0          58m\nkube-system                          etcd-florin-control-plane                             1/1     Running   0          58m\nkube-system                          kindnet-p8vkv                                         1/1     Running   0          58m\nkube-system                          kube-apiserver-florin-control-plane                   1/1     Running   0          58m\nkube-system                          kube-controller-manager-florin-control-plane          1/1     Running   0          58m\nkube-system                          kube-proxy-jmxsg                                      1/1     Running   0          58m\nkube-system                          kube-scheduler-florin-control-plane                   1/1     Running   0          58m\nlocal-path-storage                   local-path-provisioner-684f458cdd-kw2xz               1/1     Running   0          58m\n</code></pre> <p>Now, let's onboard the <code>guilder</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:imw1 guilder env=prod extended=si\n</code></pre> <p>Apply the created edge syncer manifest:</p> <pre><code>kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre>"},{"location":"Getting-Started/quickstart/#c-create-and-deploy-the-apache-server-workload-into-florin-and-guilder-clusters","title":"c. Create and deploy the Apache Server workload into florin and guilder clusters","text":"<p>KubeStellar will have automatically created a Workload Management Workspace (WMW) for the user to store workload descriptions and MCCM control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create the <code>EdgePlacement</code> object for your workload. Its \u201cwhere predicate\u201d (the locationSelectors array) has one label selector that matches the Location objects (<code>florin</code> and <code>guilder</code>) created earlier, thus directing the workload to both edge clusters.</p> <p>In the <code>root:wmw1</code> workspace create the following <code>EdgePlacement</code> object: </p> <pre><code>kubectl ws root:wmw1\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"common\":\"si\"}\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"common\":\"si\"}\n    objectNames: [ commond ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <p>Put the prescription of the HTTP server workload into the WMW. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>edge-placement-c</code>) object created above. </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Now, let's check that the deployment was created in the <code>florin</code> edge cluster - it may take a few 10s of seconds to appear:</p> <pre><code>kubectl --context kind-florin get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                          commond                              1/1     1            1           6m48s\nkubestellar-syncer-florin-2upj1awn   kubestellar-syncer-florin-2upj1awn   1/1     1            1           16m\nkube-system                          coredns                              2/2     2            2           28m\nlocal-path-storage                   local-path-provisioner               1/1     1            1           28m\n</code></pre> <p>Also, let's check that the deployment was created in the <code>guilder</code> edge cluster:</p> <pre><code>kubectl --context kind-guilder get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                           commond                               1/1     1            1           7m54s\nkubestellar-syncer-guilder-6tuay5d6   kubestellar-syncer-guilder-6tuay5d6   1/1     1            1           12m\nkube-system                           coredns                               2/2     2            2           27m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           27m\n</code></pre> <p>Lastly, let's check that the workload is working in both clusters:</p> <p>For <code>florin</code>:</p> <pre><code>while [[ $(kubectl --context kind-florin get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8094\n</code></pre> <p>which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>For <code>guilder</code>:</p> <p><pre><code>while [[ $(kubectl --context kind-guilder get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8096\n</code></pre> which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Congratulations, you\u2019ve just deployed a workload to two edge clusters using kubestellar! To learn more about kubestellar please visit our User Guide</p>"},{"location":"Getting-Started/quickstart/#4-teardown-the-environment","title":"4. Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl kubestellar remove wmw example-wmw\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"Getting-Started/quickstart/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Getting-Started/quickstart/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"},{"location":"Getting-Started/quickstart/#5-next-steps","title":"5. Next Steps","text":"<p>What you just did is a shortened version of the  more detailed example on the website, but with the some steps reorganized and combined and the special workload and summarization aspiration removed.  You can continue from here, learning more details about what you did in the QuickStart, and adding on some more steps for the special workload.</p>"},{"location":"Getting-Started/user-guide/","title":"User Guide","text":""},{"location":"Getting-Started/user-guide/#kubestellar-primer","title":"KubeStellar Primer","text":"<p>This is a description of the concepts behind KubeStellar.</p>"},{"location":"Getting-Started/user-guide/#glossary","title":"Glossary","text":"<p>Downsynced Object - One of two categories of workload object, complementary to \"upsynced object\".  In KubeStellar, a downsynced object first appears in a Workload Description Space and the object's desired state propagates from there through Mailbox Spaces to Workload Execution Clusters and that object's reported state originates in the Workload Execution Clusters and propagates back to the Mailbox Spaces and in the future will be summarized into the Workload Description Space.</p> <p>EdgePlacement - A kind of Kubernetes API object, in a Workload Description Space. One of objects these binds some workload (\"what\") with a set of workload execution clusters (\"where\") it should run. The workload is identified by a predicate over namespaced objects and a predicate over cluster-scoped objects. The where is identified by a predicate over workload execution clusters as represented by <code>Location</code> objects.</p> <p>Inventory Space (IS) - Holds the <code>SyncTarget</code> and <code>Location</code> objects describing the Workload Execution Clusters. </p> <p>kcp - A project in whose context KubeStellar currently sits.  See kcp.io and its concepts page.  KubeStellar relies on two aspects of the functionality of the core of kcp: (1) \"logical clusters\" and their API, \"workspaces\", and (2) API export and binding.  KubeStellar is in the process of separating from kcp, introducing an abstraction layer for \"spaces\" and developing the ability to use kube-bind for API export and binding.</p> <p>KubeStellar Core Space (KCS) - Exports the Kubestellar API.</p> <p>KubeStellar Syncer - The KubeStellar agent in a Workload Execution Cluster; syncs workload objects between the Workload Execution Cluster and the corresponding Mailbox Space.</p> <p>Location - A kind of Kubernetes API object, in an Inventory Space. Paired one-to-one with a <code>SyncTarget</code> object in the same space. Together these describe a workload execution cluster. The Location's labels are tested by the \"where predicate\" in an <code>EdgePlacement</code> object, and this object's labels and annotations provide values used in customization of workload objects going to the workload execution cluster.</p> <p>Mailbox Controller - One of the central KubeStellar controllers; maintains a Mailbox Space for each <code>SyncTarget</code> object. This includes putting an APIBinding to the KubeStellar API into those mailbox spaces.</p> <p>Mailbox Space - There is one Mailbox Space for each workload execution cluster. It stores the <code>SyncerConfig</code> object and copies of the workload(s).</p> <p>PlacementTranslator - One of the central KubeStellar controllers; maintains the <code>SyncerConfig</code> objects in the Mailbox Spaces and syncs workload objects between the Workload Description Spaces and the Mailbox Spaces.</p> <p>SinglePlacementSlice - A kind of Kubernetes API object, in a Workload Description Space.  Such an object holds a list of references to <code>Location</code> &amp; <code>SyncTarget</code> objects that match the \"where predicate\" of an <code>EdgePlacement</code>.  Currently there is exactly one <code>SinglePlacementSlice</code> for each <code>EdgePlacement</code> but in the future the matches for one <code>EdgePlacement</code> could be spread among several <code>SinglePlacementSlice</code> objects (analogously to <code>EndpointSlice</code> vs <code>Service</code> in Kubernetes).</p> <p>Space Object - A kind of Kubernetes API object that represents a physical space.  Its \"kind\" is <code>Space</code>, so we include the \"object\" part when writing specifically about the API objects to clearly distinguish them from what they represent.</p> <p>Space, physical space (use the latter term to clearly distinguish from \"space object\") - A thing that behaves like a Kubernetes kube-apiserver (including the persistent storage behind it) and the subset of controllers in the kube-controller-manager that are concerned with API machinery generalities (not management of containerized workloads). A kcp logical cluster is an example. A regular Kubernetes cluster is another example.</p> <p>Status Summarizer - A planned central KubeStellar controller that will maintain the status summary objects in the Workload Description Spaces as a function of the <code>EdgePlacement</code> objects and the workload objects in the Mailbox Spaces.</p> <p>SyncerConfig - A kind of Kubernetes API object, in a Mailbox Space. Such an object holds the dynamic configuration for the syncer in the corresponding workload execution cluster.</p> <p>SyncTarget - A kind of Kubernetes API object, in an Inventory Space. Paired one-to-one with a Location in the same space, jointly representing a Workload Execution Cluster.</p> <p>Upsynced Object - One of two categories of workload object, complementary to \"downsynced object\".  Upsynced objects originate in Workload Execution Clusters and propagate inward to Mailbox Spaces and in the future will be summarized into Workload Description Spaces.</p> <p>Where Resolver - One of the central KubeStellar controllers; tests the <code>Location</code> objects against the \"where predicates\" in the <code>EdgePlacement</code> objects to maintain the corresponding <code>SinglePlacementSlice</code> objects.</p> <p>Workload Description Space (WDS) - Holds workload objects and the adjacent KubeStellar control objects, which are the <code>EdgePlacement</code>, <code>SinglePlacementSlice</code>, and <code>Customizer</code> objects and, eventually, the ones developed to prescribe summarization.</p> <p>Workload Execution Cluster - A Kubernetes cluster which can execute a workload. In the examples on this website, we use Kind clusters.</p>"},{"location":"Getting-Started/user-guide/#older-terminology","title":"Older Terminology","text":"<p>There have been some terminology shifts since the start of the project.  The project started with a focus on edge computing scenarios; later we realized that the technical problems addressed are not limited to those scenarios.  The project started in the context of kcp and unabashedly used concepts from kcp; later we began working on generalizing KubeStellar so that it can run in the context of kcp and also can run in other contexts that do not have kcp.</p> <ul> <li>The term \"space\" is intended to be a generalization covering both a kcp \"logical cluster\" or \"workspace\" and other things that have the same essential behavior</li> <li>The term \"workload execution cluster\" was formerly \"edge cluster\"</li> <li>The term \"workload description space\" was formerly \"workload management workspace\"</li> <li>The term \"inventory space\" was formerly \"inventory management workspace\"</li> <li>The term \"kubestellar core space\" was formerly \"edge service provider workspace\"</li> <li>The term \"mailbox space\" was formerly \"mailbox workspace\"</li> </ul>"},{"location":"Getting-Started/quickstart-subs/quickstart-0-demo/","title":"Quickstart 0 demo","text":""},{"location":"Getting-Started/quickstart-subs/quickstart-0-demo/#demo-video","title":"Demo Video","text":"<p>Watch this video to see a step-by-step demo of KubeStellar running and then follow the instructions below to get your own KubeStellar started quickly.</p> <p> </p>"},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/","title":"Quickstart 1 install and run kubestellar","text":"<p>KubeStellar works in the context of kcp, so to use KubeStellar you also need kcp.</p> <p>KubeStellar works with release <code>v0.11.0</code> of kcp.</p> <p>We support two ways to deploy kcp and KubeStellar. The older way is to run them as bare processes. The newer way is to deploy them as workload in a Kubernetes (possibly OpenShift) cluster.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/#deploy-kcp-and-kubestellar-as-bare-processes","title":"Deploy kcp and KubeStellar as bare processes","text":"<p>The following commands will download the kcp and KubeStellar executables into subdirectories of your current working directory, deploy (i.e., start and configure) kcp and KubeStellar as bare processes, and configure your shell to use kcp and KubeStellar.  If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation about the commands for bare process deployment.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.9.0\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport KUBECONFIG=\"$(pwd)/.kcp/admin.kubeconfig\"\n</code></pre> <p>Check that <code>KubeStellar</code> is running.</p> <p>First, check that controllers are running with the following command:</p> <pre><code>ps aux | grep -e mailbox-controller -e placement-translator -e kubestellar-where-resolver\n</code></pre> <p>which should yield something like:</p> <pre><code>user     1892  0.0  0.3 747644 29628 pts/1    Sl   10:51   0:00 mailbox-controller -v=2\nuser     1902  0.3  0.3 743652 27504 pts/1    Sl   10:51   0:02 kubestellar-where-resolver -v 2\nuser     1912  0.3  0.5 760428 41660 pts/1    Sl   10:51   0:02 placement-translator -v=2\n</code></pre> <p>Second, check that TMC compute service provider workspace and the KubeStellar Edge Service Provider Workspace (<code>espw</code>) have been created with the following command:</p> <pre><code>kubectl ws tree\n</code></pre> <p>which should yield:</p> <pre><code>.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 compute\n    \u251c\u2500\u2500 espw\n    \u251c\u2500\u2500 imw1\n    \u2514\u2500\u2500 wmw1\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/#deploy-kcp-and-kubestellar-as-kubernetes-workload","title":"Deploy kcp and KubeStellar as Kubernetes workload","text":"<p>This requires a KubeStellar release GREATER THAN v0.5.0.</p> <p>This example uses a total of three <code>kind</code> clusters, which tends to run into a known issue with a known work-around, so take care of that.</p> <p>Before you can deploy kcp and KubeStellar as workload in a Kubernetes cluster, you need a Kubernetes cluster and it needs to have an Ingress controller installed.  We use the term \"hosting cluster\" for the cluster that plays this role.  In this quickstart, we make such a cluster with kind.  Follow the developer directions for making a hosting cluster with kind; you need not worry about loading a locally built container image into that cluster.</p> <p>This example uses the domain name \"hostname.favorite.my\" for the machine where you invoked <code>kind create cluster</code>. If you have not already done so then issue the following command, replacing <code>a_good_IP_address_for_this_machine</code> with an IPv4 address for your machine that can be reached from inside a container or VM (i.e., not 127.0.0.1).</p> <pre><code>sudo sh -c \"echo a_good_IP_address_for_this_machine hostname.favorite.my &gt;&gt; /etc/hosts\"\n</code></pre> <p>The next command relies on <code>kubectl</code> already being configured to manipulate the hosting cluster, which is indeed the state that <code>kind create cluster</code> leaves it in.</p> <p>The following commands will (a) download the kcp and KubeStellar executables into subdirectories of your current working directory and (b) deploy (i.e., start and configure) kcp and KubeStellar as workload in the hosting cluster. If you want to suppress the deployment part then add <code>--deploy false</code> to the first command's flags (e.g., after the specification of the KubeStellar version); for the deployment-only part, once the executable have been fetched, see the documentation for the commands about deployment into a Kubernetes cluster.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version v0.9.0 --external-endpoint hostname.favorite.my:{{ no such element: mkdocs.config.defaults.MkDocsConfig object['ks_kind_port_num'] }}\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\n</code></pre> <p>Using your original <code>kubectl</code> configuration that manipulates the hosting cluster, check that the KubeStellar Deployment has its intended one running Pod.</p> <pre><code>kubectl get deployments -n kubestellar\n</code></pre> <p>which should yield something like:</p> <pre><code>NAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-server   1/1     1            1           2m42s\n</code></pre> <p>It may take some time for that Pod to reach Running state.</p> <p>The bootstrap command above will print out instructions to set your KUBECONFIG environment variable to the pathname of a kubeconfig file that you can use as a user of kcp and KubeStellar.  Do that now, for the benefit of the remaining commands in this example.  It will look something like the following command.</p> <pre><code>export KUBECONFIG=\"$(pwd)/kubestellar.kubeconfig\"\n</code></pre> <p>Check that the TMC compute service provider workspace and the KubeStellar Edge Service Provider Workspace (<code>espw</code>) have been created with the following command:</p> <pre><code>kubectl ws tree\n</code></pre> <p>which should yield:</p> <pre><code>.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 compute\n    \u251c\u2500\u2500 espw\n    \u251c\u2500\u2500 imw1\n    \u2514\u2500\u2500 wmw1\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-a-kind-clusters-florin/","title":"Quickstart 2 apache example deployment a kind clusters florin","text":"<p>Create the first edge cluster:</p> <pre><code>kind create cluster --name florin --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'florin' from a previous exercise of KubeStellar, please delete the florin cluster ('kind delete cluster --name florin') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-a-kind-clusters-guilder/","title":"Quickstart 2 apache example deployment a kind clusters guilder","text":"<p>Create the second edge cluster:</p> <pre><code>kind create cluster --name guilder --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'guilder' from a previous exercise of KubeStellar, please delete the guilder cluster ('kind delete cluster --name guilder') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-c-onboarding-clusters/","title":"Quickstart 2 apache example deployment c onboarding clusters","text":"<p>The above use of <code>kind</code> has knocked kcp's <code>kubectl ws</code> plugin off kilter, as the latter uses the local kubeconfig to store its state about the \"current\" and \"previous\" workspaces.  Get it back on track with the following command.</p> <pre><code>kubectl config use-context root\n</code></pre> <p>KubeStellar will have created an Inventory Management Workspace (IMW) for the user to put inventory objects in, describing the user's clusters. The IMW that is automatically created for the user is at <code>root:imw1</code>.</p> <p>Let's begin by onboarding the <code>florin</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:imw1 florin env=prod\n</code></pre> <p>which should yield something like:</p> <pre><code>Current workspace is \"root:imw1\".\nsynctarget.workload.kcp.io/florin created\nlocation.scheduling.kcp.io/florin created\nsynctarget.workload.kcp.io/florin labeled\nlocation.scheduling.kcp.io/florin labeled\nCurrent workspace is \"root:imw1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:9nemli4rpx83ahnz-mb-c44d04db-ae85-422c-9e12-c5e7865bf37a\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-1yi5q9c4\"\nCreating cluster role \"kubestellar-syncer-florin-1yi5q9c4\" to give service account \"kubestellar-syncer-florin-1yi5q9c4\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-1yi5q9c4\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-1yi5q9c4\" to bind service account \"kubestellar-syncer-florin-1yi5q9c4\" to cluster role \"kubestellar-syncer-florin-1yi5q9c4\".\n\nWrote WEC manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-1yi5q9c4\". Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;workload-execution-cluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-1yi5q9c4\" kubestellar-syncer-florin-1yi5q9c4\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:imw1\".\nCurrent workspace is \"root\".\n</code></pre> <p>An edge syncer manifest yaml file was created in your current directory: <code>florin-syncer.yaml</code>. The default for the output file is the name of the SyncTarget object with \u201c-syncer.yaml\u201d appended.</p> <p>Now let's deploy the edge syncer to the <code>florin</code> edge cluster:</p> <pre><code>kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <p>which should yield something like:</p> <pre><code>namespace/kubestellar-syncer-florin-1yi5q9c4 created\nserviceaccount/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4 created\ndeployment.apps/kubestellar-syncer-florin-1yi5q9c4 created\n</code></pre> <p>Optionally, check that the edge syncer pod is running:</p> <pre><code>kubectl --context kind-florin get pods -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                                  READY   STATUS    RESTARTS   AGE\nkubestellar-syncer-florin-1yi5q9c4   kubestellar-syncer-florin-1yi5q9c4-77cb588c89-xc5qr   1/1     Running   0          12m\nkube-system                          coredns-565d847f94-92f4k                              1/1     Running   0          58m\nkube-system                          coredns-565d847f94-kgddm                              1/1     Running   0          58m\nkube-system                          etcd-florin-control-plane                             1/1     Running   0          58m\nkube-system                          kindnet-p8vkv                                         1/1     Running   0          58m\nkube-system                          kube-apiserver-florin-control-plane                   1/1     Running   0          58m\nkube-system                          kube-controller-manager-florin-control-plane          1/1     Running   0          58m\nkube-system                          kube-proxy-jmxsg                                      1/1     Running   0          58m\nkube-system                          kube-scheduler-florin-control-plane                   1/1     Running   0          58m\nlocal-path-storage                   local-path-provisioner-684f458cdd-kw2xz               1/1     Running   0          58m\n</code></pre> <p>Now, let's onboard the <code>guilder</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:imw1 guilder env=prod extended=si\n</code></pre> <p>Apply the created edge syncer manifest:</p> <pre><code>kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-d-create-and-deploy-apache-into-clusters/","title":"Quickstart 2 apache example deployment d create and deploy apache into clusters","text":"<p>KubeStellar will have automatically created a Workload Management Workspace (WMW) for the user to store workload descriptions and MCCM control objects in. The automatically created WMW is at <code>root:wmw1</code>.</p> <p>Create the <code>EdgePlacement</code> object for your workload. Its \u201cwhere predicate\u201d (the locationSelectors array) has one label selector that matches the Location objects (<code>florin</code> and <code>guilder</code>) created earlier, thus directing the workload to both edge clusters.</p> <p>In the <code>root:wmw1</code> workspace create the following <code>EdgePlacement</code> object: </p> <pre><code>kubectl ws root:wmw1\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kubestellar.io/v2alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  downsync:\n  - apiGroup: \"\"\n    resources: [ configmaps ]\n    namespaceSelectors:\n    - matchLabels: {\"common\":\"si\"}\n    objectNames: [ \"*\" ]\n  - apiGroup: apps\n    resources: [ deployments ]\n    namespaceSelectors:\n    - matchLabels: {\"common\":\"si\"}\n    objectNames: [ commond ]\n  - apiGroup: apis.kcp.io\n    resources: [ apibindings ]\n    namespaceSelectors: []\n    objectNames: [ \"bind-kubernetes\", \"bind-apps\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <p>Put the prescription of the HTTP server workload into the WMW. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>edge-placement-c</code>) object created above. </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Now, let's check that the deployment was created in the <code>florin</code> edge cluster - it may take a few 10s of seconds to appear:</p> <pre><code>kubectl --context kind-florin get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                          commond                              1/1     1            1           6m48s\nkubestellar-syncer-florin-2upj1awn   kubestellar-syncer-florin-2upj1awn   1/1     1            1           16m\nkube-system                          coredns                              2/2     2            2           28m\nlocal-path-storage                   local-path-provisioner               1/1     1            1           28m\n</code></pre> <p>Also, let's check that the deployment was created in the <code>guilder</code> edge cluster:</p> <pre><code>kubectl --context kind-guilder get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                           commond                               1/1     1            1           7m54s\nkubestellar-syncer-guilder-6tuay5d6   kubestellar-syncer-guilder-6tuay5d6   1/1     1            1           12m\nkube-system                           coredns                               2/2     2            2           27m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           27m\n</code></pre> <p>Lastly, let's check that the workload is working in both clusters:</p> <p>For <code>florin</code>:</p> <pre><code>while [[ $(kubectl --context kind-florin get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8094\n</code></pre> <p>which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>For <code>guilder</code>:</p> <p><pre><code>while [[ $(kubectl --context kind-guilder get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8096\n</code></pre> which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Congratulations, you\u2019ve just deployed a workload to two edge clusters using kubestellar! To learn more about kubestellar please visit our User Guide</p>"},{"location":"common-subs/check-command-syncerconfig-the-one/","title":"Check command syncerconfig the one","text":"<pre><code>let increment=10\nlet slept=1\nwhile ! kubectl get SyncerConfig the-one -o yaml; do\nif (( slept &gt;= 300 )); then\necho \"FAILURE to run command 'kubectl get SyncerConfig the-one -o yaml' (slept $slept)\" &gt;&amp;2\nexit 86\nfi\nsleep $increment\nlet slept=slept+increment\n    echo slept\ndone\n</code></pre>"},{"location":"common-subs/coming-soon/","title":"Coming soon","text":""},{"location":"common-subs/pre-req/","title":"Pre req","text":"<pre><code>os_type=\"\"\narch_type=\"\"\nfolder=\"\"\nget_os_type() {\ncase \"$OSTYPE\" in\nlinux*)   echo \"linux\" ;;\ndarwin*)  echo \"darwin\" ;;\n*)        echo \"Unsupported operating system type: $OSTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_arch_type() {\ncase \"$HOSTTYPE\" in\nx86_64*)  echo \"amd64\" ;;\naarch64*) echo \"arm64\" ;;\narm64*)   echo \"arm64\" ;;\n*)        echo \"Unsupported architecture type: $HOSTTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_os_type() {\ncase \"$OSTYPE\" in\nlinux*)   echo \"linux\" ;;\ndarwin*)  echo \"darwin\" ;;\n*)        echo \"Unsupported operating system type: $OSTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_arch_type() {\ncase \"$HOSTTYPE\" in\nx86_64*)  echo \"amd64\" ;;\naarch64*) echo \"arm64\" ;;\narm64*)   echo \"arm64\" ;;\n*)        echo \"Unsupported architecture type: $HOSTTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nif [ \"$os_type\" == \"\" ]; then\nos_type=$(get_os_type)\nfi\nif [ \"$arch_type\" == \"\" ]; then\narch_type=$(get_arch_type)\nfi\nif [ \"$folder\" == \"\" ]; then\nfolder=\"$PWD\"\nfi\necho $os_type\necho $arch_type\necho $folder\nif command -v docker &gt;/dev/null 2&gt;&amp;1; then\necho \"Docker is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install docker\n    fi\nfi\nif docker info &gt;/dev/null 2&gt;&amp;1; then\necho \"Docker is started\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nopen --background -a Docker\n      sleep 30\nfi\nfi\nif command -v go &gt;/dev/null 2&gt;&amp;1; then\ngo_version_output=$(go version)\ngo_version=$(echo \"$go_version_output\" | awk '{print $3}')\necho \"GO is installed\"\nif [[ \"$go_version\" == \"go1.19\"* ]]; then\necho \"Go version 1.19 is installed.\"\nelse\necho \"Go version 1.19 is not installed.  Please install Go version 1.19 to continue\"\nfi\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install go@1.19\n    fi\nfi\nif command -v kubectl &gt;/dev/null 2&gt;&amp;1; then\necho \"kubectl is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install kubectl\n    fi\nfi\nif command -v jq &gt;/dev/null 2&gt;&amp;1; then\necho \"jq is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install jq\n    fi\nfi\nif command -v kind &gt;/dev/null 2&gt;&amp;1; then\necho \"kind is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install kind\n    fi\nfi\nps -ef | grep mailbox-controller | grep -v grep | grep -v make | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep where-resolver | grep -v grep | grep -v make | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep placement-translator | grep -v grep | grep -v make | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kcp | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep 'exe/main -v 2' | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nkind delete cluster --name florin 2&gt;&amp;1\nkind delete cluster --name guilder 2&gt;&amp;1\n</code></pre>"},{"location":"common-subs/remove-all/","title":"Remove all","text":"<pre><code>ps -ef | grep mailbox-controller | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kubestellar-where-resolver | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep placement-translator | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kcp | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep 'exe/main -v 2' | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nkind delete cluster --name florin 2&gt;&amp;1\nkind delete cluster --name guilder 2&gt;&amp;1\n</code></pre>"},{"location":"common-subs/required-packages/","title":"Required packages","text":"<p>Required Packages for running and using KubeStellar:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools to deploy and use KubeStellar.  Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>curl (omitted from most OS-specific instructions)</p> </li> <li> <p>jq</p> </li> <li> <p>yq</p> </li> <li> <p>kubectl (version range expected: 1.23-1.25)</p> </li> <li> <p>helm (required when deploying as workload)</p> </li> </ul> <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>brew install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>brew install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo snap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>sudo apt-get install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code># easiest to install with snap\nsnap install yq\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n# for ARM64 / aarch64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>dnf install helm\n</code></pre></p> <p>Chocolatey - https://chocolatey.org/install#individual<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> curl<pre><code>choco install curl -y\n</code></pre> jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\n</code></pre> yq - https://github.com/mikefarah/yq#install<pre><code>choco install yq -y\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"    \n</code></pre> helm (required when deploying as workload) - https://helm.sh/docs/intro/install/<pre><code>choco install kubernetes-helm\n</code></pre></p> <p>Required Packages for the example usage:</p> GeneralMacUbuntuDebianFedora/RHEL/CentOSWindowsWSL with Ubuntu <p>You will need the following tools for the example usage of KubeStellar in this quickstart example. Select the tab for your environment for suggested commands to install them</p> <ul> <li> <p>docker or podman, to support <code>kind</code></p> </li> <li> <p>kind</p> </li> </ul> <p>docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y uidmap\ndockerd-rootless-setuptool.sh install\nsystemctl --user restart docker.service\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Enable rootless usage of Docker (requires relogin) - https://docs.docker.com/engine/security/rootless/<pre><code>sudo apt-get install -y dbus-user-session # *** Relogin after this\nsudo apt-get install -y fuse-overlayfs\nsudo apt-get install -y slirp4netns\ndockerd-rootless-setuptool.sh install\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> Enable rootless usage of Docker by following the instructions at https://docs.docker.com/engine/security/rootless/ kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code># For AMD64 / x86_64\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64\n# For ARM64\n[ $(uname -m) = aarch64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-arm64 \nchmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin/kind\n</code></pre></p> <p>docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre></p>"},{"location":"common-subs/required-packages/#how-to-install-pre-requisites-for-a-windows-subsystem-for-linux-wsl-envronment-using-an-ubuntu-220401-distribution","title":"How to install pre-requisites for a Windows Subsystem for Linux (WSL) envronment using an Ubuntu 22.04.01 distribution","text":"<p>(Tested on a Intel(R) Core(TM) i7-9850H CPU @ 2.60GHz 2.59 GHz with 32GB RAM, a 64-bit operating system, x64-based processor Using Windows 11 Enterprise)</p>"},{"location":"common-subs/required-packages/#1-if-youre-using-a-vpn-turn-it-off","title":"1. If you're using a VPN, turn it off","text":""},{"location":"common-subs/required-packages/#2-install-ubuntu-into-wsl","title":"2. Install Ubuntu into WSL","text":""},{"location":"common-subs/required-packages/#20-if-wsl-is-not-yet-installed-open-a-powershell-administrator-window-and-run-the-following","title":"2.0 If wsl is not yet installed, open a powershell administrator window and run the following","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"common-subs/required-packages/#21-reboot-your-system","title":"2.1 reboot your system","text":""},{"location":"common-subs/required-packages/#22-in-a-windows-command-terminal-run-the-following-to-list-all-the-linux-distributions-that-are-available-online","title":"2.2 In a Windows command terminal run the following to list all the linux distributions that are available online","text":"<pre><code>wsl -l -o\n</code></pre>"},{"location":"common-subs/required-packages/#23-select-a-linux-distribution-and-install-it-into-wsl","title":"2.3 Select a linux distribution and install it into WSL","text":"<p><pre><code>wsl --install -d Ubuntu 22.04.01\n</code></pre> You will see something like: <pre><code>Installing, this may take a few minutes...\nPlease create a default UNIX user account. The username does not need to match your Windows username.\nFor more information visit: https://aka.ms/wslusers\nEnter new UNIX username:\n</code></pre></p>"},{"location":"common-subs/required-packages/#24-enter-your-new-username-and-password-at-the-prompts-and-you-will-eventually-see-something-like","title":"2.4 Enter your new username and password at the prompts, and you will eventually see something like:","text":"<pre><code>Welcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.10.102.1-microsoft-standard-WSL2 x86_64)\n</code></pre>"},{"location":"common-subs/required-packages/#25-click-on-the-windows-start-icon-and-type-in-the-name-of-your-distribution-into-the-search-box","title":"2.5 Click on the Windows \"Start\" icon and type in the name of your distribution into the search box.","text":"<p>Your new linux distribution should appear as a local \"App\". You can pin it to the Windows task bar or to Start for your future convenience. Start a VM using your distribution by clicking on the App.</p>"},{"location":"common-subs/required-packages/#3-install-pre-requisites-into-your-new-vm","title":"3. Install pre-requisites into your new VM","text":""},{"location":"common-subs/required-packages/#31-update-and-apply-apt-get-packages","title":"3.1 update and apply apt-get packages","text":"<pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"common-subs/required-packages/#32-install-golang","title":"3.2 Install golang","text":"<pre><code>wget https://golang.org/dl/go1.19.linux-amd64.tar.gz\nsudo tar -zxvf go1.19.linux-amd64.tar.gz -C /usr/local\necho export GOROOT=/usr/local/go | sudo tee -a /etc/profile\necho export PATH=\"$PATH:/usr/local/go/bin\" | sudo tee -a /etc/profile\nsource /etc/profile\ngo version\n</code></pre>"},{"location":"common-subs/required-packages/#33-install-ko-but-dont-do-ko-set-action-step","title":"3.3 Install ko (but don't do ko set action step)","text":"<pre><code>go install github.com/google/ko@latest\n</code></pre>"},{"location":"common-subs/required-packages/#34-install-gcc","title":"3.4 Install gcc","text":"<p>Either run this: <pre><code>sudo apt install build-essential\n</code></pre> or this: <pre><code>sudo apt-get update\napt install gcc\ngcc --version\n</code></pre></p>"},{"location":"common-subs/required-packages/#35-install-make-if-you-installed-build-essential-this-may-already-be-installed","title":"3.5 Install make (if you installed build-essential this may already be installed)","text":"<pre><code>apt install make\n</code></pre>"},{"location":"common-subs/required-packages/#36-install-jq","title":"3.6 Install jq","text":"<pre><code>DEBIAN_FRONTEND=noninteractive apt-get install -y jq\njq --version\n</code></pre>"},{"location":"common-subs/required-packages/#37-install-kubectl","title":"3.7 install kubectl","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"common-subs/required-packages/#38-install-helm-required-when-deploying-as-workload","title":"3.8 install helm (required when deploying as workload)","text":"<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"common-subs/required-packages/#how-to-install-docker-and-kind-into-a-windows-subsystem-for-linux-wsl-environment-using-an-ubuntu-220401-distribution","title":"How to install docker and kind into a Windows Subsystem for Linux (WSL) environment using an Ubuntu 22.04.01 distribution","text":""},{"location":"common-subs/required-packages/#10-start-a-vm-terminal-by-clicking-on-the-app-you-configured-using-the-instructions-in-the-general-pre-requisites-described-above","title":"1.0 Start a VM terminal by clicking on the App you configured using the instructions in the General pre-requisites described above.","text":""},{"location":"common-subs/required-packages/#20-install-docker","title":"2.0 Install docker","text":"<p>The installation instructions from docker are not sufficient to get docker working with WSL</p>"},{"location":"common-subs/required-packages/#21-follow-instructions-here-to-install-docker-httpsdocsdockercomengineinstallubuntu","title":"2.1 Follow instructions here to install docker https://docs.docker.com/engine/install/ubuntu/","text":"<p>Here some additonal steps you will need to take:</p>"},{"location":"common-subs/required-packages/#22-ensure-that-etcwslconf-is-configured-so-that-systemd-will-run-on-booting","title":"2.2 Ensure that /etc/wsl.conf is configured so that systemd will run on booting.","text":"<p>If /etc/wsl.conf does not contain [boot] systemd=true, then edit /etc/wsl.com as follows: <pre><code>sudo vi /etc/wsl.conf\n</code></pre> Insert <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"common-subs/required-packages/#23-edit-etcsudoers-it-is-strongly-recommended-to-not-add-directives-directly-to-etcsudoers-but-instead-to-put-them-in-files-in-etcsudoersd-which-are-auto-included-so-makemodify-a-new-file-via","title":"2.3 Edit /etc/sudoers: it is strongly recommended to not add directives directly to /etc/sudoers, but instead to put them in files in /etc/sudoers.d which are auto-included. So make/modify a new file via","text":"<p><pre><code>sudo vi /etc/sudoers.d/docker\n</code></pre> Insert <pre><code># Docker daemon specification\n&lt;your user account&gt; ALL=(ALL) NOPASSWD: /usr/bin/dockerd\n</code></pre></p>"},{"location":"common-subs/required-packages/#24-add-your-user-to-the-docker-group","title":"2.4 Add your user to the docker group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"common-subs/required-packages/#25-if-dockerd-is-already-running-then-stop-it-and-restart-it-as-follows-note-the-new-dockerd-instance-will-be-running-in-the-foreground","title":"2.5 If dockerd is already running, then stop it and restart it as follows (note: the new dockerd instance will be running in the foreground):","text":"<pre><code>sudo systemctl stop docker\nsudo dockerd &amp;\n</code></pre>"},{"location":"common-subs/required-packages/#251-if-you-encounter-an-iptables-issue-which-is-described-here-httpsgithubcommicrosoftwslissues6655","title":"2.5.1 If you encounter an iptables issue, which is described here: https://github.com/microsoft/WSL/issues/6655","text":"<p>The following commands will fix the issue:  <pre><code>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy\nsudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nsudo dockerd &amp; \n</code></pre></p>"},{"location":"common-subs/required-packages/#3-you-will-now-need-to-open-new-terminals-to-access-the-vm-since-dockerd-is-running-in-the-foreground-of-this-terminal","title":"3. You will now need to open new terminals to access the VM since dockerd is running in the foreground of this terminal","text":""},{"location":"common-subs/required-packages/#31-in-your-new-terminal-install-kind","title":"3.1 In your new terminal, install kind","text":"<pre><code>wget -nv https://github.com/kubernetes-sigs/kind/releases/download/v0.17.0/kind-linux-$(dpkg --print-architecture) -O kind \nsudo install -m 0755 kind /usr/local/bin/kind \nrm kind \nkind version\n</code></pre>"},{"location":"common-subs/save-some-time/","title":"Save some time","text":"<p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -b main https://github.com/kubestellar/kubestellar\ncd kubestellar\nmake MANIFEST=\"'{{ no such element: dict object['pre_req_name'] }}','{{ no such element: dict object['manifest_name'] }}'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ..\nrm -rf kubestellar\n</code></pre>"},{"location":"common-subs/teardown-the-environment/","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl kubestellar remove wmw example-wmw\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Teardown of KubeStellar depends on which style of deployment was used.</p>"},{"location":"common-subs/teardown-the-environment/#teardown-bare-processes","title":"Teardown bare processes","text":"<p>The following command will stop whatever KubeStellar controllers are running.</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"common-subs/teardown-the-environment/#teardown-kubernetes-workload","title":"Teardown Kubernetes workload","text":"<p>With <code>kubectl</code> configured to manipulate the hosting cluster, the following command will remove the workload that is kcp and KubeStellar.</p> <pre><code>helm delete kubestellar\n</code></pre>"}]}